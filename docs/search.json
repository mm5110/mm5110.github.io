[
  {
    "objectID": "old/markdown_generator/talks.html",
    "href": "old/markdown_generator/talks.html",
    "title": "Talks markdown generator for academicpages",
    "section": "",
    "text": "Takes a TSV of talks with metadata and converts them for use with academicpages.github.io. This is an interactive Jupyter notebook (see more info here). The core python code is also in talks.py. Run either from the markdown_generator folder after replacing talks.tsv with one containing your data.\nTODO: Make this work with BibTex and other databases, rather than Stuart’s non-standard TSV format and citation style.\nimport pandas as pd\nimport os"
  },
  {
    "objectID": "old/markdown_generator/talks.html#data-format",
    "href": "old/markdown_generator/talks.html#data-format",
    "title": "Talks markdown generator for academicpages",
    "section": "Data format",
    "text": "Data format\nThe TSV needs to have the following columns: title, type, url_slug, venue, date, location, talk_url, description, with a header at the top. Many of these fields can be blank, but the columns must be in the TSV.\n\nFields that cannot be blank: title, url_slug, date. All else can be blank. type defaults to “Talk”\ndate must be formatted as YYYY-MM-DD.\nurl_slug will be the descriptive part of the .md file and the permalink URL for the page about the paper.\n\nThe .md file will be YYYY-MM-DD-[url_slug].md and the permalink will be https://[yourdomain]/talks/YYYY-MM-DD-[url_slug]\nThe combination of url_slug and date must be unique, as it will be the basis for your filenames\n\n\nThis is how the raw file looks (it doesn’t look pretty, use a spreadsheet or other program to edit and create).\n\n!cat talks.tsv\n\ntitle   type    url_slug    venue   date    location    talk_url    description\nTalk 1 on Relevant Topic in Your Field  Talk    talk-1  UC San Francisco, Department of Testing 2012-03-01  San Francisco, California       This is a description of your talk, which is a markdown files that can be all markdown-ified like any other post. Yay markdown!\nTutorial 1 on Relevant Topic in Your Field  Tutorial    tutorial-1  UC-Berkeley Institute for Testing Science   2013-03-01  Berkeley CA, USA    http://exampleurl.com   This is a description of your tutorial, note the different field in type. This is a markdown files that can be all markdown-ified like any other post. Yay markdown!\nTalk 2 on Relevant Topic in Your Field  Talk    talk-2  London School of Testing    2014-02-01  London, UK  http://example2.com This is a description of your talk, which is a markdown files that can be all markdown-ified like any other post. Yay markdown!\nConference Proceeding talk 3 on Relevant Topic in Your Field    Conference proceedings talk talk-3  Testing Institute of America 2014 Annual Conference 2014-03-01  Los Angeles, CA     This is a description of your conference proceedings talk, note the different field in type. You can put anything in this field."
  },
  {
    "objectID": "old/markdown_generator/talks.html#import-tsv",
    "href": "old/markdown_generator/talks.html#import-tsv",
    "title": "Talks markdown generator for academicpages",
    "section": "Import TSV",
    "text": "Import TSV\nPandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or \\t.\nI found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others.\n\ntalks = pd.read_csv(\"talks.tsv\", sep=\"\\t\", header=0)\ntalks\n\n\n\n\n\n\n\ntitle\ntype\nurl_slug\nvenue\ndate\nlocation\ntalk_url\ndescription\n\n\n\n\n0\nTalk 1 on Relevant Topic in Your Field\nTalk\ntalk-1\nUC San Francisco, Department of Testing\n2012-03-01\nSan Francisco, California\nNaN\nThis is a description of your talk, which is a...\n\n\n1\nTutorial 1 on Relevant Topic in Your Field\nTutorial\ntutorial-1\nUC-Berkeley Institute for Testing Science\n2013-03-01\nBerkeley CA, USA\nhttp://exampleurl.com\nThis is a description of your tutorial, note t...\n\n\n2\nTalk 2 on Relevant Topic in Your Field\nTalk\ntalk-2\nLondon School of Testing\n2014-02-01\nLondon, UK\nhttp://example2.com\nThis is a description of your talk, which is a...\n\n\n3\nConference Proceeding talk 3 on Relevant Topic...\nConference proceedings talk\ntalk-3\nTesting Institute of America 2014 Annual Confe...\n2014-03-01\nLos Angeles, CA\nNaN\nThis is a description of your conference proce..."
  },
  {
    "objectID": "old/markdown_generator/talks.html#escape-special-characters",
    "href": "old/markdown_generator/talks.html#escape-special-characters",
    "title": "Talks markdown generator for academicpages",
    "section": "Escape special characters",
    "text": "Escape special characters\nYAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely.\n\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\n\ndef html_escape(text):\n    if type(text) is str:\n        return \"\".join(html_escape_table.get(c,c) for c in text)\n    else:\n        return \"False\""
  },
  {
    "objectID": "old/markdown_generator/talks.html#creating-the-markdown-files",
    "href": "old/markdown_generator/talks.html#creating-the-markdown-files",
    "title": "Talks markdown generator for academicpages",
    "section": "Creating the markdown files",
    "text": "Creating the markdown files\nThis is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (md) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page.\n\nloc_dict = {}\n\nfor row, item in talks.iterrows():\n    \n    md_filename = str(item.date) + \"-\" + item.url_slug + \".md\"\n    html_filename = str(item.date) + \"-\" + item.url_slug \n    year = item.date[:4]\n    \n    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n    md += \"collection: talks\" + \"\\n\"\n    \n    if len(str(item.type)) &gt; 3:\n        md += 'type: \"' + item.type + '\"\\n'\n    else:\n        md += 'type: \"Talk\"\\n'\n    \n    md += \"permalink: /talks/\" + html_filename + \"\\n\"\n    \n    if len(str(item.venue)) &gt; 3:\n        md += 'venue: \"' + item.venue + '\"\\n'\n        \n    if len(str(item.location)) &gt; 3:\n        md += \"date: \" + str(item.date) + \"\\n\"\n    \n    if len(str(item.location)) &gt; 3:\n        md += 'location: \"' + str(item.location) + '\"\\n'\n           \n    md += \"---\\n\"\n    \n    \n    if len(str(item.talk_url)) &gt; 3:\n        md += \"\\n[More information here](\" + item.talk_url + \")\\n\" \n        \n    \n    if len(str(item.description)) &gt; 3:\n        md += \"\\n\" + html_escape(item.description) + \"\\n\"\n        \n        \n    md_filename = os.path.basename(md_filename)\n    #print(md)\n    \n    with open(\"../_talks/\" + md_filename, 'w') as f:\n        f.write(md)\n\nThese files are in the talks directory, one directory below where we’re working from.\n\n!ls ../_talks\n\n2012-03-01-talk-1.md      2014-02-01-talk-2.md\n2013-03-01-tutorial-1.md  2014-03-01-talk-3.md\n\n\n\n!cat ../_talks/2013-03-01-tutorial-1.md\n\n---\ntitle: \"Tutorial 1 on Relevant Topic in Your Field\"\ncollection: talks\ntype: \"Tutorial\"\npermalink: /talks/2013-03-01-tutorial-1\nvenue: \"UC-Berkeley Institute for Testing Science\"\ndate: 2013-03-01\nlocation: \"Berkeley CA, USA\"\n---\n\n[More information here](http://exampleurl.com)\n\nThis is a description of your tutorial, note the different field in type. This is a markdown files that can be all markdown-ified like any other post. Yay markdown!"
  },
  {
    "objectID": "old/markdown_generator/publications.html",
    "href": "old/markdown_generator/publications.html",
    "title": "Publications markdown generator for academicpages",
    "section": "",
    "text": "Takes a TSV of publications with metadata and converts them for use with academicpages.github.io. This is an interactive Jupyter notebook (see more info here). The core python code is also in publications.py. Run either from the markdown_generator folder after replacing publications.tsv with one containing your data.\nTODO: Make this work with BibTex and other databases of citations, rather than Stuart’s non-standard TSV format and citation style."
  },
  {
    "objectID": "old/markdown_generator/publications.html#data-format",
    "href": "old/markdown_generator/publications.html#data-format",
    "title": "Publications markdown generator for academicpages",
    "section": "Data format",
    "text": "Data format\nThe TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top.\n\nexcerpt and paper_url can be blank, but the others must have values.\npub_date must be formatted as YYYY-MM-DD.\nurl_slug will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be YYYY-MM-DD-[url_slug].md and the permalink will be https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]\n\nThis is how the raw file looks (it doesn’t look pretty, use a spreadsheet or other program to edit and create).\n\n!cat publications.tsv\n\npub_date    title   venue   excerpt citation    url_slug    paper_url\n2009-10-01  Paper Title Number 1    Journal 1   This paper is about the number 1. The number 2 is left for future work. Your Name, You. (2009). \"Paper Title Number 1.\" &lt;i&gt;Journal 1&lt;/i&gt;. 1(1). paper-title-number-1    http://academicpages.github.io/files/paper1.pdf\n2010-10-01  Paper Title Number 2    Journal 1   This paper is about the number 2. The number 3 is left for future work. Your Name, You. (2010). \"Paper Title Number 2.\" &lt;i&gt;Journal 1&lt;/i&gt;. 1(2). paper-title-number-2    http://academicpages.github.io/files/paper2.pdf\n2015-10-01  Paper Title Number 3    Journal 1   This paper is about the number 3. The number 4 is left for future work. Your Name, You. (2015). \"Paper Title Number 3.\" &lt;i&gt;Journal 1&lt;/i&gt;. 1(3). paper-title-number-3    http://academicpages.github.io/files/paper3.pdf"
  },
  {
    "objectID": "old/markdown_generator/publications.html#import-pandas",
    "href": "old/markdown_generator/publications.html#import-pandas",
    "title": "Publications markdown generator for academicpages",
    "section": "Import pandas",
    "text": "Import pandas\nWe are using the very handy pandas library for dataframes.\n\nimport pandas as pd"
  },
  {
    "objectID": "old/markdown_generator/publications.html#import-tsv",
    "href": "old/markdown_generator/publications.html#import-tsv",
    "title": "Publications markdown generator for academicpages",
    "section": "Import TSV",
    "text": "Import TSV\nPandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or \\t.\nI found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others.\n\npublications = pd.read_csv(\"publications.tsv\", sep=\"\\t\", header=0)\npublications\n\n\n\n\n\n\n\npub_date\ntitle\nvenue\nexcerpt\ncitation\nurl_slug\npaper_url\n\n\n\n\n0\n2009-10-01\nPaper Title Number 1\nJournal 1\nThis paper is about the number 1. The number 2...\nYour Name, You. (2009). \"Paper Title Number 1....\npaper-title-number-1\nhttp://academicpages.github.io/files/paper1.pdf\n\n\n1\n2010-10-01\nPaper Title Number 2\nJournal 1\nThis paper is about the number 2. The number 3...\nYour Name, You. (2010). \"Paper Title Number 2....\npaper-title-number-2\nhttp://academicpages.github.io/files/paper2.pdf\n\n\n2\n2015-10-01\nPaper Title Number 3\nJournal 1\nThis paper is about the number 3. The number 4...\nYour Name, You. (2015). \"Paper Title Number 3....\npaper-title-number-3\nhttp://academicpages.github.io/files/paper3.pdf"
  },
  {
    "objectID": "old/markdown_generator/publications.html#escape-special-characters",
    "href": "old/markdown_generator/publications.html#escape-special-characters",
    "title": "Publications markdown generator for academicpages",
    "section": "Escape special characters",
    "text": "Escape special characters\nYAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely.\n\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\n\ndef html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)"
  },
  {
    "objectID": "old/markdown_generator/publications.html#creating-the-markdown-files",
    "href": "old/markdown_generator/publications.html#creating-the-markdown-files",
    "title": "Publications markdown generator for academicpages",
    "section": "Creating the markdown files",
    "text": "Creating the markdown files\nThis is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (md) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page.\n\nimport os\nfor row, item in publications.iterrows():\n    \n    md_filename = str(item.pub_date) + \"-\" + item.url_slug + \".md\"\n    html_filename = str(item.pub_date) + \"-\" + item.url_slug\n    year = item.pub_date[:4]\n    \n    ## YAML variables\n    \n    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n    \n    md += \"\"\"collection: publications\"\"\"\n    \n    md += \"\"\"\\npermalink: /publication/\"\"\" + html_filename\n    \n    if len(str(item.excerpt)) &gt; 5:\n        md += \"\\nexcerpt: '\" + html_escape(item.excerpt) + \"'\"\n    \n    md += \"\\ndate: \" + str(item.pub_date) \n    \n    md += \"\\nvenue: '\" + html_escape(item.venue) + \"'\"\n    \n    if len(str(item.paper_url)) &gt; 5:\n        md += \"\\npaperurl: '\" + item.paper_url + \"'\"\n    \n    md += \"\\ncitation: '\" + html_escape(item.citation) + \"'\"\n    \n    md += \"\\n---\"\n    \n    ## Markdown description for individual page\n        \n    if len(str(item.excerpt)) &gt; 5:\n        md += \"\\n\" + html_escape(item.excerpt) + \"\\n\"\n    \n    if len(str(item.paper_url)) &gt; 5:\n        md += \"\\n[Download paper here](\" + item.paper_url + \")\\n\" \n        \n    md += \"\\nRecommended citation: \" + item.citation\n    \n    md_filename = os.path.basename(md_filename)\n       \n    with open(\"../_publications/\" + md_filename, 'w') as f:\n        f.write(md)\n\nThese files are in the publications directory, one directory below where we’re working from.\n\n!ls ../_publications/\n\n2009-10-01-paper-title-number-1.md  2015-10-01-paper-title-number-3.md\n2010-10-01-paper-title-number-2.md\n\n\n\n!cat ../_publications/2009-10-01-paper-title-number-1.md\n\n---\ntitle: \"Paper Title Number 1\"\ncollection: publications\npermalink: /publication/2009-10-01-paper-title-number-1\nexcerpt: 'This paper is about the number 1. The number 2 is left for future work.'\ndate: 2009-10-01\nvenue: 'Journal 1'\npaperurl: 'http://academicpages.github.io/files/paper1.pdf'\ncitation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; &lt;i&gt;Journal 1&lt;/i&gt;. 1(1).'\n---\nThis paper is about the number 1. The number 2 is left for future work.\n\n[Download paper here](http://academicpages.github.io/files/paper1.pdf)\n\nRecommended citation: Your Name, You. (2009). \"Paper Title Number 1.\" &lt;i&gt;Journal 1&lt;/i&gt;. 1(1)."
  },
  {
    "objectID": "old/markdown_generator/readme.html",
    "href": "old/markdown_generator/readme.html",
    "title": "Jupyter notebook markdown generator",
    "section": "",
    "text": "Jupyter notebook markdown generator\nThese .ipynb files are Jupyter notebook files that convert a TSV containing structured data about talks (talks.tsv) or presentations (presentations.tsv) into individual markdown files that will be properly formatted for the academicpages template. The notebooks contain a lot of documentation about the process. The .py files are pure python that do the same things if they are executed in a terminal, they just don’t have pretty documentation."
  },
  {
    "objectID": "old/markdown_generator/PubsFromBib.html",
    "href": "old/markdown_generator/PubsFromBib.html",
    "title": "Publications markdown generator for academicpages",
    "section": "",
    "text": "Takes a set of bibtex of publications and converts them for use with academicpages.github.io. This is an interactive Jupyter notebook (see more info here).\nThe core python code is also in pubsFromBibs.py. Run either from the markdown_generator folder after replacing updating the publist dictionary with: * bib file names * specific venue keys based on your bib file preferences * any specific pre-text for specific files * Collection Name (future feature)\nTODO: Make this work with other databases of citations, TODO: Merge this with the existing TSV parsing solution\n\nfrom pybtex.database.input import bibtex\nimport pybtex.database.input.bibtex \nfrom time import strptime\nimport string\nimport html\nimport os\nimport re\n\n\n#todo: incorporate different collection types rather than a catch all publications, requires other changes to template\npublist = {\n    \"proceeding\": {\n        \"file\" : \"proceedings.bib\",\n        \"venuekey\": \"booktitle\",\n        \"venue-pretext\": \"In the proceedings of \",\n        \"collection\" : {\"name\":\"publications\",\n                        \"permalink\":\"/publication/\"}\n        \n    },\n    \"journal\":{\n        \"file\": \"pubs.bib\",\n        \"venuekey\" : \"journal\",\n        \"venue-pretext\" : \"\",\n        \"collection\" : {\"name\":\"publications\",\n                        \"permalink\":\"/publication/\"}\n    } \n}\n\n\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\n\ndef html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\n\n\nfor pubsource in publist:\n    parser = bibtex.Parser()\n    bibdata = parser.parse_file(publist[pubsource][\"file\"])\n\n    #loop through the individual references in a given bibtex file\n    for bib_id in bibdata.entries:\n        #reset default date\n        pub_year = \"1900\"\n        pub_month = \"01\"\n        pub_day = \"01\"\n        \n        b = bibdata.entries[bib_id].fields\n        \n        try:\n            pub_year = f'{b[\"year\"]}'\n\n            #todo: this hack for month and day needs some cleanup\n            if \"month\" in b.keys(): \n                if(len(b[\"month\"])&lt;3):\n                    pub_month = \"0\"+b[\"month\"]\n                    pub_month = pub_month[-2:]\n                elif(b[\"month\"] not in range(12)):\n                    tmnth = strptime(b[\"month\"][:3],'%b').tm_mon   \n                    pub_month = \"{:02d}\".format(tmnth) \n                else:\n                    pub_month = str(b[\"month\"])\n            if \"day\" in b.keys(): \n                pub_day = str(b[\"day\"])\n\n                \n            pub_date = pub_year+\"-\"+pub_month+\"-\"+pub_day\n            \n            #strip out {} as needed (some bibtex entries that maintain formatting)\n            clean_title = b[\"title\"].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\").replace(\" \",\"-\")    \n\n            url_slug = re.sub(\"\\\\[.*\\\\]|[^a-zA-Z0-9_-]\", \"\", clean_title)\n            url_slug = url_slug.replace(\"--\",\"-\")\n\n            md_filename = (str(pub_date) + \"-\" + url_slug + \".md\").replace(\"--\",\"-\")\n            html_filename = (str(pub_date) + \"-\" + url_slug).replace(\"--\",\"-\")\n\n            #Build Citation from text\n            citation = \"\"\n\n            #citation authors - todo - add highlighting for primary author?\n            for author in bibdata.entries[bib_id].persons[\"author\"]:\n                citation = citation+\" \"+author.first_names[0]+\" \"+author.last_names[0]+\", \"\n\n            #citation title\n            citation = citation + \"\\\"\" + html_escape(b[\"title\"].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\")) + \".\\\"\"\n\n            #add venue logic depending on citation type\n            venue = publist[pubsource][\"venue-pretext\"]+b[publist[pubsource][\"venuekey\"]].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\")\n\n            citation = citation + \" \" + html_escape(venue)\n            citation = citation + \", \" + pub_year + \".\"\n\n            \n            ## YAML variables\n            md = \"---\\ntitle: \\\"\"   + html_escape(b[\"title\"].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\")) + '\"\\n'\n            \n            md += \"\"\"collection: \"\"\" +  publist[pubsource][\"collection\"][\"name\"]\n\n            md += \"\"\"\\npermalink: \"\"\" + publist[pubsource][\"collection\"][\"permalink\"]  + html_filename\n            \n            note = False\n            if \"note\" in b.keys():\n                if len(str(b[\"note\"])) &gt; 5:\n                    md += \"\\nexcerpt: '\" + html_escape(b[\"note\"]) + \"'\"\n                    note = True\n\n            md += \"\\ndate: \" + str(pub_date) \n\n            md += \"\\nvenue: '\" + html_escape(venue) + \"'\"\n            \n            url = False\n            if \"url\" in b.keys():\n                if len(str(b[\"url\"])) &gt; 5:\n                    md += \"\\npaperurl: '\" + b[\"url\"] + \"'\"\n                    url = True\n\n            md += \"\\ncitation: '\" + html_escape(citation) + \"'\"\n\n            md += \"\\n---\"\n\n            \n            ## Markdown description for individual page\n            if note:\n                md += \"\\n\" + html_escape(b[\"note\"]) + \"\\n\"\n\n            if url:\n                md += \"\\n[Access paper here](\" + b[\"url\"] + \"){:target=\\\"_blank\\\"}\\n\" \n            else:\n                md += \"\\nUse [Google Scholar](https://scholar.google.com/scholar?q=\"+html.escape(clean_title.replace(\"-\",\"+\"))+\"){:target=\\\"_blank\\\"} for full citation\"\n\n            md_filename = os.path.basename(md_filename)\n\n            with open(\"../_publications/\" + md_filename, 'w') as f:\n                f.write(md)\n            print(f'SUCESSFULLY PARSED {bib_id}: \\\"', b[\"title\"][:60],\"...\"*(len(b['title'])&gt;60),\"\\\"\")\n        # field may not exist for a reference\n        except KeyError as e:\n            print(f'WARNING Missing Expected Field {e} from entry {bib_id}: \\\"', b[\"title\"][:30],\"...\"*(len(b['title'])&gt;30),\"\\\"\")\n            continue"
  },
  {
    "objectID": "old/talkmap.html",
    "href": "old/talkmap.html",
    "title": "Leaflet cluster map of talk locations",
    "section": "",
    "text": "Run this from the _talks/ directory, which contains .md files of all your talks. This scrapes the location YAML field from each .md file, geolocates it with geopy/Nominatim, and uses the getorg library to output data, HTML, and Javascript for a standalone cluster map.\n\n!pip install getorg --upgrade\nimport glob\nimport getorg\nfrom geopy import Nominatim\n\nRequirement already up-to-date: getorg in /home/vm/anaconda3/lib/python3.5/site-packages\nRequirement already up-to-date: geopy in /home/vm/.local/lib/python3.5/site-packages (from getorg)\nRequirement already up-to-date: retrying in /home/vm/.local/lib/python3.5/site-packages (from getorg)\nRequirement already up-to-date: pygithub in /home/vm/anaconda3/lib/python3.5/site-packages (from getorg)\nRequirement already up-to-date: six&gt;=1.7.0 in /home/vm/.local/lib/python3.5/site-packages (from retrying-&gt;getorg)\nIPywidgets and ipyleaflet support enabled.\n\n\n\ng = glob.glob(\"*.md\")\n\n\ngeocoder = Nominatim()\nlocation_dict = {}\nlocation = \"\"\npermalink = \"\"\ntitle = \"\"\n\n\n\nfor file in g:\n    with open(file, 'r') as f:\n        lines = f.read()\n        if lines.find('location: \"') &gt; 1:\n            loc_start = lines.find('location: \"') + 11\n            lines_trim = lines[loc_start:]\n            loc_end = lines_trim.find('\"')\n            location = lines_trim[:loc_end]\n                            \n           \n        location_dict[location] = geocoder.geocode(location)\n        print(location, \"\\n\", location_dict[location])\n\nBerkeley CA, USA \n Berkeley, Alameda County, California, United States of America\nLos Angeles, CA \n LA, Los Angeles County, California, United States of America\nLondon, UK \n London, Greater London, England, UK\nSan Francisco, California \n SF, California, United States of America\n\n\n\nm = getorg.orgmap.create_map_obj()\ngetorg.orgmap.output_html_cluster_map(location_dict, folder_name=\"../talkmap\", hashed_usernames=False)\n\n'Written map to ../talkmap/'"
  },
  {
    "objectID": "old/CHANGELOG.html",
    "href": "old/CHANGELOG.html",
    "title": "mm5110.github.io",
    "section": "",
    "text": "Improve UX of static comment forms. #448"
  },
  {
    "objectID": "old/CHANGELOG.html#section",
    "href": "old/CHANGELOG.html#section",
    "title": "mm5110.github.io",
    "section": "",
    "text": "Improve UX of static comment forms. #448"
  },
  {
    "objectID": "old/CHANGELOG.html#section-1",
    "href": "old/CHANGELOG.html#section-1",
    "title": "mm5110.github.io",
    "section": "3.4.1",
    "text": "3.4.1\n\nEnhancements\n\nAdd staticman.filename configuration with UNIX timestamp for sorting data files. example ~&gt; comment-1470943149.\n\n\n\nBug Fixes\n\nDon’t add &lt;a&gt; to author name if URL is blank."
  },
  {
    "objectID": "old/CHANGELOG.html#section-2",
    "href": "old/CHANGELOG.html#section-2",
    "title": "mm5110.github.io",
    "section": "3.4.0",
    "text": "3.4.0\n\nEnhancements\n\nSupport static-based commenting via Staticman for sites hosted with GitHub Pages. #424"
  },
  {
    "objectID": "old/CHANGELOG.html#section-3",
    "href": "old/CHANGELOG.html#section-3",
    "title": "mm5110.github.io",
    "section": "3.3.7",
    "text": "3.3.7\n\nBug Fixes\n\nRe-enabled Jekyll plugins in _config.yml in case they aren’t autoloaded in Gemfile. #417\n\n\n\nEnhancements\n\nFallback to site.github.url for use in {{ base_path }} when site.url is nil.\nReplace Sass and Autoprefixer npm build scripts with Jekyll’s built-in asset support. #333\n\n\n\nMaintenance\n\nDocument site.repository and its role with github-metadata gem.\nAdd sample archive page with content for testing styles on demo site."
  },
  {
    "objectID": "old/CHANGELOG.html#section-4",
    "href": "old/CHANGELOG.html#section-4",
    "title": "mm5110.github.io",
    "section": "3.3.6",
    "text": "3.3.6\n\nBug Fixes\n\nFix blank site.teaser bug. #412"
  },
  {
    "objectID": "old/CHANGELOG.html#section-5",
    "href": "old/CHANGELOG.html#section-5",
    "title": "mm5110.github.io",
    "section": "3.3.5",
    "text": "3.3.5\n\nEnhancements\n\nAdd English default text site.locale strings. #407\nAdd Portuguese localized UI text. #411\nAdd Italian localized UI text. #409\n\n\n\nMaintenance\n\nRemove unused Google AdSense variables in _config.yml. #404\nUpdate Gemfile instructions for using github-pages vs. native jekyll gems.\nDisable gems: in _config.yml and enable plugins with Bundler instead.\nAdd repository to _config.yml to suppress GitHub Pages error Liquid Exception: No repo name found."
  },
  {
    "objectID": "old/CHANGELOG.html#section-6",
    "href": "old/CHANGELOG.html#section-6",
    "title": "mm5110.github.io",
    "section": "3.3.4",
    "text": "3.3.4\n\nEnhancements\n\nAdd support for configurable feed URL to use a service like FeedBurner instead of linking directly to feed.xml in &lt;head&gt; and the site footer. #378, #379, #406\nAdd Turkish localized UI text. #403\n\n\n\nMaintenance\n\nUpdate gems: activesupport (4.2.7), ffi (1.9.14), github-pages (88), jekyll-redirect-from (0.11.0), jekyll-watch (1.5.0)."
  },
  {
    "objectID": "old/CHANGELOG.html#section-7",
    "href": "old/CHANGELOG.html#section-7",
    "title": "mm5110.github.io",
    "section": "3.3.3",
    "text": "3.3.3\n\nEnhancements\n\nMake footer stick to the bottom of the page.\n\n\n\nBug Fixes\n\nFix gallery size bug #402\n\n\n\nMaintenance\n\nSet default lang to en."
  },
  {
    "objectID": "old/CHANGELOG.html#section-8",
    "href": "old/CHANGELOG.html#section-8",
    "title": "mm5110.github.io",
    "section": "3.3.2",
    "text": "3.3.2\n\nBug Fixes\n\nFix JavaScript that triggers “sticky” sidebar to avoid layout issues on screen sizes &lt; 1024px. #396"
  },
  {
    "objectID": "old/CHANGELOG.html#section-9",
    "href": "old/CHANGELOG.html#section-9",
    "title": "mm5110.github.io",
    "section": "3.3.1",
    "text": "3.3.1\n\nEnhancements\n\nEnable image popup on &lt; 500px wide screens. #385\nIndicate the relationship between component URLs in a paginated series by applying rel=\"prev\" and rel=\"next\" to pages that use site.paginator. #253\nImprove link posts in archive listings. #276\n\n\n\nMaintenance\n\nUpdate gems: github-pages (86), ffi 1.9.13, jekyll-mentions 1.1.3, and rouge 1.11.1\nFix note about custom sidebar content appearing below author profile. #388"
  },
  {
    "objectID": "old/CHANGELOG.html#section-10",
    "href": "old/CHANGELOG.html#section-10",
    "title": "mm5110.github.io",
    "section": "3.2.13",
    "text": "3.2.13\n\nEnhancements\n\nAdd English default UI text for Canada, Great Britain, and Australia. #377\nSwitch default locale from en-US to en."
  },
  {
    "objectID": "old/CHANGELOG.html#section-11",
    "href": "old/CHANGELOG.html#section-11",
    "title": "mm5110.github.io",
    "section": "3.2.12",
    "text": "3.2.12\n\nEnhancements\n\nRemove window width “magic number” from sticky sidebar check in main.js for improved flexibility. #375\n\n\n\nBug Fixes\n\nFix author override conditional where a missing authors.yml would show broken sidebar content. Defaults to site.author. #376"
  },
  {
    "objectID": "old/CHANGELOG.html#section-12",
    "href": "old/CHANGELOG.html#section-12",
    "title": "mm5110.github.io",
    "section": "3.2.11",
    "text": "3.2.11\n\nBug Fixes\n\nFix disappearing author sidebar links #372\n\n\n\nMaintenance\n\nUpdate gems: github-pages (84), jekyll-github-metadata 2.0.2, and kramdown 1.11.1\nUpdate vendor JavaScript: jQuery 1.12.4, Stickyfill.js 1.1.4\nUpdate Font Awesome 4.6.3"
  },
  {
    "objectID": "old/CHANGELOG.html#section-13",
    "href": "old/CHANGELOG.html#section-13",
    "title": "mm5110.github.io",
    "section": "3.2.10",
    "text": "3.2.10\n\nMaintenance\n\nAdd CONTRIBUTING.md"
  },
  {
    "objectID": "old/CHANGELOG.html#section-14",
    "href": "old/CHANGELOG.html#section-14",
    "title": "mm5110.github.io",
    "section": "3.2.9",
    "text": "3.2.9\n\nEnhancements\n\nAdd support for header overlay images for Open Graph images. #358\n\n\n\nBug Fixes\n\nFix Person typo Schema.org type #358\n\n\n\nMaintenance\n\nUpdate github-pages gem and dependencies.\nRemove minutes_read to avoid awkward reading time wording #356"
  },
  {
    "objectID": "old/CHANGELOG.html#section-15",
    "href": "old/CHANGELOG.html#section-15",
    "title": "mm5110.github.io",
    "section": "3.2.8",
    "text": "3.2.8\n\nBug Fixes\n\nRemove cursor: pointer that appears on white-space surrounding author side list items and links. #354\n\n\n\nMaintenance\n\nAdd contributing information to README.md. #357"
  },
  {
    "objectID": "old/CHANGELOG.html#section-16",
    "href": "old/CHANGELOG.html#section-16",
    "title": "mm5110.github.io",
    "section": "3.2.7",
    "text": "3.2.7\n\nEnhancements\n\nAdd French localized UI text. #346\n\n\n\nBug Fixes\n\nFix branch logic for Yandex and Alexa in seo.html. #348"
  },
  {
    "objectID": "old/CHANGELOG.html#section-17",
    "href": "old/CHANGELOG.html#section-17",
    "title": "mm5110.github.io",
    "section": "3.2.6",
    "text": "3.2.6\n\nBug Fixes\n\nFix error Liquid Exception: divided by 0 in _includes/archive-single.html, included in _layouts/single.html caused by null words_per_minute in _config.yml. #345"
  },
  {
    "objectID": "old/CHANGELOG.html#section-18",
    "href": "old/CHANGELOG.html#section-18",
    "title": "mm5110.github.io",
    "section": "3.2.5",
    "text": "3.2.5\n\nBug Fixes\n\nFix link color in hero overlay to be white.\nRemove underlines from archive item titles."
  },
  {
    "objectID": "old/CHANGELOG.html#section-19",
    "href": "old/CHANGELOG.html#section-19",
    "title": "mm5110.github.io",
    "section": "3.2.4",
    "text": "3.2.4\n\nEnhancements\n\nImprove text alignment of masthead, hero overlay, page footer to be flush left and remove awkward white-space gaps. #342\nAdd Spanish localized UI text. #338\n\n\n\nBug Fixes\n\nFix alignment of icons in author sidebar #341\n\n\n\nMaintenance\n\nAdd background color to page footer to set it apart from main content. #342\nAdd terms and privacy policy to theme’s demo site. #343\nUpdate screenshots found in theme documentation."
  },
  {
    "objectID": "old/CHANGELOG.html#section-20",
    "href": "old/CHANGELOG.html#section-20",
    "title": "mm5110.github.io",
    "section": "3.2.3",
    "text": "3.2.3\n\nEnhancements\n\nAdd Discourse as a commenting provider. #335"
  },
  {
    "objectID": "old/CHANGELOG.html#section-21",
    "href": "old/CHANGELOG.html#section-21",
    "title": "mm5110.github.io",
    "section": "3.2.2",
    "text": "3.2.2\n\nEnhancements\n\nAdd support for image captions in Magnific Popup overlays via the gallery helper. #334"
  },
  {
    "objectID": "old/CHANGELOG.html#section-22",
    "href": "old/CHANGELOG.html#section-22",
    "title": "mm5110.github.io",
    "section": "3.2.1",
    "text": "3.2.1\n\nBug Fixes\n\nRemove need for “double tapping” masthead menu links on iOS devices. #315\n\n\n\nMaintenance\n\nAdd ISSUE_TEMPLATE.md for improve issue submission process."
  },
  {
    "objectID": "old/CHANGELOG.html#section-23",
    "href": "old/CHANGELOG.html#section-23",
    "title": "mm5110.github.io",
    "section": "3.2.0",
    "text": "3.2.0\n\nBug Fixes\n\nFix missing category/tag links in post footer due to possible conflict with site.tags and site.categories. #329"
  },
  {
    "objectID": "old/CHANGELOG.html#section-24",
    "href": "old/CHANGELOG.html#section-24",
    "title": "mm5110.github.io",
    "section": "3.1.8",
    "text": "3.1.8\n\nBug Fixes\n\nFix Liquid Exception: undefined method 'gsub' for nil:NilClass in _layouts/single.html error when page.title is null. &lt;h1&gt; element is now conditional if title: is not set for a page or collection item. #312\n\n\n\nMaintenance\n\nRemove duplicate fa-twitter and fa-twitter-square classes from _utilities.scss. #302\nDocument installing additional Jekyll gem dependencies when using gem \"jekyll\" instead of gem \"github-pages\" to avoid any errors on run. #305"
  },
  {
    "objectID": "old/CHANGELOG.html#section-25",
    "href": "old/CHANGELOG.html#section-25",
    "title": "mm5110.github.io",
    "section": "3.1.7",
    "text": "3.1.7\n\nEnhancements\n\nAdd translation key for “Recent Posts” used in home page index.html. #316\n\n\n\nMaintenance\n\nSmall fix to avoid underlying the whitespace between icons and related text when hovering. #303"
  },
  {
    "objectID": "old/CHANGELOG.html#section-26",
    "href": "old/CHANGELOG.html#section-26",
    "title": "mm5110.github.io",
    "section": "3.1.6",
    "text": "3.1.6\n\nMaintenance\n\nUpdate gem dependencies. Run bundle to update Gemfile.lock."
  },
  {
    "objectID": "old/CHANGELOG.html#section-27",
    "href": "old/CHANGELOG.html#section-27",
    "title": "mm5110.github.io",
    "section": "3.1.5",
    "text": "3.1.5\n\nMaintenance\n\nFix www and https links in author profile include #293"
  },
  {
    "objectID": "old/CHANGELOG.html#section-28",
    "href": "old/CHANGELOG.html#section-28",
    "title": "mm5110.github.io",
    "section": "3.1.4",
    "text": "3.1.4\n\nEnhancements\n\nAdd overlay_filter param to hero headers #298"
  },
  {
    "objectID": "old/CHANGELOG.html#section-29",
    "href": "old/CHANGELOG.html#section-29",
    "title": "mm5110.github.io",
    "section": "3.1.3",
    "text": "3.1.3\n\nEnhancements\n\nImprove site.locale documentation #284\nRemove ProTip note about protocol-less site.url as it is an anti-pattern #288\n\n\n\nBug Fixes\n\nFix og_image URL in seo.html #277\nFix author_profile toggle when assigned in a _layout #285\nFix typo in build:all npm script #283\nFix URL typo documentation #287\nSEO author bug. If twitter.username is set and author.twitter is nil bad things happen. #289"
  },
  {
    "objectID": "old/CHANGELOG.html#section-30",
    "href": "old/CHANGELOG.html#section-30",
    "title": "mm5110.github.io",
    "section": "3.1.2",
    "text": "3.1.2\n\nEnhancements\n\nExplain how to use nav_list helper in documentation.\nReduce left/right padding on smaller screens to increase width of main content column.\n\n\n\nBug Fixes\n\nFix alignment issues with related posts #273 and “Follow” button in author profile #274."
  },
  {
    "objectID": "old/CHANGELOG.html#section-31",
    "href": "old/CHANGELOG.html#section-31",
    "title": "mm5110.github.io",
    "section": "3.1.1",
    "text": "3.1.1\n\nBug Fix\n\nFixed reading time bug when words_per_minute wasn’t set in _config.yml #271"
  },
  {
    "objectID": "old/CHANGELOG.html#section-32",
    "href": "old/CHANGELOG.html#section-32",
    "title": "mm5110.github.io",
    "section": "3.1.0",
    "text": "3.1.0\n\nEnhancements\n\nUpdated Font Awesome to version 4.6.1\nAdded optional GitHub and Bitbucket links to footer if set on site.author in _config.yml.\n\n\n\nBug Fixes\n\nFixed Bitbucket URL typo in author sidebar."
  },
  {
    "objectID": "old/CHANGELOG.html#section-33",
    "href": "old/CHANGELOG.html#section-33",
    "title": "mm5110.github.io",
    "section": "3.0.3",
    "text": "3.0.3\n\nEnhancements\n\nRebuilt the entire theme: layouts, includes, stylesheets, scripts, you name it.\nRefreshed the look and feel while staying true to the original design of the theme (author sidebar/main content).\nReplaced grid system with Susy.\nReplaced Grunt tasks with npm scripts.\nRemoved Google Fonts and replaced with system fonts to improve performance (they can be added back if desired)\nGreatly improved theme documentation.\nIncreased the amount of sample posts, sample pages, and sample collections to throughly test the theme and edge-cases.\nMoved all sample content and assets out of master to keep it as clean as possible for forking.\nAdded new layouts for splash pages, archives for jekyll-archives if enabled, and compress.html to improve performance.\nAdded taxonomy links to posts (tags and categories).\nAdded optional “reading time” meta data.\nImproved Liquid used for Twitter Cards and Open Graph data in &lt;head&gt;.\nImproved gallery include helper and added feature_row for use with splash page layout.\nAdded Keybase.io, author web URI, and Bitbucket optional links to sidebar.\nAdd feed.xml link to footer.\nAdded a UI text data file to easily change all text found in the theme.\nAdded LinkedIn to optional social share buttons.\nAdded Facebook, Google+, and custom commenting options in addition to Disqus.\nAdded optional breadcrumb links."
  },
  {
    "objectID": "old/CHANGELOG.html#section-34",
    "href": "old/CHANGELOG.html#section-34",
    "title": "mm5110.github.io",
    "section": "2.2.1",
    "text": "2.2.1"
  },
  {
    "objectID": "old/CHANGELOG.html#section-35",
    "href": "old/CHANGELOG.html#section-35",
    "title": "mm5110.github.io",
    "section": "2.2.0",
    "text": "2.2.0\n\nEnhancements\n\nAdd support for Jekyll 3.0\nMinor updates to syntax highlighting CSS and theme documentation"
  },
  {
    "objectID": "old/CHANGELOG.html#section-36",
    "href": "old/CHANGELOG.html#section-36",
    "title": "mm5110.github.io",
    "section": "2.1.3",
    "text": "2.1.3\n\nEnhancements\n\nCleaner print styles that remove the top navigation, social sharing buttons, and other elements not needed when printed."
  },
  {
    "objectID": "old/CHANGELOG.html#section-37",
    "href": "old/CHANGELOG.html#section-37",
    "title": "mm5110.github.io",
    "section": "2.1.2",
    "text": "2.1.2\n\nEnhancements\n\nAdd optional CodePen icon/url to author side bar #156\nDocumented Stackoverflow username explanation in _config.yml #157\nSimplified Liquid in post-index.html to better handle year listings #166\n\n\n\nBug Fixes\n\nCleanup Facebook related Open Graph meta tags #149\nCorrected minor typos #158 #175"
  },
  {
    "objectID": "old/CHANGELOG.html#section-38",
    "href": "old/CHANGELOG.html#section-38",
    "title": "mm5110.github.io",
    "section": "2.1.1",
    "text": "2.1.1\n\nEnhancements\n\nAdd optional XING profile link to author sidebar\nInclude open graph meta tags for feature image (if assigned) #149\nCreate an include for feed footer\n\n\n\nBug Fixes\n\nRemove http protocol from Google search form on sample 404 page\nOnly show related posts if there are one or more available\nFix alignment of email address link in author sidebar"
  },
  {
    "objectID": "old/CHANGELOG.html#section-39",
    "href": "old/CHANGELOG.html#section-39",
    "title": "mm5110.github.io",
    "section": "2.1.0",
    "text": "2.1.0\n\nEnhancements\n\nAdd optional social sharing buttons (#42)\n\n\n\n\nsocial sharing buttons\n\n\n\nAdd Soundcloud, YouTube (#95), Flickr (#119), and Weibo (#116) icons for use in author sidebar.\nFix typos in posts and documentation and remove references to Less\nInclude note about Octopress gem being optional\nPost author override support extended to the Atom feed (#71)\nOnly include email address in feed if specified in _config.yml or author _data\nWrap all page content in #main to harmonize article and post index styles (#86)\nInclude new sample feature images for posts and pages\nTable of contents improvements: fix collapse toggle, indent nested elements, show on small screens, and create an _include for reusing in posts and pages.\nInclude note about running Jekyll with bundle exec when using Bundler\nFix home page path in top navigation\nRemove Google Authorship (#120)\nRemove duplicate author content that displayed in div.article-author-bottom\nRemoved unused _sass/print.scss styles\nImprove comments in .scss files"
  },
  {
    "objectID": "old/CHANGELOG.html#section-40",
    "href": "old/CHANGELOG.html#section-40",
    "title": "mm5110.github.io",
    "section": "2.0.0",
    "text": "2.0.0"
  },
  {
    "objectID": "old/CHANGELOG.html#section-41",
    "href": "old/CHANGELOG.html#section-41",
    "title": "mm5110.github.io",
    "section": "1.3.3",
    "text": "1.3.3\n\nEnhancements\n\nAdded new icons and profile links for Stackoverflow, Dribbble, Pinterest, Foursquare, and Steam to the author bio sidebar.\nCleaned up the Kramdown auto table of contents styling to be more readable\nRemoved page width specific .less stylesheets and created mixins for easier updating\nRemoved Modernizr since it wasn’t being used\nAdded pages to sitemap.xml\nAdded category: to rake new_post task\nMinor typographic changes\n\n\n\nBug Fixes\n\nCorrected various broken links in README and Theme Setup."
  },
  {
    "objectID": "old/CHANGELOG.html#section-42",
    "href": "old/CHANGELOG.html#section-42",
    "title": "mm5110.github.io",
    "section": "1.3.1",
    "text": "1.3.1\n\nEnhancements\n\nCleaned up table of contents styling\nReworked top navigation to be a better experience on small screens. Nav items now display vertically when the menu button is tapped, revealing links with larger touch targets.\n\n\n\n\nmenu animation"
  },
  {
    "objectID": "old/CHANGELOG.html#section-43",
    "href": "old/CHANGELOG.html#section-43",
    "title": "mm5110.github.io",
    "section": "1.2.0",
    "text": "1.2.0\n\nBug Fixes\n\nTable weren’t filling the entire width of the content container. They now scale at 100%. Thanks @dhruvbhatia\n\n\n\nEnhancements\n\nDecreased spacing between Markdown footnotes\nRemoved dark background on footer\nRemoved UPPERCASE styling on post titles in the index listing"
  },
  {
    "objectID": "old/CHANGELOG.html#section-44",
    "href": "old/CHANGELOG.html#section-44",
    "title": "mm5110.github.io",
    "section": "1.1.4",
    "text": "1.1.4\n\nBug Fixes\n\nFix top navigation bug issue (#10) for real this time. Remember to clear your floats kids."
  },
  {
    "objectID": "old/CHANGELOG.html#section-45",
    "href": "old/CHANGELOG.html#section-45",
    "title": "mm5110.github.io",
    "section": "1.1.3",
    "text": "1.1.3\n\nBug Fixes\n\nFix top navigation links that weren’t click able on small viewports (Issue #10).\nRemove line wrap from top navigation links that may span multiple lines."
  },
  {
    "objectID": "old/CHANGELOG.html#section-46",
    "href": "old/CHANGELOG.html#section-46",
    "title": "mm5110.github.io",
    "section": "1.1.2",
    "text": "1.1.2\n\nEnhancements\n\nAdded Grunt build script for compiling Less/JavaScript and optimizing image assets.\nAdded support for large image summary Twitter card.\nStylesheet adjustments"
  },
  {
    "objectID": "old/CHANGELOG.html#section-47",
    "href": "old/CHANGELOG.html#section-47",
    "title": "mm5110.github.io",
    "section": "1.1.1",
    "text": "1.1.1\n\nBug Fixes\n\nRemoved Typeplate styles. Was causing issues with newer versions of Less and is no longer maintained.\n\n\n\nEnhancements\n\nAdded image attribution for post and page feature images.\nAdded 404 page.\nCleaned up various Less variables to better align with naming conventions used in other MM Jekyll themes.\nRemoved Chrome Frame references.\nAdded global CSS3 transitions to text and block elements.\nImproved typography in a few places."
  },
  {
    "objectID": "old/CHANGELOG.html#section-48",
    "href": "old/CHANGELOG.html#section-48",
    "title": "mm5110.github.io",
    "section": "1.0.2",
    "text": "1.0.2\n\nEnhancements\n\nGoogle Analytics, Google Authorship, webmaster verifies, and Twitter card meta are now optional."
  },
  {
    "objectID": "old/CHANGELOG.html#section-49",
    "href": "old/CHANGELOG.html#section-49",
    "title": "mm5110.github.io",
    "section": "1.0.1",
    "text": "1.0.1"
  },
  {
    "objectID": "old/CONTRIBUTING.html",
    "href": "old/CONTRIBUTING.html",
    "title": "mm5110.github.io",
    "section": "",
    "text": "Contributions are welcome! Please add issues and make pull requests. There are no stupid questions. All ideas are welcome. This is a volunteer project. Be excellent to each other.\nFork from master and go from there. This repository is intended to remain a generic, ready-to-fork template that demonstrates the features of academicpages.\nIf you make a pull request and change code, please make sure there is a closed issue tagged with ‘code change’ that has some comment linking to either the single commit (if the change was just one commit) or a diff comparing before/after the change (see issue 21 for example). This is so that those who have forked this repo and modified it for their purposes can more easily patch bugs and new features."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael Murray",
    "section": "",
    "text": "Research overview\nI use maths to better understand and improve deep learning algorithms.\n\nTheoretical foundations: develop theory to reconcile modern phenomena with conventional machine learning wisdom.\nImproving efficiency: use intuition from theory to identify algorithmic innovations to lower training and inference costs.\n\n\n\nBio Sketch\n\nLecturer in Mathematics, University of Bath (September 2024 - current)\nHedrick Assistant Adjunct Professor, mentored by Deanna Needell and Guido Montufar, UCLA (July 2021 - June 2024)\nDPhil in Mathematics, supervised by Jared Tanner, University of Oxford\n(September 2017 - June 2021)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Research",
    "section": "",
    "text": "I use maths to better understand and improve the performance of algorithms for machine learning and data science, notably deep learning. I am particularly interested in deep learning for two reasons: first because it offers state-of-the-art performance across so many important applications (e.g., natural language processing, computer vision, drug prediction etc.), and second because the fact that it works so well challenges a number of aspects of conventional machine learning wisdom. I think mathematics will allow us to not only better understand deep learning, thereby helping us extract and generalize principles for successful learning systems, but will also play a crucial role in making such systems safer, more reliable and less costly to train and use in terms of time, energy and memory."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Mathematics of Machine Learning (MA50263)"
  },
  {
    "objectID": "projects.html#principles-for-neural-network-architecture-and-design",
    "href": "projects.html#principles-for-neural-network-architecture-and-design",
    "title": "Research",
    "section": "Principles for neural network architecture and design",
    "text": "Principles for neural network architecture and design\nXXXX\nMy research is mostly concerned with using maths to better understand and improve deep learning algorithms. Examples of specific topics include the role of overparameterization, principled design of network architectures and transitions between benign and non-benign forms of overfitting. See the projects page for current and past projects are available here and an up-to-date list of publications on Google scholar."
  },
  {
    "objectID": "projects.html#research-perspective",
    "href": "projects.html#research-perspective",
    "title": "Research",
    "section": "",
    "text": "My work concerns using maths to better understand and improve the performance of algorithms for machine learning and data science. I am particularly interested in deep learning for two reasons: first because it offers game-changing performance across so many important applications (e.g., natural language processing, computer vision, drug prediction…), and second because the fact that it works so well challenges a number of aspects of conventional machine learning wisdom. I think mathematics will allow us to not only better understand deep learning, thereby helping us extract and generalize principles for successful learning systems, but will also play a crucial role in making it safer, more reliable and less costly to train and use in terms of time, energy and memory."
  },
  {
    "objectID": "projects.html#current-projects",
    "href": "projects.html#current-projects",
    "title": "Research",
    "section": "Current projects",
    "text": "Current projects\n\nImplict and explicit regularization of denoising neural networks\n\n\nGeometric NTK\n\n\nBenign and tempered overfitting: linearly seperable data models and beyond\n\n\nTraining guarantees in the mildly overparameterized regime"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "On some elementary properties of Hopfield networks\n\n\n\n\n\n\n\nMatrices\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2024\n\n\nMichael Murray\n\n\n\n\n\n\n  \n\n\n\n\nA simple neural network with a benign loss landscape\n\n\n\n\n\n\n\nNeural Networks\n\n\nOptimization\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\nMichael Murray\n\n\n\n\n\n\n  \n\n\n\n\nGlobal optimization guarantees for shallow neural networks via the Perceptron algorithm\n\n\n\n\n\n\n\nNeural Networks\n\n\nOptimization\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nMichael Murray\n\n\n\n\n\n\n  \n\n\n\n\nOptimization guarantees for neural networks via the NTK\n\n\n\n\n\n\n\nNeural Networks\n\n\nOptimization\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nMichael Murray\n\n\n\n\n\n\n  \n\n\n\n\nConcentration of Gibbs measures onto minimizers of objective functions\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nMichael Murray\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gibbs/index.html",
    "href": "posts/gibbs/index.html",
    "title": "Concentration of Gibbs measures onto minimizers of objective functions",
    "section": "",
    "text": "Under what conditions do Gibbs measures concentrate on the minimizers of an associated objective function in the inverse temperature limit?"
  },
  {
    "objectID": "posts/gibbs/index.html#introduction",
    "href": "posts/gibbs/index.html#introduction",
    "title": "Concentration of Gibbs measures onto minimizers of objective functions",
    "section": "Introduction",
    "text": "Introduction\nIn many situations encountered in machine learning one is confronted with minimizing some parameterized objective or loss function which depends on data. For example, consider finding the smallest eigenvalue of a data covariance matrix \\({\\mathbf{M}}\\) by minimizing the objective \\(E({\\mathbf{w}}) = \\frac{1}{2}{\\mathbf{w}}^T {\\mathbf{M}}{\\mathbf{w}}\\) over the unit sphere. One approach to analyzing such problems and which is particularly popular among Physicists is to reduce the problem to that of Gibbs sampling. In particular, in the example above, instead of directly studying the objective \\(E\\) we instead consider a sequence of associated Gibbs measures with densities defined as \\[\np_{\\beta}({\\mathbf{w}}) = \\frac{\\exp(- \\beta E({\\mathbf{w}}))}{\\int_{{\\mathbf{w}}\\in \\mathbb{S}^{d-1}} \\exp \\left( - \\beta E({\\mathbf{w}})\\right) dQ},\n\\] where here \\(Q\\) denotes the uniform measure over the unit sphere in dimension \\(d\\). The kind of statement I’ve often come across, at least in the machine learning related tutorials concerning this technique, is something like “as the free parameter (also called the inverse temperature) \\(\\beta\\) goes to infinity then the limiting Gibbs measure concentrates on the minimizers of the objective function”. The main idea then is that one can analyze and access the set of minimizers of the objective by instead analyzing and sampling from the limiting Gibbs measure (if this exists), which seems pretty neat! However, at least with regards to the ones I have read, many of the tutorials or notes on this subject seem to skip over the technical details: for example, what conditions are required for instance on the objective for the limiting distribution to exist, what type of convergence are we talking about and what does the limiting distribution look like? I was happy to find the answer to these in (Hwang 1980) but personally found the presentation slightly confusing. On the off-chance it might be helpful to others in a similar position I thought I would write down the results in (Hwang 1980) and also write down the proof in little more detail."
  },
  {
    "objectID": "posts/gibbs/index.html#results",
    "href": "posts/gibbs/index.html#results",
    "title": "Concentration of the Gibbs measure on minimizers of energy functions",
    "section": "Results",
    "text": "Results\nTo place ourselves in a general setting we adopt the following definitions and conditions.\n\nLet \\((\\Omega, \\Sigma, Q)\\) be a measure space where \\(Q: \\sigma [0,1]\\) is a probability measure. \nLet \\((\\Omega, d)\\) be a metric space and assume \\(f:(\\Omega,d) \\rightarrow (\\mathbb{R}_{\\geq 0},|\\cdot|)\\) is continuous and \\(\\inf_{x \\in \\Omega} f(x) = 0\\). \nLet \\(N = \\{x \\in \\Omega: f(x) = 0\\}\\) to be the set of minimizers of \\(f\\) and assume \\(N \\neq \\emptyset\\).\nLet \\(P_\\beta\\) for \\(\\beta \\in \\mathbb{R}_{&gt;0}\\) denote a sequence of probability measures with Radon-Nikodym derivative \\[\n\\frac{dP_{\\beta}}{dQ} = \\frac{\\exp(- \\beta f(x))dQ}{\\int_{x \\in\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}.\n\\]\n\nWith these definitions and assumptions in mind we have the following result, equivalent to Proposition and Corollary 2.1 in (Hwang 1980).\n\nTheorem 1 Suppose there exists a probability measure \\(P: \\Sigma \\rightarrow [0,1]\\) such that \\(P_\\beta \\rightarrow P\\) weakly, then \\(P(N) = 1\\).\n\n\nProof. In what follows for some set \\(A \\subseteq \\mathbb{R}_{\\geq 0}\\) we use the notation \\(f^{-1}(A) = \\{x \\in \\Omega: f(x) \\in A \\}\\) for the preimage. We first claim it is possible to construct a strictly decreasing sequence of positive real numbers \\((a_m)_{m\\in \\mathbb{N}}\\) such that \\(a_m \\downarrow 0\\) and \\(P(f^{-1}(\\{ a_m \\}) = 0\\). Indeed, suppose this is not possible, then there must exist a smallest real \\(\\epsilon&gt;0\\) such that \\(P(f^{-1}(\\{ \\epsilon\\})) = 0\\) and \\(P(f^{-1}(\\{ y \\})&gt; 0\\) for all \\(y \\in (0, \\epsilon)\\). Now consider disjoint sets \\((S_k)_{k \\in \\mathbb{N}}\\) of \\(\\Omega\\) defined as \\[\nS_k = \\{x \\in \\Omega: f(x) \\in (0, \\epsilon), (k+1)^{-1} &lt; P(\\{ x\\}) \\leq k^{-1} \\},\n\\]\nthen for any \\(y \\in (0, \\epsilon)\\) there exists a \\(k \\in \\mathbb{N}\\) such that \\(f^{-1}(y) \\in S_k\\) and thus \\(f^{-1}((0, \\epsilon)) = \\bigcup_{k=1}^{\\infty} S_k\\). As the \\(S_k\\) are disjoint and \\(P\\) is a measure this implies \\(P(f^{-1}((0, \\epsilon))) = \\sum_{k=1}^{\\infty} P(S_k)\\). Suppose now there exists a \\(k \\in \\mathbb{N}\\) such that the cardinality of \\(S_k\\) is non-finite, choose a countable subset \\((x_i)_{i=1}^{\\infty}\\) from \\(S_k\\) and observe that \\(P(f^{-1}(x)) &gt; (k+1)^{-1}\\) for any \\(x \\in S_k\\). Then \\[\nP(f^{-1}((0, \\epsilon))) \\geq P(S_k) \\geq (k+1)^{-1} \\sum_{i=1}^{\\infty} 1 =  \\infty\n\\] which is of course a contradiction. Therefore it must hold that \\(S_k\\) is finite for all \\(k \\in \\mathbb{N}\\), which in turn implies \\((f^{-1}((0, \\epsilon)))\\) is at most countable. However, \\(P(f^{-1}(\\{y \\}))&gt;0\\) implies \\(f^{-1}(y) \\neq \\emptyset\\) for all \\(y \\in (0, \\epsilon)\\) and furthermore the sets \\((f^{-1}(\\{ y\\}))_{y \\in (0, \\epsilon)}\\) are evidently disjoint. Therefore, as \\((0, \\epsilon)\\) is uncountable and \\(f^{-1}((0, \\epsilon)) = \\bigcup_{y \\in (0, \\epsilon)} f^{-1}(\\{ y\\})\\) then \\(f^{-1}((0, \\epsilon))\\) must also be uncountable, which is a contradiction. We conclude then that there exists a postitive, strictly decreasing sequence \\(a_m \\downarrow 0\\) such that \\(P(f(x) = a_m) = 0\\) for all \\(m \\in \\mathbb{N}\\).\nRecall a set \\(A\\in \\Sigma\\) is called \\(P\\)-continuous if \\(P(\\partial A) = 0\\) where here \\(\\partial A\\) denotes the boundary of \\(A\\). If \\(P_{\\beta}\\) converges weakly to \\(P\\) then by the Portmanteau Theorem for any \\(P\\)-continuous sets \\(A\\in \\Sigma\\) we have \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(A) = P(A).\n\\] Define \\[\nN'_m = f^{-1}([a_m, \\infty)) = \\{x \\in \\Omega: f(x) \\in [a_m, \\infty) \\}.\n\\] By asssumption \\(f\\) is continuous, therefore \\[\n\\partial N_m'  = \\partial f^{-1}([a_m, \\infty)) \\subseteq f^{-1}(\\partial [a_m, \\infty)) = f^{-1}(\\{a_m \\}).\n\\] As by construction \\(P(f^{-1}(\\{a_m \\})) = 0\\) it follows that \\(P(\\partial N_m') = 0\\) and thus \\(N_m'\\) is a \\(P\\)-continuous set for all \\(m \\in \\mathbb{N}\\). As a result \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = P(N_m')\n\\] Observe \\[\n\\begin{align*}\n  P_\\beta(N_m') &= \\frac{\\int_{N_m'}\\exp(- \\beta f(x))dQ}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\frac{\\exp( -\\beta a_m\\} Q(N_m')}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\left(\\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) \\right)^{-1}.\n\\end{align*}\n\\] As \\[\n\\begin{align*}\n  \\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) &\\geq \\int_{f^{-1}([0,a_m/2])} \\exp \\left( - \\beta (f(x) - a_m)\\right)\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right) Q(f^{-1}([0,a_m/2]))\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right)\n\\end{align*}\n\\] then for any \\(m \\in \\mathbb{N}\\) \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = \\lim_{\\beta \\rightarrow \\infty}\\exp \\left(-\\frac{\\beta a_m}{2} \\right)  = 0.\n\\] As a result, for all \\(m \\in \\mathbb{N}\\) it follows that \\(P(N_m') = 0\\) and therefore \\(\\lim_{m \\rightarrow \\infty} P(N_m') = 0\\). By construction \\((N_m')_{m \\in \\mathbb{N}}\\) is an increasing sequence of sets, \\(N' = \\bigcup_{m = 1}^{\\infty} N_m'\\) and therefore \\(P(N') = \\lim_{m \\rightarrow \\infty} P(N_m')\\). Hence we conclude \\[\nP(N') = \\lim_{m \\rightarrow \\infty} P(N_m') = 0.\n\\]"
  },
  {
    "objectID": "posts/gibbs/index.html#presenting-the-results-of-lap-revisited",
    "href": "posts/gibbs/index.html#presenting-the-results-of-lap-revisited",
    "title": "Concentration of the Gibbs measure on minimizers of energy functions",
    "section": "Presenting the results of (Hwang 1980)",
    "text": "Presenting the results of (Hwang 1980)\nTo place ourselves in a general setting we adopt the following definitions and conditions.\n\nLet \\((\\Omega, \\Sigma, Q)\\) be a measure space where \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra and \\(Q: \\Sigma \\rightarrow [0,1]\\) is a probability measure. \nLet \\((\\Omega, d)\\) be a seperable complete metric space and assume \\(f:(\\Omega,d) \\rightarrow (\\mathbb{R}_{\\geq 0},|\\cdot|)\\) is continuous and \\(\\inf_{x \\in \\Omega} f(x) = 0\\). \nLet \\(N = \\{x \\in \\Omega: f(x) = 0\\}\\) to be the set of minimizers of \\(f\\) and assume \\(N \\neq \\emptyset\\). We denote the compliment of \\(N\\) as \\(N'= \\Omega \\backslash N\\).\nLet \\(P_\\beta\\) for \\(\\beta \\in \\mathbb{R}_{&gt;0}\\) denote a sequence of probability measures with Radon-Nikodym derivative \\[\n\\frac{dP_{\\beta}}{dQ} = \\frac{\\exp(- \\beta f(x))dQ}{\\int_{x \\in\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}.\n\\]\n\nWith these definitions and assumptions in mind we have the following result which is essentially a re-phrased version of Proposition and Corollary 2.1 in (Hwang 1980).\n\nTheorem 1 Suppose there exists a probability measure \\(P: \\Sigma \\rightarrow [0,1]\\) such that \\(P_\\beta \\rightarrow P\\) weakly, then \\(P(N) = 1\\).\n\nBefore we turn to the proof, a few remarks.\n\nWe assume \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra in order to use the Portmanteau Theorem.\nWe assume \\(f\\) is continuous (as all our measures are Borel measures this also implies \\(f\\) is measurable) as this implies \\(\\partial f^{-1}(A) \\subseteq f^{-1}(\\partial A)\\) which we use in the proof.\nWe assume there exists at least one minimizer of \\(f\\) due to Proposition 1 of (Hwang 1980) (note as per comment at the end of the paper this result can be extended from Euclidean space to a complete separable metric space) as this condition is necessary for the the sequence \\((P_{\\beta})\\) to converge weakly.\nIf a minimizer exists we can assume without loss of generality that the minimum of \\(f\\) is \\(0\\) as if instead we consider the Gibbs measure associated with \\(f(x) - \\min_{x \\in \\Omega}f(x)\\) we observe that the additional term cancels due to the normalization factor and therefore these Gibbs measures are the same.\nAlthough we assume weak convergence with additional assumptions, for instance on the underlying metric space, this assumption can be at least moderated. For instance, if we assume in addition that \\((\\Omega, d)\\) is compact then \\((P_{\\beta})\\) is tight and so by Prokhorov’s Theorem we know that \\((P_{\\beta})\\) has at the very least a weakly convergent subsequence.\n\n\nProof. It suffices to show that \\(P(N') = 0\\). We first claim it is possible to construct a strictly decreasing sequence of positive real numbers \\((a_m)_{m\\in \\mathbb{N}}\\) such that \\(a_m \\downarrow 0\\) and \\(P(f^{-1}(\\{ a_m \\}) = 0\\). Suppose then it is not is not possible to construct such a sequence, then there must exist a smallest real \\(\\epsilon&gt;0\\) such that \\(P(f^{-1}(\\{ \\epsilon\\})) = 0\\) and \\(P(f^{-1}(\\{ y \\})&gt; 0\\) for all \\(y \\in (0, \\epsilon)\\). In short this implies that there are an uncountable number of singletons in \\((0, \\epsilon)\\) which have nonzero measure which is ripe for implying a contradiction. There are many ways of proceeding from here, one option is to consider disjoint subsets \\((S_k)_{k \\in \\mathbb{N}}\\) of \\(\\Omega\\) defined as \\[\nS_k = \\{x \\in \\Omega: f(x) \\in (0, \\epsilon), (k+1)^{-1} &lt; P(f(x)\\leq k^{-1} \\},\n\\]\nthen for any \\(y \\in (0, \\epsilon)\\) there exists a \\(k \\in \\mathbb{N}\\) such that \\(f^{-1}(y) \\in S_k\\) and thus \\(f^{-1}((0, \\epsilon)) = \\bigcup_{k=1}^{\\infty} S_k\\). Observe for any \\(x \\in S_k\\) that \\(P(f^{-1}(x)) &gt; (k+1)^{-1}\\), therefore the cardinality of \\(S_k\\) can be at most \\(k+1\\). As a result every \\(S_k\\) must be finite which in turn implies \\((f^{-1}((0, \\epsilon)))\\) is also countable as it is a countable union of finite sets. However, \\(P(f^{-1}(\\{y \\}))&gt;0\\) implies \\(f^{-1}(y) \\neq \\emptyset\\) for all \\(y \\in (0, \\epsilon)\\) and furthermore these preimages of singletons are evidently disjoint. Therefore, as \\((0, \\epsilon)\\) is uncountable and \\(f^{-1}((0, \\epsilon)) = \\bigcup_{y \\in (0, \\epsilon)} f^{-1}(\\{ y\\})\\) then \\(f^{-1}((0, \\epsilon))\\) must also be uncountable, which is a contradiction. We conclude then that there exists a postitive, strictly decreasing sequence of positive reals \\(a_m \\downarrow 0\\) such that \\(P(f(x) = a_m) = 0\\) for all \\(m \\in \\mathbb{N}\\).\nDefine \\[\nN'_m = f^{-1}([a_m, \\infty)) = \\{x \\in \\Omega: f(x) \\in [a_m, \\infty) \\}.\n\\] By asssumption \\(f\\) is continuous, therefore \\[\n\\partial N_m'  = \\partial f^{-1}([a_m, \\infty)) \\subseteq f^{-1}(\\partial [a_m, \\infty)) = f^{-1}(\\{a_m \\}).\n\\] As by construction \\(P(f^{-1}(\\{a_m \\})) = 0\\) it follows that \\(P(\\partial N_m') = 0\\) and thus \\(N_m'\\) is a \\(P\\)-continuous set for all \\(m \\in \\mathbb{N}\\). By the Portmanteau Theorem it therefore follows that \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = P(N_m').\n\\] Observe \\[\n\\begin{align*}\n  P_\\beta(N_m') &= \\frac{\\int_{N_m'}\\exp(- \\beta f(x))dQ}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\frac{\\exp( -\\beta a_m\\} Q(N_m')}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\left(\\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) \\right)^{-1}.\n\\end{align*}\n\\] Furthermore, as \\[\n\\begin{align*}\n  \\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) &\\geq \\int_{f^{-1}([0,a_m/2])} \\exp \\left( - \\beta (f(x) - a_m)\\right)\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right) Q(f^{-1}([0,a_m/2]))\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right)\n\\end{align*}\n\\] then it holds that \\(P_\\beta(N_m') \\leq \\exp \\left( - \\frac{\\beta a_m}{2} \\right)\\) and as a result for any \\(m \\in \\mathbb{N}\\) we have \\[\nP(N_m') = \\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = \\lim_{\\beta \\rightarrow \\infty}\\exp \\left(-\\frac{\\beta a_m}{2} \\right)  = 0.\n\\] By construction \\((N_m')_{m \\in \\mathbb{N}}\\) is an increasing sequence of sets, \\(N' = \\bigcup_{m = 1}^{\\infty} N_m'\\) and therefore \\(P(N') = \\lim_{m \\rightarrow \\infty} P(N_m')\\). Hence we conclude \\[\nP(N') = \\lim_{m \\rightarrow \\infty} P(N_m') = 0.\n\\]"
  },
  {
    "objectID": "posts/gibbs/index.html#the-limiting-gibbs-measure-is-supported-on-the-set-of-minimizers",
    "href": "posts/gibbs/index.html#the-limiting-gibbs-measure-is-supported-on-the-set-of-minimizers",
    "title": "Concentration of the Gibbs measure on minimizers of energy functions",
    "section": "The limiting Gibbs measure is supported on the set of minimizers",
    "text": "The limiting Gibbs measure is supported on the set of minimizers\nTo place ourselves in a general setting we adopt the following definitions and conditions.\n\nLet \\((\\Omega, \\Sigma, Q)\\) be a measure space where \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra and \\(Q: \\Sigma \\rightarrow [0,1]\\) is a probability measure. \nLet \\((\\Omega, d)\\) be a seperable complete metric space and assume \\(f:(\\Omega,d) \\rightarrow (\\mathbb{R}_{\\geq 0},|\\cdot|)\\) is continuous and \\(\\inf_{x \\in \\Omega} f(x) = 0\\). \nLet \\(N = \\{x \\in \\Omega: f(x) = 0\\}\\) to be the set of minimizers of \\(f\\) and assume \\(N \\neq \\emptyset\\). We denote the compliment of \\(N\\) as \\(N'= \\Omega \\backslash N\\).\nLet \\(P_\\beta\\) for \\(\\beta \\in \\mathbb{R}_{&gt;0}\\) denote a sequence of probability measures with Radon-Nikodym derivative \\[\n\\frac{dP_{\\beta}}{dQ} = \\frac{\\exp(- \\beta f(x))dQ}{\\int_{x \\in\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}.\n\\]\n\nWith these definitions and assumptions in mind we have the following result which is essentially a re-phrased version of Proposition and Corollary 2.1 in (Hwang 1980).\n\nTheorem 1 Suppose there exists a probability measure \\(P: \\Sigma \\rightarrow [0,1]\\) such that \\(P_\\beta \\rightarrow P\\) weakly, then \\(P(N) = 1\\).\n\nBefore we turn to the proof, a few remarks.\n\nWe assume \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra in order to use the Portmanteau Theorem.\nWe assume \\(f\\) is continuous (as all our measures are Borel measures this also implies \\(f\\) is measurable) as this implies \\(\\partial f^{-1}(A) \\subseteq f^{-1}(\\partial A)\\) which we use in the proof.\nWe assume there exists at least one minimizer of \\(f\\) due to Proposition 1 of (Hwang 1980) (note as per comment at the end of the paper this result can be extended from Euclidean space to a complete separable metric space) as this condition is necessary for the the sequence \\((P_{\\beta})\\) to converge weakly.\nIf a minimizer exists we can assume without loss of generality that the minimum of \\(f\\) is \\(0\\) as if instead we consider the Gibbs measure associated with \\(f(x) - \\min_{x \\in \\Omega}f(x)\\) we observe that the additional term cancels due to the normalization factor and therefore these Gibbs measures are the same.\nAlthough we assume weak convergence with additional assumptions, for instance on the underlying metric space, this assumption can be at least moderated. For instance, if we assume in addition that \\((\\Omega, d)\\) is compact then \\((P_{\\beta})\\) is tight and so by Prokhorov’s Theorem we know that \\((P_{\\beta})\\) has at the very least a weakly convergent subsequence.\n\n\nProof. It suffices to show that \\(P(N') = 0\\). We first claim it is possible to construct a strictly decreasing sequence of positive real numbers \\((a_m)_{m\\in \\mathbb{N}}\\) such that \\(a_m \\downarrow 0\\) and \\(P(f^{-1}(\\{ a_m \\}) = 0\\). Suppose then it is not is not possible to construct such a sequence, then there must exist a smallest real \\(\\epsilon&gt;0\\) such that \\(P(f^{-1}(\\{ \\epsilon\\})) = 0\\) and \\(P(f^{-1}(\\{ y \\})&gt; 0\\) for all \\(y \\in (0, \\epsilon)\\). In short this implies that there are an uncountable number of singletons in \\((0, \\epsilon)\\) which have nonzero measure which is ripe for implying a contradiction. There are many ways of proceeding from here, one option is to consider disjoint subsets \\((S_k)_{k \\in \\mathbb{N}}\\) of \\(\\Omega\\) defined as \\[\nS_k = \\{x \\in \\Omega: f(x) \\in (0, \\epsilon), (k+1)^{-1} &lt; P(f(x)\\leq k^{-1} \\},\n\\]\nthen for any \\(y \\in (0, \\epsilon)\\) there exists a \\(k \\in \\mathbb{N}\\) such that \\(f^{-1}(y) \\in S_k\\) and thus \\(f^{-1}((0, \\epsilon)) = \\bigcup_{k=1}^{\\infty} S_k\\). Observe for any \\(x \\in S_k\\) that \\(P(f^{-1}(x)) &gt; (k+1)^{-1}\\), therefore the cardinality of \\(S_k\\) can be at most \\(k+1\\). As a result every \\(S_k\\) must be finite which in turn implies \\((f^{-1}((0, \\epsilon)))\\) is also countable as it is a countable union of finite sets. However, \\(P(f^{-1}(\\{y \\}))&gt;0\\) implies \\(f^{-1}(y) \\neq \\emptyset\\) for all \\(y \\in (0, \\epsilon)\\) and furthermore these preimages of singletons are evidently disjoint. Therefore, as \\((0, \\epsilon)\\) is uncountable and \\(f^{-1}((0, \\epsilon)) = \\bigcup_{y \\in (0, \\epsilon)} f^{-1}(\\{ y\\})\\) then \\(f^{-1}((0, \\epsilon))\\) must also be uncountable, which is a contradiction. We conclude then that there exists a postitive, strictly decreasing sequence of positive reals \\(a_m \\downarrow 0\\) such that \\(P(f(x) = a_m) = 0\\) for all \\(m \\in \\mathbb{N}\\).\nDefine \\[\nN'_m = f^{-1}([a_m, \\infty)) = \\{x \\in \\Omega: f(x) \\in [a_m, \\infty) \\}.\n\\] By asssumption \\(f\\) is continuous, therefore \\[\n\\partial N_m'  = \\partial f^{-1}([a_m, \\infty)) \\subseteq f^{-1}(\\partial [a_m, \\infty)) = f^{-1}(\\{a_m \\}).\n\\] As by construction \\(P(f^{-1}(\\{a_m \\})) = 0\\) it follows that \\(P(\\partial N_m') = 0\\) and thus \\(N_m'\\) is a \\(P\\)-continuous set for all \\(m \\in \\mathbb{N}\\). By the Portmanteau Theorem it therefore follows that \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = P(N_m').\n\\] Observe \\[\n\\begin{align*}\n  P_\\beta(N_m') &= \\frac{\\int_{N_m'}\\exp(- \\beta f(x))dQ}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\frac{\\exp( -\\beta a_m\\} Q(N_m')}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\left(\\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) \\right)^{-1}.\n\\end{align*}\n\\] Furthermore, as \\[\n\\begin{align*}\n  \\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) &\\geq \\int_{f^{-1}([0,a_m/2])} \\exp \\left( - \\beta (f(x) - a_m)\\right)\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right) Q(f^{-1}([0,a_m/2]))\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right)\n\\end{align*}\n\\] then it holds that \\(P_\\beta(N_m') \\leq \\exp \\left( - \\frac{\\beta a_m}{2} \\right)\\) and as a result for any \\(m \\in \\mathbb{N}\\) we have \\[\nP(N_m') = \\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = \\lim_{\\beta \\rightarrow \\infty}\\exp \\left(-\\frac{\\beta a_m}{2} \\right)  = 0.\n\\] By construction \\((N_m')_{m \\in \\mathbb{N}}\\) is an increasing sequence of sets, \\(N' = \\bigcup_{m = 1}^{\\infty} N_m'\\) and therefore \\(P(N') = \\lim_{m \\rightarrow \\infty} P(N_m')\\). Hence we conclude \\[\nP(N') = \\lim_{m \\rightarrow \\infty} P(N_m') = 0.\n\\]"
  },
  {
    "objectID": "posts/gibbs/index.html#proving-the-limiting-gibbs-measure-if-it-exists-is-supported-on-the-set-of-minimizers",
    "href": "posts/gibbs/index.html#proving-the-limiting-gibbs-measure-if-it-exists-is-supported-on-the-set-of-minimizers",
    "title": "Concentration of the Gibbs measure on minimizers of energy functions",
    "section": "Proving the limiting Gibbs measure (if it exists) is supported on the set of minimizers",
    "text": "Proving the limiting Gibbs measure (if it exists) is supported on the set of minimizers\nTo place ourselves in a general setting we adopt the following definitions and conditions.\n\nLet \\((\\Omega, \\Sigma, Q)\\) be a measure space where \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra and \\(Q: \\Sigma \\rightarrow [0,1]\\) is a probability measure. \nLet \\((\\Omega, d)\\) be a seperable complete metric space and assume \\(f:(\\Omega,d) \\rightarrow (\\mathbb{R}_{\\geq 0},|\\cdot|)\\) is continuous and \\(\\inf_{x \\in \\Omega} f(x) = 0\\). \nLet \\(N = \\{x \\in \\Omega: f(x) = 0\\}\\) to be the set of minimizers of \\(f\\) and assume \\(N \\neq \\emptyset\\). We denote the compliment of \\(N\\) as \\(N'= \\Omega \\backslash N\\).\nLet \\(P_\\beta\\) for \\(\\beta \\in \\mathbb{R}_{&gt;0}\\) denote a sequence of probability measures with Radon-Nikodym derivative \\[\n\\frac{dP_{\\beta}}{dQ} = \\frac{\\exp(- \\beta f(x))dQ}{\\int_{x \\in\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}.\n\\]\n\nWith these definitions and assumptions in mind we have the following result which is essentially a re-phrased version of Proposition and Corollary 2.1 in (Hwang 1980).\n\nTheorem 1 Suppose there exists a probability measure \\(P: \\Sigma \\rightarrow [0,1]\\) such that \\(P_\\beta \\rightarrow P\\) weakly, then \\(P(N) = 1\\).\n\nBefore we turn to the proof, a few remarks.\n\nWe assume \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra in order to use the Portmanteau Theorem.\nWe assume \\(f\\) is continuous (as all our measures are Borel measures this also implies \\(f\\) is measurable) as this implies \\(\\partial f^{-1}(A) \\subseteq f^{-1}(\\partial A)\\) which we use in the proof.\nWe assume there exists at least one minimizer of \\(f\\) due to Proposition 1 of (Hwang 1980) (note as per comment at the end of the paper this result can be extended from Euclidean space to a complete separable metric space) as this condition is necessary for the the sequence \\((P_{\\beta})\\) to converge weakly.\nIf a minimizer exists we can assume without loss of generality that the minimum of \\(f\\) is \\(0\\) as if instead we consider the Gibbs measure associated with \\(f(x) - \\min_{x \\in \\Omega}f(x)\\) we observe that the additional term cancels due to the normalization factor and therefore these Gibbs measures are the same.\nAlthough we assume weak convergence with additional assumptions, for instance on the underlying metric space, this assumption can be at least moderated. For instance, if we assume in addition that \\((\\Omega, d)\\) is compact then \\((P_{\\beta})\\) is tight and so by Prokhorov’s Theorem we know that \\((P_{\\beta})\\) has at the very least a weakly convergent subsequence.\n\n\nProof. It suffices to show that \\(P(N') = 0\\). We first claim it is possible to construct a strictly decreasing sequence of positive real numbers \\((a_m)_{m\\in \\mathbb{N}}\\) such that \\(a_m \\downarrow 0\\) and \\(P(f^{-1}(\\{ a_m \\}) = 0\\). Suppose then it is not is not possible to construct such a sequence, then there must exist a smallest real \\(\\epsilon&gt;0\\) such that \\(P(f^{-1}(\\{ \\epsilon\\})) = 0\\) and \\(P(f^{-1}(\\{ y \\})&gt; 0\\) for all \\(y \\in (0, \\epsilon)\\). In short this implies that there are an uncountable number of singletons in \\((0, \\epsilon)\\) which have nonzero measure which is ripe for implying a contradiction. There are many ways of proceeding from here, one option is to consider disjoint subsets \\((S_k)_{k \\in \\mathbb{N}}\\) of \\(\\Omega\\) defined as \\[\nS_k = \\{x \\in \\Omega: f(x) \\in (0, \\epsilon), (k+1)^{-1} &lt; P(f(x)\\leq k^{-1} \\},\n\\]\nthen for any \\(y \\in (0, \\epsilon)\\) there exists a \\(k \\in \\mathbb{N}\\) such that \\(f^{-1}(y) \\in S_k\\) and thus \\(f^{-1}((0, \\epsilon)) = \\bigcup_{k=1}^{\\infty} S_k\\). Observe for any \\(x \\in S_k\\) that \\(P(f^{-1}(x)) &gt; (k+1)^{-1}\\), therefore the cardinality of \\(S_k\\) can be at most \\(k+1\\). As a result every \\(S_k\\) must be finite which in turn implies \\((f^{-1}((0, \\epsilon)))\\) is also countable as it is a countable union of finite sets. However, \\(P(f^{-1}(\\{y \\}))&gt;0\\) implies \\(f^{-1}(y) \\neq \\emptyset\\) for all \\(y \\in (0, \\epsilon)\\) and furthermore these preimages of singletons are evidently disjoint. Therefore, as \\((0, \\epsilon)\\) is uncountable and \\(f^{-1}((0, \\epsilon)) = \\bigcup_{y \\in (0, \\epsilon)} f^{-1}(\\{ y\\})\\) then \\(f^{-1}((0, \\epsilon))\\) must also be uncountable, which is a contradiction. We conclude then that there exists a postitive, strictly decreasing sequence of positive reals \\(a_m \\downarrow 0\\) such that \\(P(f(x) = a_m) = 0\\) for all \\(m \\in \\mathbb{N}\\).\nDefine \\[\nN'_m = f^{-1}([a_m, \\infty)) = \\{x \\in \\Omega: f(x) \\in [a_m, \\infty) \\}.\n\\] By asssumption \\(f\\) is continuous, therefore \\[\n\\partial N_m'  = \\partial f^{-1}([a_m, \\infty)) \\subseteq f^{-1}(\\partial [a_m, \\infty)) = f^{-1}(\\{a_m \\}).\n\\] As by construction \\(P(f^{-1}(\\{a_m \\})) = 0\\) it follows that \\(P(\\partial N_m') = 0\\) and thus \\(N_m'\\) is a \\(P\\)-continuous set for all \\(m \\in \\mathbb{N}\\). By the Portmanteau Theorem it therefore follows that \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = P(N_m').\n\\] Observe \\[\n\\begin{align*}\n  P_\\beta(N_m') &= \\frac{\\int_{N_m'}\\exp(- \\beta f(x))dQ}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\frac{\\exp( -\\beta a_m\\} Q(N_m')}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\left(\\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) \\right)^{-1}.\n\\end{align*}\n\\] Furthermore, as \\[\n\\begin{align*}\n  \\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) &\\geq \\int_{f^{-1}([0,a_m/2])} \\exp \\left( - \\beta (f(x) - a_m)\\right)\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right) Q(f^{-1}([0,a_m/2]))\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right)\n\\end{align*}\n\\] then it holds that \\(P_\\beta(N_m') \\leq \\exp \\left( - \\frac{\\beta a_m}{2} \\right)\\) and as a result for any \\(m \\in \\mathbb{N}\\) we have \\[\nP(N_m') = \\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = \\lim_{\\beta \\rightarrow \\infty}\\exp \\left(-\\frac{\\beta a_m}{2} \\right)  = 0.\n\\] By construction \\((N_m')_{m \\in \\mathbb{N}}\\) is an increasing sequence of sets, \\(N' = \\bigcup_{m = 1}^{\\infty} N_m'\\) and therefore \\(P(N') = \\lim_{m \\rightarrow \\infty} P(N_m')\\). Hence we conclude \\[\nP(N') = \\lim_{m \\rightarrow \\infty} P(N_m') = 0.\n\\]"
  },
  {
    "objectID": "posts/gibbs/index.html#proving-the-limiting-gibbs-measure-assuming-it-exists-is-supported-on-the-set-of-minimizers",
    "href": "posts/gibbs/index.html#proving-the-limiting-gibbs-measure-assuming-it-exists-is-supported-on-the-set-of-minimizers",
    "title": "Concentration of Gibbs measures onto minimizers of objective functions",
    "section": "Proving the limiting Gibbs measure (assuming it exists) is supported on the set of minimizers",
    "text": "Proving the limiting Gibbs measure (assuming it exists) is supported on the set of minimizers\nTo place ourselves in a general setting we adopt the following definitions and conditions.\n\nLet \\((\\Omega, \\Sigma, Q)\\) be a measure space where \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra and \\(Q: \\Sigma \\rightarrow [0,1]\\) is a probability measure. \nLet \\((\\Omega, d)\\) be a seperable complete metric space and assume \\(f:(\\Omega,d) \\rightarrow (\\mathbb{R}_{\\geq 0},|\\cdot|)\\) is continuous and \\(\\inf_{x \\in \\Omega} f(x) = 0\\). \nLet \\(N = \\{x \\in \\Omega: f(x) = 0\\}\\) to be the set of minimizers of \\(f\\) and assume \\(N \\neq \\emptyset\\). We denote the compliment of \\(N\\) as \\(N'= \\Omega \\backslash N\\).\nLet \\(P_\\beta\\) for \\(\\beta \\in \\mathbb{R}_{&gt;0}\\) denote a sequence of probability measures with Radon-Nikodym derivative \\[\n\\frac{dP_{\\beta}}{dQ} = \\frac{\\exp(- \\beta f(x))dQ}{\\int_{x \\in\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}.\n\\]\n\nWith these definitions and assumptions in mind we have the following result which is essentially a re-phrased version of Proposition and Corollary 2.1 in (Hwang 1980).\n\nTheorem 1 Suppose there exists a probability measure \\(P: \\Sigma \\rightarrow [0,1]\\) such that \\(P_\\beta \\rightarrow P\\) weakly, then \\(P(N) = 1\\).\n\nBefore we turn to the proof, a few remarks.\n\nWe assume \\(\\Sigma\\) is a Borel-\\(\\sigma\\) algebra in order to use the Portmanteau Theorem.\nWe assume \\(f\\) is continuous (as all our measures are Borel measures this also implies \\(f\\) is measurable) as this implies \\(\\partial f^{-1}(A) \\subseteq f^{-1}(\\partial A)\\) which we use in the proof.\nWe assume there exists at least one minimizer of \\(f\\) due to Proposition 1 of (Hwang 1980) (note as per comment at the end of the paper this result can be extended from Euclidean space to a complete separable metric space) as this condition is necessary for the the sequence \\((P_{\\beta})\\) to converge weakly.\nIf a minimizer exists we can assume without loss of generality that the minimum of \\(f\\) is \\(0\\) as if instead we consider the Gibbs measure associated with \\(f(x) - \\min_{x \\in \\Omega}f(x)\\) we observe that the additional term cancels due to the normalization factor and therefore these Gibbs measures are the same.\nAlthough we assume weak convergence with additional assumptions, for instance on the underlying metric space, this assumption can be at least moderated. For instance, if we assume in addition that \\((\\Omega, d)\\) is compact then \\((P_{\\beta})\\) is tight and so by Prokhorov’s Theorem we know that \\((P_{\\beta})\\) has at the very least a weakly convergent subsequence.\n\n\nProof. It suffices to show that \\(P(N') = 0\\). We first claim it is possible to construct a strictly decreasing sequence of positive real numbers \\((a_m)_{m\\in \\mathbb{N}}\\) such that \\(a_m \\downarrow 0\\) and \\(P(f^{-1}(\\{ a_m \\}) = 0\\). Suppose then it is not possible to construct such a sequence, then there must exist a smallest real \\(\\epsilon&gt;0\\) such that \\(P(f^{-1}(\\{ \\epsilon\\})) = 0\\) and \\(P(f^{-1}(\\{ y \\})&gt; 0\\) for all \\(y \\in (0, \\epsilon)\\). In short this implies that there are an uncountable number of singletons in \\((0, \\epsilon)\\) which have nonzero measure which is ripe for implying a contradiction. There are many ways of proceeding from here, one option is to consider disjoint subsets \\((S_k)_{k \\in \\mathbb{N}}\\) of \\(\\Omega\\) defined as \\[\nS_k = \\{x \\in \\Omega: f(x) \\in (0, \\epsilon), (k+1)^{-1} &lt; P(f(x)\\leq k^{-1} \\},\n\\]\nthen for any \\(y \\in (0, \\epsilon)\\) there exists a \\(k \\in \\mathbb{N}\\) such that \\(f^{-1}(y) \\in S_k\\) and thus \\(f^{-1}((0, \\epsilon)) = \\bigcup_{k=1}^{\\infty} S_k\\). Observe for any \\(x \\in S_k\\) that \\(P(f^{-1}(x)) &gt; (k+1)^{-1}\\), therefore the cardinality of \\(S_k\\) can be at most \\(k+1\\). As a result every \\(S_k\\) must be finite which in turn implies \\((f^{-1}((0, \\epsilon)))\\) is also countable as it is a countable union of finite sets. However, \\(P(f^{-1}(\\{y \\}))&gt;0\\) implies \\(f^{-1}(y) \\neq \\emptyset\\) for all \\(y \\in (0, \\epsilon)\\) and furthermore these preimages of singletons are evidently disjoint. Therefore, as \\((0, \\epsilon)\\) is uncountable and \\(f^{-1}((0, \\epsilon)) = \\bigcup_{y \\in (0, \\epsilon)} f^{-1}(\\{ y\\})\\) then \\(f^{-1}((0, \\epsilon))\\) must also be uncountable, which is a contradiction. We conclude then that there exists a postitive, strictly decreasing sequence of positive reals \\(a_m \\downarrow 0\\) such that \\(P(f(x) = a_m) = 0\\) for all \\(m \\in \\mathbb{N}\\).\nDefine \\[\nN'_m = f^{-1}([a_m, \\infty)) = \\{x \\in \\Omega: f(x) \\in [a_m, \\infty) \\}.\n\\] By asssumption \\(f\\) is continuous, therefore \\[\n\\partial N_m'  = \\partial f^{-1}([a_m, \\infty)) \\subseteq f^{-1}(\\partial [a_m, \\infty)) = f^{-1}(\\{a_m \\}).\n\\] As by construction \\(P(f^{-1}(\\{a_m \\})) = 0\\) it follows that \\(P(\\partial N_m') = 0\\) and thus \\(N_m'\\) is a \\(P\\)-continuous set for all \\(m \\in \\mathbb{N}\\). By the Portmanteau Theorem it therefore follows that \\[\n\\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = P(N_m').\n\\] Observe \\[\n\\begin{align*}\n  P_\\beta(N_m') &= \\frac{\\int_{N_m'}\\exp(- \\beta f(x))dQ}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\frac{\\exp( -\\beta a_m\\} Q(N_m')}{\\int_{\\Omega} \\exp \\left( - \\beta f(x)\\right) dQ}\\\\\n  & \\leq \\left(\\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) \\right)^{-1}.\n\\end{align*}\n\\] Furthermore, as \\[\n\\begin{align*}\n  \\int_{\\Omega} \\exp \\left( - \\beta (f(x) - a_m)\\right) &\\geq \\int_{f^{-1}([0,a_m/2])} \\exp \\left( - \\beta (f(x) - a_m)\\right)\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right) Q(f^{-1}([0,a_m/2]))\\\\\n  & \\geq \\exp \\left(\\frac{\\beta a_m}{2} \\right)\n\\end{align*}\n\\] then it holds that \\(P_\\beta(N_m') \\leq \\exp \\left( - \\frac{\\beta a_m}{2} \\right)\\) and as a result for any \\(m \\in \\mathbb{N}\\) we have \\[\nP(N_m') = \\lim_{\\beta \\rightarrow \\infty} P_\\beta(N_m') = \\lim_{\\beta \\rightarrow \\infty}\\exp \\left(-\\frac{\\beta a_m}{2} \\right)  = 0.\n\\] By construction \\((N_m')_{m \\in \\mathbb{N}}\\) is an increasing sequence of sets, \\(N' = \\bigcup_{m = 1}^{\\infty} N_m'\\) and therefore \\(P(N') = \\lim_{m \\rightarrow \\infty} P(N_m')\\). Hence we conclude \\[\nP(N') = \\lim_{m \\rightarrow \\infty} P(N_m') = 0.\n\\]"
  },
  {
    "objectID": "projects.html#past-projects",
    "href": "projects.html#past-projects",
    "title": "Research",
    "section": "Past projects",
    "text": "Past projects"
  },
  {
    "objectID": "projects.html#previous-projects",
    "href": "projects.html#previous-projects",
    "title": "Research",
    "section": "Previous projects",
    "text": "Previous projects\n\nHow does the choice of activation function impact training?\n\n\nEncoder blind compressed sensing"
  },
  {
    "objectID": "teaching.html#current-projects",
    "href": "teaching.html#current-projects",
    "title": "Teaching",
    "section": "Current projects",
    "text": "Current projects"
  },
  {
    "objectID": "teaching.html#current-courses",
    "href": "teaching.html#current-courses",
    "title": "Teaching",
    "section": "",
    "text": "Mathematics of Machine Learning (MA50263)"
  },
  {
    "objectID": "teaching.html#past-courses",
    "href": "teaching.html#past-courses",
    "title": "Teaching",
    "section": "Past courses",
    "text": "Past courses\n\nProgram in Computing (PIC) Instructor, UCLA\n\nIntroduction to Python with Data Science Applications (PIC16A)\nIntroduction programming with C++ (PIC10A)\nIntermediate C++ (PIC10B:)\n\n\n\nTutor and Teaching Assistant, University of Oxford\n\nInformation Theory (B8.4)\nTheories of Deep Learning (C6.5)."
  },
  {
    "objectID": "teaching.html#for-students",
    "href": "teaching.html#for-students",
    "title": "Teaching",
    "section": "For students",
    "text": "For students"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html",
    "href": "posts/opt-guarantees-ntk/index.html",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "",
    "text": "In this post we will look at how the Neural Tangent Kernel (NTK) can be used to derive training guarantees for sufficiently overparameterized neural networks."
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#introduction",
    "href": "posts/opt-guarantees-ntk/index.html#introduction",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this post is to sketch out in the simplest setting possible how the Neural Tangent Kernel (NTK) can be used to derive training guarantees for sufficiently overparameterized neural networks. For now let \\(f: \\mathbb{R}^p \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) be a parameterized function with parameters \\(\\theta \\in \\mathbb{R}^p\\) mapping vectors in \\(\\mathbb{R}^d\\) to a real scalar value. Consider an arbitrary training sample consisting of \\(n\\) pairs of points and their corresponding targets \\(({\\mathbf{x}}_i, y_i)_{i=1}^n \\in (\\mathbb{R}^d \\times \\mathbb{R})^n\\), recall the least squares loss defined as \\[\\begin{align*}\n    L(\\theta) &= \\frac{1}{2}\\sum_{i=1}^n (f(\\theta, {\\mathbf{x}}_i) - y_i)^2.\n\\end{align*}\\] To solve the least squares problem we study the trajectory through parameter space under gradient flow, a continuous time simplification of gradient descent: simplifying our notation by using \\(L(t)\\) instead of \\(L(\\theta(t))\\), then for \\(t\\geq 0\\) \\[\\begin{equation} \\label{opt2-eq:grad-flow}\n    \\frac{d \\theta(t)}{dt} = - \\nabla_{\\theta} L(t).\n\\end{equation}\\] For now we assume that \\(f\\) is at differentiable with respect to its parameters. With \\({\\mathbf{u}}(t) = [f(\\theta(t), {\\mathbf{x}}_1), f(\\theta(t), {\\mathbf{x}}_2)... f(\\theta(t), {\\mathbf{x}}_n)] \\in \\mathbb{R}^n\\) denoting the vector of predictions and \\({\\mathbf{r}}(t) = {\\mathbf{u}}(t) - {\\mathbf{y}}\\) the vector of residuals at time \\(t \\geq 0\\), then we denote the Jacobian as \\[\\begin{align*}\n{\\mathbf{J}}(t) = \\nabla_\\theta {\\mathbf{r}}(t)= \\nabla_\\theta {\\mathbf{u}}(t) =\n  \\begin{bmatrix}\n    \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_1) }{\\partial \\theta_1} & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_2) }{\\partial \\theta_1} & ... & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_n) }{\\partial \\theta_1} \\\\\n    \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_1) }{\\partial \\theta_2} & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_2) }{\\partial \\theta_2} & ... & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_n) }{\\partial \\theta_2} \\\\\n    \\rvdots & \\rvdots & \\ddots & \\rvdots\\\\\n    \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_1) }{\\partial \\theta_p} & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_2) }{\\partial \\theta_p} & ... & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_n) }{\\partial \\theta_p}\n  \\end{bmatrix}\n  \\in \\mathbb{R}^{p \\times n}.\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#the-relevance-of-the-neural-tangent-kernel-for-training-general-models",
    "href": "posts/opt-guarantees-ntk/index.html#the-relevance-of-the-neural-tangent-kernel-for-training-general-models",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "The relevance of the Neural Tangent Kernel for training general models",
    "text": "The relevance of the Neural Tangent Kernel for training general models\nThe Jacobian and its gram, \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t) \\in \\mathbb{R}^{n \\times n}\\), which is also referred to as the Neural Tangent Kernel (NTK) gram matrix, will play a critical role in what follows here and also when it comes to studying linearized neural networks. Note calling \\({\\mathbf{H}}(t)\\) the NTK or NTK gram matrix is somewhat misleading as it can be studied more generally for any sufficiently smooth model, not just neural networks! However, as it is now accepted terminology we will stick with it. The following proposition illustrates the significance of this matrix for training.\n\nProposition 1 Assume \\(f\\) is differentiable with respect to its parameters \\(\\theta \\in \\mathbb{R}^p\\) at all points along the trajectory of gradient flow. Then \\[\\begin{align*}\n    \\frac{d {\\mathbf{r}}(t)}{dt} = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] \n\n\nProof. Observe that as \\[\\begin{align*}\n        \\frac{\\partial L(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta, {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i)\n    \\end{align*}\\] then collecting terms we have \\[\n    \\nabla_{\\theta} L(\\theta(t)) =  {\\mathbf{J}}(t){\\mathbf{r}}(t).\n    \\] Noting that \\(f\\) is a function of \\(p\\) parameters which each depend on \\(t\\), then from the chain rule it follows that \\[\\begin{align*}\n        \\frac{d u_i(t)}{dt }   =  \\frac{d f(\\theta(t), {\\mathbf{x}}_i)}{dt}\n        = \\sum_{k = 1}^p \\frac{d \\theta_k}{dt} \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{\\partial \\theta_k}\n        = \\nabla_{\\theta} f(\\theta(t), {\\mathbf{x}}_i)^T  \\left(\\frac{d \\theta(t)}{dt}\\right).\n    \\end{align*}\\] Again collecting terms and substituting the expression for gradient flow given in we have \\[\\begin{align*}\n        \\frac{d {\\mathbf{u}}(t)}{dt} = {\\mathbf{J}}(t)^T \\frac{d \\theta(t)}{dt}\n        = - {\\mathbf{J}}(t)^T \\nabla_{\\theta}L(t) = - {\\mathbf{J}}(t)^T{\\mathbf{J}}(t){\\mathbf{r}}(t) = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] To finish observe \\(\\frac{d {\\mathbf{u}}(t)}{dt} = \\frac{d {\\mathbf{r}}(t)}{dt}\\).\n\nThe following lemma hints at how we might be able to use Proposition 1 to derive guarantees for training, note here we use \\(\\lambda_i({\\mathbf{A}})\\) to denote the \\(ith\\) eigenvalue of a matrix \\({\\mathbf{A}}\\in \\mathbb{C}^{n \\times n}\\) where \\(\\lambda_1({\\mathbf{A}}) \\geq \\lambda_2({\\mathbf{A}}) \\geq ... \\lambda_n({\\mathbf{A}})\\).\n\nLemma 1 For some \\(T\\in \\mathbb{R}_{\\geq 0}\\), suppose for all \\(t \\in [0,T]\\) there exists a constant \\(\\kappa \\geq 0\\) such that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\kappa}{2}\\). Suppose \\(f\\) is differentiable along the trajectory of the gradient flow. Then for all \\(t \\in [0, T]\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\kappa t) L(0).\n\\end{align*}\\] \n\nIndeed, Lemma 1 suggests that arbitrarily small training error can be guaranteed as long as we can bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) above zero for sufficiently long enough.\n\nProof. Observe by definition that \\(L(t) = ||{\\mathbf{r}}(t) ||_2^2\\). Using Lemma 1 it follows that \\[\\begin{align*}\n        \\frac{d}{dt}||{\\mathbf{r}}(t)||^2 &=2{\\mathbf{r}}(t)^T\\frac{d {\\mathbf{r}}(t)}{dt}\n        = - 2{\\mathbf{r}}(t)^T {\\mathbf{H}}(t) {\\mathbf{r}}(t)\n       = -2 ||{\\mathbf{J}}(t) {\\mathbf{r}}(t)||^2\n       \\leq - 2\\lambda_n({\\mathbf{H}}(t)) ||r(t)||^2 \\leq -\\kappa||r(t)||^2.\n    \\end{align*}\\] As \\(||r(s)||^2\\) and \\(-\\kappa\\) are real and continuous functions of \\(s\\) for \\(s \\in [0,t]\\), then the result claimed follows from Gronwall’s inequality, \\[\\begin{align*}\n        ||{\\mathbf{r}}(t)||^2 \\leq \\exp\\left( - \\int_{0}^t \\kappa ds \\right) ||{\\mathbf{r}}(0)||^2 = \\exp(-\\kappa t) ||{\\mathbf{r}}(0)||^2.\n    \\end{align*}\\]\n\nAs a result, to prove convergence it suffices to uniformly lower bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) for all \\(t\\geq0\\). Note as \\({\\mathbf{H}}(t)\\) is a gram matrix then its eigenvalues are both real and non-negative. This observation leads to the following somewhat trivial corollary.\n\nCorollary 1 Under the same conditions as Lemma 1 we have \\(L(t) \\leq L(0)\\).\n\nBefore proceeding a small side point to make is that we actually only need to bound the smallest eigenvalue of the eigenspace of \\({\\mathbf{H}}(t)\\) in which the residue lies. To be clear, suppose \\({\\mathbf{r}}(t)\\) lies in the span of the top \\(k(t)\\) eigenvectors of \\({\\mathbf{H}}(t)\\), then it would suffice to lower bound instead \\(\\lambda_{k(t)}({\\mathbf{H}}(t))\\). However, for neural networks, and indeed many other models, analyzing the spectrum of \\({\\mathbf{H}}(t)\\) directly is difficult. In particular, before one even considers the dynamics, observe, due to the random initialization of the network parameters, that \\({\\mathbf{H}}(0)\\) is a random matrix whose distribution is typically not easily analyzed. The approach we will instead pursue is as follows: i) substitute the analysis of the eigenvalues of \\({\\mathbf{H}}(0)\\) with that of a simpler `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) (the choice of notation here will soon become clear!), which we assume for now is positive semi-definite, then ii) derive conditions to ensure that \\(\\lambda_n({\\mathbf{H}}(t))\\) remains close to \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) for all \\(t \\in [0,T]\\) where \\(T\\in \\mathbb{R}_{&gt;0}\\) is arbitrary.\n\nLemma 2 Let \\({\\mathbf{H}}_{\\infty} \\in \\mathbb{R}^{n \\times n}\\) be positive semi-definite. Given a \\(T \\in \\mathbb{R}_{&gt;0}\\), if \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\in [0, T]\\), then \\(\\lambda_n(H(t)) \\geq \\frac{\\lambda_{n}({\\mathbf{H}}_{\\infty})}{2}\\) for all \\(t \\in [0, T]\\).\n\n\nProof. For any square matrix \\({\\mathbf{A}}\\in \\mathbb{R}^{n \\times n}\\) we have \\(\\lambda_1({\\mathbf{A}}) = -\\lambda_n(-{\\mathbf{A}})\\). Therefore, and as \\({\\mathbf{H}}(t)\\) and \\({\\mathbf{H}}_{\\infty}\\) are Hermitian by construction and assumption respectively, using a Weyl inequality we have \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| = |\\lambda_n({\\mathbf{H}}(t)) + \\lambda_1(-{\\mathbf{H}}_{\\infty})| \\leq |\\lambda_1({\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty})| = ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}||.\n    \\] Therefore, from the assumptions of the lemma and using the triangle inequality, it follows that \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}|| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| +  ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}.\n    \\] Trivially the result of the lemma holds if \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_{n}({\\mathbf{H}}_{\\infty})\\), therefore assume \\(\\lambda_n({\\mathbf{H}}(t)) &lt; \\lambda_{n}({\\mathbf{H}}_{\\infty})\\): in this case rearranging the inequality derived above it follows that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}\\).\n\nBased on Lemmas Lemma 1 and Lemma 2} we therefore can use the following approach for deriving training guarantees when confronted with non-linear least squares.\n\nIdentify a suitable `proxy’ matrix \\({\\bm{H}}_{\\infty}\\) which is positive semi-definite and is close to \\({\\bm{H}}(0)\\), in particular \\(||{\\bm{H}}(0) - {\\bm{H}}_{\\infty}|| \\leq \\lambda_n({\\bm{H}}_{\\infty})/4\\)\nIdentify a parameter regime which ensures the NTK remains close to its initialization, \\(||{\\bm{H}}(t) - {\\bm{H}}(0)|| \\leq \\lambda_n({\\bm{H}}_{\\infty})/4\\).\n\nFor neural networks we will see that a good candidate for \\({\\bm{H}}_{\\infty}\\) is the expected value of \\({\\bm{H}}(0)\\) which coincides with the infinite width limit of the network. By making the width of the network sufficiently large we will prove the above conditions are satisfied. Finally note for the bound on the loss to be useful we require \\(\\lambda_{n}({\\bm{H}}_{\\infty})&gt;0\\)!"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#a-recipe-for-training-guarantees-via-the-ntk",
    "href": "posts/opt-guarantees-ntk/index.html#a-recipe-for-training-guarantees-via-the-ntk",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "A recipe for training guarantees via the NTK",
    "text": "A recipe for training guarantees via the NTK\nThe Jacobian and its gram, \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t) \\in \\mathbb{R}^{n \\times n}\\), which is also referred to as the Neural Tangent Kernel (NTK) gram matrix, will play a critical role in what follows here and also when it comes to studying linearized neural networks. Note calling \\({\\mathbf{H}}(t)\\) the NTK or NTK gram matrix is somewhat misleading as it can be studied more generally for any sufficiently smooth model, not just neural networks! However, as it is now accepted terminology we will stick with it. The following proposition illustrates the significance of this matrix for training.\n\nProposition 1 Assume \\(f\\) is differentiable with respect to its parameters \\(\\theta \\in \\mathbb{R}^p\\) at all points along the trajectory of gradient flow. Then \\[\\begin{align*}\n    \\frac{d {\\mathbf{r}}(t)}{dt} = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] \n\n\nProof. Observe that as \\[\\begin{align*}\n        \\frac{\\partial L(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta, {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i)\n    \\end{align*}\\] then collecting terms we have \\[\n    \\nabla_{\\theta} L(\\theta(t)) =  {\\mathbf{J}}(t){\\mathbf{r}}(t).\n    \\] Noting that \\(f\\) is a function of \\(p\\) parameters which each depend on \\(t\\), then from the chain rule it follows that \\[\\begin{align*}\n        \\frac{d u_i(t)}{dt }   =  \\frac{d f(\\theta(t), {\\mathbf{x}}_i)}{dt}\n        = \\sum_{k = 1}^p \\frac{d \\theta_k}{dt} \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{\\partial \\theta_k}\n        = \\nabla_{\\theta} f(\\theta(t), {\\mathbf{x}}_i)^T  \\left(\\frac{d \\theta(t)}{dt}\\right).\n    \\end{align*}\\] Again collecting terms and substituting the expression for gradient flow given in we have \\[\\begin{align*}\n        \\frac{d {\\mathbf{u}}(t)}{dt} = {\\mathbf{J}}(t)^T \\frac{d \\theta(t)}{dt}\n        = - {\\mathbf{J}}(t)^T \\nabla_{\\theta}L(t) = - {\\mathbf{J}}(t)^T{\\mathbf{J}}(t){\\mathbf{r}}(t) = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] To finish observe \\(\\frac{d {\\mathbf{u}}(t)}{dt} = \\frac{d {\\mathbf{r}}(t)}{dt}\\).\n\nThe following lemma hints at how we might be able to use Proposition 1 to derive guarantees for training, note here we use \\(\\lambda_i({\\mathbf{A}})\\) to denote the \\(ith\\) eigenvalue of a matrix \\({\\mathbf{A}}\\in \\mathbb{C}^{n \\times n}\\) where \\(\\lambda_1({\\mathbf{A}}) \\geq \\lambda_2({\\mathbf{A}}) \\geq ... \\lambda_n({\\mathbf{A}})\\).\n\nLemma 1 For some \\(T\\in \\mathbb{R}_{\\geq 0}\\), suppose for all \\(t \\in [0,T]\\) there exists a constant \\(\\kappa \\geq 0\\) such that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\kappa}{2}\\). Suppose \\(f\\) is differentiable along the trajectory of the gradient flow. Then for all \\(t \\in [0, T]\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\kappa t) L(0).\n\\end{align*}\\] \n\nIndeed, Lemma 1 suggests that arbitrarily small training error can be guaranteed as long as we can bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) above zero for sufficiently long enough.\n\nProof. Observe by definition that \\(L(t) = ||{\\mathbf{r}}(t) ||_2^2\\). Using Lemma 1 it follows that \\[\\begin{align*}\n        \\frac{d}{dt}||{\\mathbf{r}}(t)||^2 &=2{\\mathbf{r}}(t)^T\\frac{d {\\mathbf{r}}(t)}{dt}\n        = - 2{\\mathbf{r}}(t)^T {\\mathbf{H}}(t) {\\mathbf{r}}(t)\n       = -2 ||{\\mathbf{J}}(t) {\\mathbf{r}}(t)||^2\n       \\leq - 2\\lambda_n({\\mathbf{H}}(t)) ||r(t)||^2 \\leq -\\kappa||r(t)||^2.\n    \\end{align*}\\] As \\(||r(s)||^2\\) and \\(-\\kappa\\) are real and continuous functions of \\(s\\) for \\(s \\in [0,t]\\), then the result claimed follows from Gronwall’s inequality, \\[\\begin{align*}\n        ||{\\mathbf{r}}(t)||^2 \\leq \\exp\\left( - \\int_{0}^t \\kappa ds \\right) ||{\\mathbf{r}}(0)||^2 = \\exp(-\\kappa t) ||{\\mathbf{r}}(0)||^2.\n    \\end{align*}\\]\n\nAs a result, to prove convergence it suffices to uniformly lower bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) for all \\(t\\geq0\\). Note as \\({\\mathbf{H}}(t)\\) is a gram matrix then its eigenvalues are both real and non-negative. This observation leads to the following somewhat trivial corollary.\n\nCorollary 1 Under the same conditions as Lemma 1 we have \\(L(t) \\leq L(0)\\).\n\nBefore proceeding a small side point to make is that we actually only need to bound the smallest eigenvalue of the eigenspace of \\({\\mathbf{H}}(t)\\) in which the residue lies. To be clear, suppose \\({\\mathbf{r}}(t)\\) lies in the span of the top \\(k(t)\\) eigenvectors of \\({\\mathbf{H}}(t)\\), then it would suffice to lower bound instead \\(\\lambda_{k(t)}({\\mathbf{H}}(t))\\). However, for neural networks, and indeed many other models, analyzing the spectrum of \\({\\mathbf{H}}(t)\\) directly is difficult. In particular, before one even considers the dynamics, observe, due to the random initialization of the network parameters, that \\({\\mathbf{H}}(0)\\) is a random matrix whose distribution is typically not easily analyzed. The approach we will instead pursue is as follows: i) substitute the analysis of the eigenvalues of \\({\\mathbf{H}}(0)\\) with that of a simpler `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) (the choice of notation here will soon become clear!), which we assume for now is positive semi-definite, then ii) derive conditions to ensure that \\(\\lambda_n({\\mathbf{H}}(t))\\) remains close to \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) for all \\(t \\in [0,T]\\) where \\(T\\in \\mathbb{R}_{&gt;0}\\) is arbitrary.\n\nLemma 2 Let \\({\\mathbf{H}}_{\\infty} \\in \\mathbb{R}^{n \\times n}\\) be positive semi-definite. Given a \\(T \\in \\mathbb{R}_{&gt;0}\\), if \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\in [0, T]\\), then \\(\\lambda_n(H(t)) \\geq \\frac{\\lambda_{n}({\\mathbf{H}}_{\\infty})}{2}\\) for all \\(t \\in [0, T]\\).\n\n\nProof. For any square matrix \\({\\mathbf{A}}\\in \\mathbb{R}^{n \\times n}\\) we have \\(\\lambda_1({\\mathbf{A}}) = -\\lambda_n(-{\\mathbf{A}})\\). Therefore, and as \\({\\mathbf{H}}(t)\\) and \\({\\mathbf{H}}_{\\infty}\\) are Hermitian by construction and assumption respectively, using a Weyl inequality we have \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| = |\\lambda_n({\\mathbf{H}}(t)) + \\lambda_1(-{\\mathbf{H}}_{\\infty})| \\leq |\\lambda_1({\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty})| = ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}||.\n    \\] Therefore, from the assumptions of the lemma and using the triangle inequality, it follows that \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}|| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| +  ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}.\n    \\] Trivially the result of the lemma holds if \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_{n}({\\mathbf{H}}_{\\infty})\\), therefore assume \\(\\lambda_n({\\mathbf{H}}(t)) &lt; \\lambda_{n}({\\mathbf{H}}_{\\infty})\\): in this case rearranging the inequality derived above it follows that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}\\).\n\nBased on Lemmas Lemma 1 and Lemma 2} we therefore can use the following approach for deriving training guarantees when confronted with non-linear least squares.\n\nIdentify a suitable `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) which is positive semi-definite and is close to \\({\\mathbf{H}}(0)\\), in particular \\(||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\)\nIdentify a parameter regime which ensures the NTK remains close to its initialization, \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\).\n\nFor neural networks we will see that a good candidate for \\({\\mathbf{H}}_{\\infty}\\) is the expected value of \\({\\mathbf{H}}(0)\\) which coincides with the infinite width limit of the network. By making the width of the network sufficiently large we will prove the above conditions are satisfied. Finally note for the bound on the loss to be useful we require \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})&gt;0\\)!"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#case-study-a-differentiable-shallow-neural-network",
    "href": "posts/opt-guarantees-ntk/index.html#case-study-a-differentiable-shallow-neural-network",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "Case-study: a differentiable, shallow neural network",
    "text": "Case-study: a differentiable, shallow neural network\nTo illustrate how the approach for proving convergence guarantees can be applied to neural networks we study potentially the simplest setting possible. Consider a two layer network \\[\nf(\\theta, {\\mathbf{x}}) = \\frac{1}{\\sqrt{m}}\\sum_{j=1}^m a_j \\phi({\\mathbf{w}}_j^T{\\mathbf{x}}).\n\\] Here \\({\\mathbf{x}}\\in \\mathbb{R}^d\\) denotes an input vector, \\({\\mathbf{w}}_j \\in \\mathbb{R}^d\\) the weights of the \\(j\\)th neuron, \\({\\mathbf{W}}\\in \\mathbb{R}^{m \\times d}\\) the matrix of neuron weights stored row-wise, \\({\\mathbf{a}}\\in \\mathbb{R}^m\\) the vector of output weights. The reasons for the explicit scaling of \\(1/\\sqrt{m}\\) will be become apparent soon. With regards to the activation function, for now we only assume \\(\\phi:\\mathbb{R}\\rightarrow \\mathbb{R}\\) is differentiable, as a result of this both \\(f\\) and in turn \\(L\\) are differentiable with respect to the network parameters. We further assume the network parameters are initialized mutually independent of one another with inner weights \\(w_{jk}(0) \\sim N(0,1)\\) and outer weights \\(a_j(0) \\sim U(\\{-1,+1\\})\\) for all \\(j \\in [m]\\), \\(k\\in [d]\\). We assume for simplicity the outer weights \\({\\mathbf{a}}\\) are frozen after initialization while the inner weights \\({\\mathbf{W}}(t)\\) evolve according to gradient flow . We therefore define the trainable network parameter vector \\(\\theta = [{\\mathbf{w}}_1^T, {\\mathbf{w}}_2^T... {\\mathbf{w}}_m^T] \\in \\mathbb{R}^{p}\\) where \\(p = dm\\). Finally, for typographical clarity we assume that the data is normalized to lie on the unit ball, i.e., \\(||{\\mathbf{x}}_i ||_2 = 1\\) for all \\(i\\in [n]\\).\nFirst we characterize the entries of \\({\\mathbf{H}}(t)\\), as \\[\n\\frac{\\partial f(\\theta(t), {\\mathbf{x}})}{\\partial w_{rc}} = \\frac{1}{\\sqrt{m}} a_r \\phi'({\\mathbf{w}}_r^T{\\mathbf{x}})x_c\n\\] then \\[\\begin{align*}\n        H_{il}(t) &= \\nabla_{\\theta}f(\\theta(t), {\\mathbf{x}}_i)^T\\nabla_{\\theta}f(\\theta(t), {\\mathbf{x}}_l)\\\\\n        & = \\frac{1}{m} \\left(\\sum_{k \\in [d]} x_{ki} x_{kl} \\right) \\left(\\sum_{r \\in [m]}  a_r^2 \\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_i)  \\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_l)\\right) \\\\  \n        &= \\frac{1}{m} \\sum_{r \\in [m]} \\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_i)  \\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_l).\n\\end{align*}\\]\nAssume now that \\(\\phi'(z)\\leq C_1\\) for all \\(z \\in \\mathbb{R}\\) and let \\({\\mathbf{w}}\\sim N(\\textbf{0}, \\textbf{I}_d)\\). As \\(({\\mathbf{w}}_r)_{r \\in [m]}\\) are mutually independent and identically distributed then \\[\n    \\mathbb{E}[H_{il}(0)] = \\mathbb{E}[\\phi'({\\mathbf{w}}^T {\\mathbf{x}}_i)  \\phi'({\\mathbf{w}}^T {\\mathbf{x}}_l)] \\leq C^2 &lt; \\infty.\n    \\] Furthermore, by the law of large numbers \\[\n    \\lim_{m \\rightarrow \\infty} H_{il}(0) = \\mathbb{E}[ H_{il}(0)]\n\\] Now let \\({\\mathbf{H}}_{\\infty}= \\mathbb{E}[{\\mathbf{H}}(0)]\\). Recall \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t)\\) is a gram matrix and therefore positive semi-definite and thus \\({\\mathbf{H}}_{\\infty}\\) is also positive semi-definite. We now show via a concentration argument that if \\(m\\) is large then \\(H_{il}(0) \\approx \\mathbb{E}[H_{il}(0)]\\). This means given sufficient width we can bound the 2-norm distance between \\({\\mathbf{H}}(0)\\) and \\({\\mathbf{H}}_{\\infty}\\) using the Frobenius norm of their difference.\n\nLemma 3 Assume \\(\\phi'(z)\\leq C\\) for all \\(z \\in \\mathbb{R}\\). For arbitrary \\(\\delta \\in (0,1]\\) and \\(\\varepsilon&gt;0\\), if \\(m \\geq n^2 \\frac{2C^4}{\\varepsilon^2} \\ln \\left( \\frac{2n^2}{\\delta} \\right)\\) then with probability at least \\(1-\\delta\\) \\[\n    || {\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| &lt; \\varepsilon.\n\\]\n\n\nProof. Let \\(i,l \\in [n]\\) be arbitrary and for typographical clarity let \\(Z_r = \\phi'({\\mathbf{w}}_r^T {\\mathbf{x}}_i) \\phi'({\\mathbf{w}}_r^T {\\mathbf{x}}_l)\\). Then by the bounded derivative assumption on \\(\\phi\\) it follows that \\(H_{il}(0)\\) is the arithmetic average of a sum of \\(m\\) mutually independent and identically distributed random variables, which almost surely are supported on \\([-C^2, C^2]\\). Applying Hoeffding’s inequality \\[\\begin{align*}\n        \\mathbb{P}\\left( \\left|\\frac{1}{m} \\sum_{r=1}^m (Z_r - \\mathbb{E}[Z_r]) \\right| \\geq \\epsilon \\right) \\leq 2\\exp\\left( -\\frac{2m\\epsilon^2}{4C^4} \\right),\n    \\end{align*}\\] and therefore \\[\\begin{align*}\n        \\mathbb{P}\\left( | H_{il}(0) - \\mathbb{E}[H_{il}(0)] | \\geq \\epsilon \\right) \\leq 2\\exp\\left( -\\frac{m\\epsilon^2}{2C^4} \\right).\n    \\end{align*}\\] Applying the union bound \\[\\begin{align*}\n        \\mathbb{P}\\left( \\bigcap_{i,l=1}^n \\{| H_{il}(0) - \\mathbb{E}[H_{il}(0)] | &lt; \\epsilon \\} \\right) & = 1 - \\mathbb{P}\\left( \\bigcup_{i,l=1}^n \\{| H_{il}(0) - \\mathbb{E}[H_{il}(0)] | \\geq \\epsilon \\} \\right)\\\\\n        &\\geq 1- \\sum_{i,l=1}^n \\mathbb{P}\\left(| H_{il}(0) - \\mathbb{E}[H_{il}(0)] | \\geq \\epsilon  \\right)\\\\\n        & \\geq 1- 2n^2 \\exp\\left( -\\frac{m\\epsilon^2}{2C^4} \\right)\n    \\end{align*}\\]\nLet \\(\\delta \\in (0,1]\\) denote the failure probability, then \\[\\begin{align*}\n       \\delta &\\geq 2n^2\\exp\\left( -\\frac{m\\epsilon^2}{2C^4} \\right) \\Leftrightarrow\n       \\ln \\left( \\frac{2n^2}{\\delta} \\right) \\leq \\frac{m\\epsilon^2}{2C^4} \\Leftrightarrow\n       m \\geq \\frac{2C^4}{\\epsilon^2} \\ln \\left( \\frac{2n^2}{\\delta} \\right).\n   \\end{align*}\\] Setting \\(\\epsilon = \\varepsilon/n\\), if \\(m \\geq \\frac{2C^4n^2}{\\varepsilon^2} \\ln \\left( \\frac{2n^2}{\\delta} \\right)\\) then with probability at least \\(1-\\delta\\) \\[\\begin{align*}\n       || {\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}||^2 &lt; || {\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}||_F^2\n       =\\sum_{i,l \\in [n]} | H_{il}(0) - \\mathbb{E}[H_{il}(0)] |^2\n        &lt; n^2 \\epsilon^2 = \\varepsilon^2.\n\\end{align*}\\]\n\nTo ensure closeness of the finite width NTK to its idealized infinite width limit at initialization we require a significant level of overparameterization! It is worth remarking that the tools we have used here are quite crude and that this dependency can indeed be relaxed, see for instance (Banerjee et al. 2023). Before we proceed to bound the dynamics we first bound \\(L(0)\\), in particular it will become apparent later that we need \\(L(0)\\) to scale sublinearly with \\(m\\)!\n\nLemma 4 Suppose there exists a \\(C \\in \\mathbb{R}_{&gt;0}\\) such that \\(\\mathbb{E}[\\phi^2(Z)] \\leq C\\) and \\(|y_i| \\leq C\\) for all \\(i \\in [n]\\) and \\(Z \\sim N(0,1)\\). For \\(\\delta \\in (0,1)\\), if \\(n \\geq \\delta^{-1}\\) then with probability at least \\(1-\\delta\\) we have \\(L(0) \\leq 2Cn^2\\).\n\n\nProof. A naive approach to bounding \\(L(0)\\) might be to uniformly bound \\(\\phi({\\mathbf{w}}_j^T{\\mathbf{x}})\\) for all \\(j\\in [m]\\) and any unit norm input \\({\\mathbf{x}}\\). This approach will clearly result in a bound which scales with \\(m\\) however. Due to the random initialization clearly \\(L(0)\\) is random, so our approach will instead be to show that the expectation of \\(L(0)\\) does not scale with \\(m\\) and then use a concentration bound. Note, having random output weights actually makes our life easier compared with say fixing the output weights according to some pre-determined pattern, e.g., half are negative and half positive! First observe \\[\\begin{align*}\n    L(0) = \\sum_{i=1}^n(f(\\theta(0), {\\mathbf{x}}_i)- y_i)^2\n    = \\sum_{i=1}^n(f^2(\\theta(0), {\\mathbf{x}}_i)- 2y_if(\\theta(0), {\\mathbf{x}}_i) ) + ||{\\mathbf{y}}||^2 .\n\\end{align*}\\] For typographical ease, letting \\({\\mathbf{w}}_j = {\\mathbf{w}}_j(0)\\) for all \\(j \\in [m]\\) then analyzing the quadratic term we have \\[\\begin{align*}\n    f^2(\\theta(0), {\\mathbf{x}}_i) = \\frac{1}{m} \\sum_{j,k=1}^m a_j a_k \\phi({\\mathbf{w}}_j^T {\\mathbf{x}}_i)\\phi({\\mathbf{w}}_k^T {\\mathbf{x}}_i),\n\\end{align*}\\] this contains \\(m(m+1)/2\\) distinct random variables which by inspection are not independent. This rules out using say Hoeffding’s bound, so instead we analyze the expectation and simply apply Markov’s inequality. First, as \\({\\mathbf{w}}_j \\in N(0, \\textbf{I})\\) and \\(||{\\mathbf{x}}_i||=1\\) then \\({\\mathbf{w}}_j^T{\\mathbf{x}}_i = \\sum_{l=1}^d {\\textnormal{w}}_{jl} x_{li} \\sim N(0, 1)\\). Therefore, and noting the assumption on \\(\\phi\\) that \\(\\mathbb{E}[\\phi^2(Z)] \\leq C &lt; \\infty\\), as \\(\\mathbb{E}[a_j] = 0\\) for all \\(j \\in [m]\\) then by independence \\[\n\\mathbb{E}[f(\\theta(0), {\\mathbf{x}}_i) )] = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^m \\mathbb{E}[a_j]\\mathbb{E}[\\phi({\\mathbf{w}}_j^T {\\mathbf{x}}_i)] = 0.\n\\] Furthermore \\[\\begin{align*}\n\\mathbb{E}[f^2(\\theta(0), {\\mathbf{x}}_i) )] &= \\frac{1}{m}\\sum_{j=1} \\mathbb{E}[a_j^2]\\mathbb{E}[\\phi^2({\\mathbf{w}}_j^T {\\mathbf{x}}_i)] + \\frac{1}{m}\\sum_{j\\neq k} \\mathbb{E}[a_j]\\mathbb{E}[a_k]\\mathbb{E}[\\phi({\\mathbf{w}}_j^T {\\mathbf{x}}_i)]\\mathbb{E}[\\phi({\\mathbf{w}}_k^T {\\mathbf{x}}_i)]\\\\\n&=\\frac{1}{m}\\sum_{j=1} \\mathbb{E}[a_j^2]\\mathbb{E}[\\phi^2({\\mathbf{w}}_j^T {\\mathbf{x}}_i)]\\\\\n&\\leq C,\n\\end{align*}\\] therefore \\[\\begin{align*}\n    \\mathbb{E}[L(0)] &= \\sum_{i=1}^n(\\mathbb{E}[f^2(\\theta(0), {\\mathbf{x}}_i)]- 2y_i\\mathbb{E}[f(\\theta(0), {\\mathbf{x}}_i)] ) + ||{\\mathbf{y}}||^2 \\leq 2nC\n\\end{align*}\\] by the assumptions of the lemma. As \\(L(0)\\) is a non-negative random variable then \\[\n\\mathbb{P}(L(0) \\geq 2Cn^2) \\leq \\frac{1}{n}.\n\\] Therefore, for any failure probability \\(\\delta \\in (0,1)\\) if \\(n\\geq \\delta^{-1}\\) then with probability at least \\(1-\\delta\\) we have \\(L(0) &lt; 2Cn^2\\).\n\nBefore proceeding it is worth remarking that this bound is certainly not tight due to the fact we have used Markov’s inequality. One alternative would be to try to using a for example Chebyshev’s inequality, or alternatively change the initialization. In particular, an antisymmetrically initialized network, in which a positive and negative equally weighted copy of each neuron is present, ensures that at initialization the network is the zero function. In this setting \\(L(0) = \\frac{1}{2}||{\\mathbf{y}}||^2\\) which scales only with \\(n\\) not \\(m\\).\nSo far we have achieved our first goal, a sufficient condition for the second, i.e., the control of the dynamics to ensure that \\({\\mathbf{H}}(t)\\) remains close to \\({\\mathbf{H}}(0)\\), can be derived by guaranteeing that the parameters of network never move far from initialization.\n\nLemma 5 Assume \\(\\phi\\) is differentiable and that there exists a \\(C\\in \\mathbb{R}_{\\geq 0}\\) such that \\(|\\phi'(z)|\\leq C\\) for all \\(z \\in \\mathbb{R}\\) and \\(\\phi'\\) is \\(C\\)-Lipschitz. For some \\(t \\in \\mathbb{R}_{\\geq 0}\\) suppose \\(|| {\\mathbf{w}}_r(t) - {\\mathbf{w}}(0)|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{8C^2 n } =: R\\). Then \\[\n|| {\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{4}\n\\]\n\n\nProof. First, for arbitrary \\(i,l \\in [n]\\) observe from that \\[\\begin{align*}\n    | H_{il}(t) - H_{il}(0)| &= \\frac{1}{m} \\left| \\sum_{r=1}^m \\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_i)\\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_l) - \\phi'({\\mathbf{w}}_r(0)^T {\\mathbf{x}}_i)\\phi'({\\mathbf{w}}_r(0)^T {\\mathbf{x}}_l) \\right|\\\\\n    &\\leq \\frac{1}{m}  \\sum_{r=1}^m \\left|\\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_i)\\phi'({\\mathbf{w}}_r(t)^T {\\mathbf{x}}_l) - \\phi'({\\mathbf{w}}_r(0)^T {\\mathbf{x}}_i)\\phi'({\\mathbf{w}}_r(0)^T {\\mathbf{x}}_l) \\right|\\\\\n    \\end{align*}\\] For typographical ease let \\(g({\\mathbf{a}}, {\\mathbf{b}}) = \\left|\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_i)\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_l) - \\phi'({\\mathbf{b}}^T {\\mathbf{x}}_i)\\phi'({\\mathbf{b}}^T {\\mathbf{x}}_l) \\right|\\), then using the triangle inequality and the fact that \\(|\\phi'(z)| &lt; C\\) \\[\\begin{align*}\n        g({\\mathbf{a}}, {\\mathbf{b}}) &\\leq \\left|\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_i)\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_l) - \\phi'({\\mathbf{a}}^T {\\mathbf{x}}_l)\\phi'({\\mathbf{b}}^T {\\mathbf{x}}_i) \\right|\\\\\n        &+\\left|\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_l)\\phi'({\\mathbf{b}}^T {\\mathbf{x}}_i) - \\phi'({\\mathbf{b}}^T {\\mathbf{x}}_i)\\phi'({\\mathbf{b}}^T {\\mathbf{x}}_l) \\right|\\\\\n        & \\leq C \\left(|\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_i) - \\phi'({\\mathbf{b}}^T {\\mathbf{x}}_i) | + |\\phi'({\\mathbf{a}}^T {\\mathbf{x}}_l) - \\phi'({\\mathbf{b}}^T {\\mathbf{x}}_l) |  \\right).\n    \\end{align*}\\] As \\(\\phi'\\) is \\(C\\)-Lipschitz continuous then \\(|\\phi'({\\mathbf{a}}^T {\\mathbf{x}}) - \\phi'({\\mathbf{b}}^T {\\mathbf{x}}) | \\leq C |{\\mathbf{a}}^T {\\mathbf{x}}- {\\mathbf{b}}^T {\\mathbf{x}}|\\), therefore \\[\\begin{align*}\n        g({\\mathbf{a}}, {\\mathbf{b}}) &\\leq C^2 \\left( |{\\mathbf{a}}^T {\\mathbf{x}}_i - {\\mathbf{b}}^T {\\mathbf{x}}_i| + |{\\mathbf{a}}^T {\\mathbf{x}}_l - {\\mathbf{b}}^T {\\mathbf{x}}_l |\\right)\\\\\n        & \\leq C^2(|| {\\mathbf{x}}_i || + || {\\mathbf{x}}_l || ) || {\\mathbf{a}}- {\\mathbf{b}}|| \\\\\n        & = 2C^2 || {\\mathbf{a}}- {\\mathbf{b}}||.\n    \\end{align*}\\] Therefore \\[\\begin{align*}\n        | H_{il}(t) - H_{il}(0)|  \\leq \\frac{1}{m} \\sum_{r=1}^m g({\\mathbf{w}}_r(t), {\\mathbf{w}}_r(0)) \\leq 2C^2 \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{8C^2 n }\n        = \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{4n}.\n    \\end{align*}\\] As a result \\[\\begin{align*}\n        || {\\mathbf{H}}(t) - {\\mathbf{H}}(0) ||^2 \\leq || {\\mathbf{H}}(t) - {\\mathbf{H}}(0) ||_F^2\n        = \\sum_{i,l=1}^n | H_{il}(t) - H_{il}(0)|^2\n        \\leq \\frac{\\lambda_n^2({\\mathbf{H}}_{\\infty})}{16}.\n    \\end{align*}\\]\n\nFollowing Lemma Lemma 5 we can bound the 2-norm distance between the parameters at time \\(t\\) and initialization as follows.\n\nLemma 6 Assume \\(\\phi\\) is continuously differentiable and also that there exists a \\(C\\in \\mathbb{R}_{\\geq 0}\\) such that \\(|\\phi'(z)|\\leq C\\) for all \\(z \\in \\mathbb{R}\\). For a given \\(T \\in \\mathbb{R}_{\\geq 0}\\) and any \\(r \\in [m]\\), \\(t \\in [0,T]\\) it holds that \\[\\begin{align*}\n    ||  {\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0)  || \\leq C\\sqrt{\\frac{n}{m}}\\int_{0}^t L(\\tau) d\\tau.\n    \\end{align*}\\]\n\n\nProof. As \\(\\phi\\) is continuously differentiable then by construction so is \\(f\\). For arbitrary \\(T \\in \\mathbb{R}_&gt;0\\), then by inspection each entry of \\(\\nabla_{\\theta}L(t)\\), \\[\\begin{align*}\n        \\frac{\\partial L(t)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i),\n    \\end{align*}\\] is continuous on \\([0,T]\\). In addition, due to the boundedness of the derivative and the continuity of \\(f\\) each entry of \\(\\nabla_{\\theta}L(t)\\) is also bounded on \\([0,T]\\). Therefore, and as we are using gradient flow each entry of \\(\\frac{d\\theta(t)}{dt}\\) is a real valued, continuous and Riemann integrable function on \\([0,T]\\). As \\(\\theta(t)\\) is the antiderivative of \\(\\frac{d\\theta(t)}{dt}\\), then by the Fundamental Theorem of Calculus for any \\(t \\in [0,T]\\) \\[\n    \\theta(t) = \\theta(0) + \\int_{0}^t \\frac{d\\theta(\\tau)}{d\\tau} d\\tau\n    \\] and therefore \\[\n    {\\mathbf{w}}_r(t) = {\\mathbf{w}}_r(0) + \\int_{0}^t \\frac{d{\\mathbf{w}}_r(\\tau)}{d\\tau} d\\tau.\n    \\] Rearranging and taking the norm gives \\[\n    ||  {\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0)  || = || \\int_{0}^t \\frac{d{\\mathbf{w}}(\\tau)}{d\\tau} d\\tau|| \\leq \\int_{0}^t \\left| \\left| \\frac{d{\\mathbf{w}}(\\tau)}{d\\tau} \\right| \\right| d\\tau.\n    \\] We proceed to upper bound the integrand as follows, as \\(|a_j| \\leq 1\\), the input data has unit norm and \\(|\\phi'(z)| \\leq C\\) for all \\(z \\in \\mathbb{R}\\), then \\[\\begin{align*}\n        \\left| \\left| \\frac{d{\\mathbf{w}}_r(t)}{d t} \\right| \\right|^2&= \\nabla_{{\\mathbf{w}}_r} L(t)^T \\nabla_{{\\mathbf{w}}_r} L(t) \\\\\n        &= \\sum_{k =1}^d \\left(\\frac{\\partial L(t)}{\\partial w_{rk}}\\right)^2\\\\\n        &= \\sum_{k =1}^d \\left(\\sum_{i=1}^n\\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{ \\partial w_{rk}} (f(\\theta, {\\mathbf{x}}_i) - y_i)\\right)^2\\\\\n        &= \\sum_{k =1}^d \\left(\\sum_{i=1}^n\\frac{1}{\\sqrt{m}} a_j \\phi'({\\mathbf{w}}_j^T{\\mathbf{x}}_i)x_k r_i(t) \\right)^2\\\\\n        & \\leq \\frac{C^2}{m} \\left(\\sum_{k =1}^d x_k^2\\right) \\left(\\sum_i r_i(t) \\right)^2 \\\\\n        & =\\frac{C^2}{m} {\\mathbf{r}}(t)^T \\textbf{1}_{n \\times n} {\\mathbf{r}}(t)\\\\\n        & =\\frac{C^2}{m} ||n^{-1/2}  \\textbf{1}_{n \\times n}{\\mathbf{r}}(t)||^2\\\\\n        & \\leq \\frac{C^2}{mn} ||\\textbf{1}_{n \\times n}||^2 || {\\mathbf{r}}(t)||^2\\\\\n        & = \\frac{C^2n}{m}|| {\\mathbf{r}}(t)||^2.\n    \\end{align*}\\] Therefore, for any \\(r \\in [m]\\) \\[\n    ||  {\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0)  || \\leq C\\sqrt{\\frac{n}{m}}\\int_{0}^t  L(\\tau) d\\tau\n    \\] as claimed.\n\nLemma 6 raises a tricky issue, in particular, it seems like we have arrived at the following circular argument.\n\nTo bound \\(L(t)\\) it suffices bound \\(|| {\\mathbf{H}}(t) - {\\mathbf{H}}(0) ||\\).\nTo bound \\(|| {\\mathbf{H}}(t) - {\\mathbf{H}}(0) ||\\) it suffices to bound \\(|| {\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0) ||\\) for all \\(r \\in [m]\\).\nTo bound \\(|| {\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0) ||\\) for all \\(r \\in [m]\\) it suffices to bound \\(L(t)\\).\n\nHowever, we can circumvent this issue using a real induction argument! We now present the main result. Note we present the result in terms of \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) and assume \\(\\lambda_n({\\mathbf{H}}_{\\infty})&gt;0\\). This is a reasonable assumption as for generic data (where the data matrix is drawn from a set with full Lebesgue measure) the smallest eigenvalue will be strictly positive. Quantitative lower bounds can be derived by placing for instance a seperation condition on the data, as an example see Corollary I.2 in (Oymak and Soltanolkotabi 2019).\n\nTheorem 1 Assume \\(\\lambda_n({\\mathbf{H}}_{\\infty})&gt;0\\), \\(\\phi\\) is continuously differentiable and that there exists a \\(C\\in \\mathbb{R}_{&gt; 0}\\) such that \\(|\\phi'(z)|\\leq C\\) for all \\(z \\in \\mathbb{R}\\), \\(\\mathbb{E}[\\phi^2(Z)]\\leq C\\) with \\(Z \\sim N(0,1)\\) and \\(\\phi'\\) is \\(C\\)-Lipschitz. For \\(\\delta \\in (0,1)\\), let \\(n \\geq 2\\delta^{-1}\\) and assume \\(m \\geq \\max \\{\\frac{32C^4n^2}{\\lambda_n({\\mathbf{H}}_{\\infty})^2} \\ln \\left( \\frac{4n^2}{\\delta} \\right), \\frac{256C^8 n^7 }{\\lambda_n({\\mathbf{H}}_{\\infty})^4}\\}\\). Then with probability at least \\(1-\\delta\\) we have for all \\(t \\geq 0\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\lambda_n({\\mathbf{H}}_{\\infty}) t) L(0).\n\\end{align*}\\]\n\n\nProof. By construction \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})\\geq 0\\), as the case \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})=0\\) is trivial assume \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})&gt;0\\). To prove the result claimed recall from Lemma 1 that it suffices to show that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_n({\\mathbf{H}}_{\\infty})/2\\) for all \\(t &gt;0\\). From Lemma 2 for this condition to be true it suffices that \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\geq 0\\). Note by Lemma 3 then as \\[m \\geq \\frac{32C^4n^2}{\\lambda_n({\\mathbf{H}}_{\\infty})^2} \\ln \\left( \\frac{4n^2}{\\delta} \\right)\n\\] then with probability at most \\(\\delta/2\\) it holds that \\[\n    || {\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| &gt;  \\lambda_n({\\mathbf{H}}_{\\infty})/4.\n    \\] In addition, as \\(n\\geq 2\\delta^{-1}\\) then with probability at most \\(\\delta/2\\) we have \\(L(0) \\geq 2Cn^2\\). Therefore and under the assumptions of the lemma, using a union bound argument it follows that \\(|| {\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) and \\(L(0) &lt; 2Cn^2\\).\nWe now proceed by real induction. As per the definition of an inductive set provided in Clark (2012), define for an arbitrary \\(T &gt;0\\) \\[\n    S = \\{t \\in [0,T]: \\lambda_n({\\mathbf{H}}(t)) &gt; \\lambda_n({\\mathbf{H}}_{\\infty})/2 \\}.\n    \\] Our goal is to show \\(S = [0,T]\\). To this end it suffices to prove that the following statements are true, i) \\(0 \\in S\\), ii) for any \\(t \\in (0, T)\\) such that \\(t \\in S\\), then there exists a \\(t'&gt;t\\) such that \\([t,t']\\subset S\\), ii) if \\(t \\in[0,T)\\) and \\([0,t) \\subset S\\) then \\(t \\in S\\). For Statement i), recall for \\(t=0\\) that \\[\n    |\\lambda_n({\\mathbf{H}}(0)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq || {\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq  \\lambda_n({\\mathbf{H}}_{\\infty})/4.\n    \\] If \\(\\lambda_n({\\mathbf{H}}(0))&lt; \\lambda_n({\\mathbf{H}}_{\\infty})\\) then \\(\\lambda_n({\\mathbf{H}}(0)) \\geq 3\\lambda_n({\\mathbf{H}}_{\\infty})/4\\), otherwise \\(\\lambda_n({\\mathbf{H}}(0))\\geq \\lambda_n({\\mathbf{H}}_{\\infty})\\) implying \\(0 \\in S\\).\nFor Statement ii), observe if \\(t \\in S\\) then there exists an \\(\\epsilon &gt;0\\) such that \\(\\lambda_n({\\mathbf{H}}(t))- \\epsilon &gt; \\lambda_n({\\mathbf{H}}_{\\infty})/2\\). To proceed we claim that \\(\\lambda_n({\\mathbf{H}}(t))\\) is continuous. Indeed, as \\(\\phi'\\) is continuous then each entry \\(H_{il}(t)\\) is continuous. Therefore \\({\\mathbf{H}}(t)\\) is continuous with respect to the Frobenius norm and indeed any other norm as all norms are equivalent in finite dimensional spaces. Therefore, by Theorem VI.1.4 Bhatia (1997), the eigenvalues including \\(\\lambda_n:\\mathbb{R}_{\\geq 0} \\rightarrow \\mathbb{R}_{\\geq 0}\\) are continuous functions. By continuity it follows that there exists a \\(\\delta'&gt;0\\) such that for all \\(\\tau \\in [t \\pm \\delta']\\) then \\(\\lambda_n({\\mathbf{H}}(\\tau)) \\in [\\lambda_n({\\mathbf{H}}_{\\infty})/2 \\pm \\epsilon]\\). This in turn implies \\(\\lambda_n({\\mathbf{H}}(\\tau)) &gt; \\lambda_n({\\mathbf{H}}_{\\infty})/2\\) for all \\(\\tau \\in [t, t+\\delta']\\).\nFor Statement iii), if \\([0,t) \\subset S\\) then for any \\(\\delta'&gt;0\\) it follows that \\([0, t - \\delta'] \\subset S\\). Using Lemma 5 \\[\\begin{align*}\n        ||{\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0) || &\\leq C\\sqrt{\\frac{n}{m}}\\int_{0}^t  L(\\tau) d\\tau\\\\\n        &= C\\sqrt{\\frac{n}{m}}\\left(\\int_{0}^{t-\\delta'}  L(\\tau) d\\tau + \\int_{t-\\delta'}^{t}  L(\\tau) d\\tau\\right)\\\\\n        & \\leq C\\sqrt{\\frac{n}{m}} L(0) \\left( \\int_{0}^{t-\\delta'} \\exp \\left(- \\lambda_n({\\mathbf{H}}_{\\infty}) \\right) d\\tau + \\delta' \\right)\\\\\n        & \\leq \\sqrt{\\frac{n}{m}} \\frac{2C^2n^2}{\\lambda_n({\\mathbf{H}}_{\\infty})} \\left( 1 + \\delta'\\lambda_n({\\mathbf{H}}_{\\infty}) \\right).\n    \\end{align*}\\] Letting \\(\\delta' \\leq \\frac{1}{2\\lambda_n({\\mathbf{H}}_{\\infty})}\\), then as \\(m\\geq \\frac{256C^8n^7 }{\\lambda_n({\\mathbf{H}}_{\\infty})^4}\\) it follows that \\[\n    ||{\\mathbf{w}}_r(t) - {\\mathbf{w}}_r(0) || \\leq \\frac{3R}{4}.\n    \\] Therefore \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty} || \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) and as a result we conclude that \\(\\lambda_n({\\mathbf{H}}(t))\\geq \\lambda_n({\\mathbf{H}}_{\\infty})/2\\) and thus \\(t \\in S\\). This concludes the proof by real induction. Note that as \\(T\\) can be arbitrarily large we can further conclude that \\(\\lambda_n({\\mathbf{H}}(t))\\geq \\lambda_n({\\mathbf{H}}_{\\infty})/2\\) for all \\(t &gt;0\\), thereby establishing a uniform lower bound on the smallest eigenvalue of \\({\\mathbf{H}}(t)\\)."
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#introduction-and-problem-setting",
    "href": "posts/opt-guarantees-ntk/index.html#introduction-and-problem-setting",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "Introduction and problem setting",
    "text": "Introduction and problem setting\nThe purpose of this post is to derive from first principles how the Neural Tangent Kernel (NTK) can be used to derive training guarantees for sufficiently overparameterized neural networks. As our goal is to highlight the key ideas we will work in the simplest setting possible, namely a single layer differentiable neural network. More complex and general forms of the argument presented here are widely present in the literature, starting with for instance (Du et al. 2019) and (Jacot, Gabriel, and Hongler 2020).\nFor now let \\(f: \\mathbb{R}^p \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) be a parameterized function with parameters \\(\\theta \\in \\mathbb{R}^p\\) mapping vectors in \\(\\mathbb{R}^d\\) to a real scalar value. Consider an arbitrary training sample consisting of \\(n\\) pairs of points and their corresponding targets \\(({\\mathbf{x}}_i, y_i)_{i=1}^n \\in (\\mathbb{R}^d \\times \\mathbb{R})^n\\), recall the least squares loss defined as \\[\\begin{align*}\n    L(\\theta) &= \\frac{1}{2}\\sum_{i=1}^n (f(\\theta, {\\mathbf{x}}_i) - y_i)^2.\n\\end{align*}\\] To solve the least squares problem we study the trajectory through parameter space under gradient flow, a continuous time simplification of gradient descent: simplifying our notation by using \\(L(t)\\) instead of \\(L(\\theta(t))\\), then for \\(t\\geq 0\\) \\[\\begin{equation} \\label{opt2-eq:grad-flow}\n    \\frac{d \\theta(t)}{dt} = - \\nabla_{\\theta} L(t).\n\\end{equation}\\] For now we assume that \\(f\\) is at differentiable with respect to its parameters. With \\({\\mathbf{u}}(t) = [f(\\theta(t), {\\mathbf{x}}_1), f(\\theta(t), {\\mathbf{x}}_2)... f(\\theta(t), {\\mathbf{x}}_n)] \\in \\mathbb{R}^n\\) denoting the vector of predictions and \\({\\mathbf{r}}(t) = {\\mathbf{u}}(t) - {\\mathbf{y}}\\) the vector of residuals at time \\(t \\geq 0\\), then we denote the Jacobian as \\[\\begin{align*}\n{\\mathbf{J}}(t) = \\nabla_\\theta {\\mathbf{r}}(t)= \\nabla_\\theta {\\mathbf{u}}(t) =\n  \\begin{bmatrix}\n    \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_1) }{\\partial \\theta_1} & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_2) }{\\partial \\theta_1} & ... & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_n) }{\\partial \\theta_1} \\\\\n    \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_1) }{\\partial \\theta_2} & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_2) }{\\partial \\theta_2} & ... & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_n) }{\\partial \\theta_2} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_1) }{\\partial \\theta_p} & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_2) }{\\partial \\theta_p} & ... & \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_n) }{\\partial \\theta_p}\n  \\end{bmatrix}\n  \\in \\mathbb{R}^{p \\times n}.\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#a-recipe-for-training-guarantees-via-the-neural-tangent-kernel-ntk",
    "href": "posts/opt-guarantees-ntk/index.html#a-recipe-for-training-guarantees-via-the-neural-tangent-kernel-ntk",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "A recipe for training guarantees via the Neural Tangent Kernel (NTK)",
    "text": "A recipe for training guarantees via the Neural Tangent Kernel (NTK)\nThe Jacobian and its gram, \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t) \\in \\mathbb{R}^{n \\times n}\\), which is also referred to as the Neural Tangent Kernel (NTK) gram matrix, will play a critical role in what follows here and also when it comes to studying linearized neural networks. Note calling \\({\\mathbf{H}}(t)\\) the NTK or NTK gram matrix is somewhat misleading as it can be studied more generally for any sufficiently smooth model, not just neural networks! However, as it is now accepted terminology we will stick with it. The following proposition illustrates the significance of this matrix for training.\n\nProposition 1 Assume \\(f\\) is differentiable with respect to its parameters \\(\\theta \\in \\mathbb{R}^p\\) at all points along the trajectory of gradient flow. Then \\[\\begin{align*}\n    \\frac{d {\\mathbf{r}}(t)}{dt} = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] \n\n\nProof. Observe that as \\[\\begin{align*}\n        \\frac{\\partial L(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta, {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i)\n    \\end{align*}\\] then collecting terms we have \\[\n    \\nabla_{\\theta} L(\\theta(t)) =  {\\mathbf{J}}(t){\\mathbf{r}}(t).\n    \\] Noting that \\(f\\) is a function of \\(p\\) parameters which each depend on \\(t\\), then from the chain rule it follows that \\[\\begin{align*}\n        \\frac{d u_i(t)}{dt }   =  \\frac{d f(\\theta(t), {\\mathbf{x}}_i)}{dt}\n        = \\sum_{k = 1}^p \\frac{d \\theta_k}{dt} \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{\\partial \\theta_k}\n        = \\nabla_{\\theta} f(\\theta(t), {\\mathbf{x}}_i)^T  \\left(\\frac{d \\theta(t)}{dt}\\right).\n    \\end{align*}\\] Again collecting terms and substituting the expression for gradient flow given in we have \\[\\begin{align*}\n        \\frac{d {\\mathbf{u}}(t)}{dt} = {\\mathbf{J}}(t)^T \\frac{d \\theta(t)}{dt}\n        = - {\\mathbf{J}}(t)^T \\nabla_{\\theta}L(t) = - {\\mathbf{J}}(t)^T{\\mathbf{J}}(t){\\mathbf{r}}(t) = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] To finish observe \\(\\frac{d {\\mathbf{u}}(t)}{dt} = \\frac{d {\\mathbf{r}}(t)}{dt}\\).\n\nThe following lemma hints at how we might be able to use Proposition 1 to derive guarantees for training, note here we use \\(\\lambda_i({\\mathbf{A}})\\) to denote the \\(ith\\) eigenvalue of a matrix \\({\\mathbf{A}}\\in \\mathbb{C}^{n \\times n}\\) where \\(\\lambda_1({\\mathbf{A}}) \\geq \\lambda_2({\\mathbf{A}}) \\geq ... \\lambda_n({\\mathbf{A}})\\).\n\nLemma 1 For some \\(T\\in \\mathbb{R}_{\\geq 0}\\), suppose for all \\(t \\in [0,T]\\) there exists a constant \\(\\kappa \\geq 0\\) such that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\kappa}{2}\\). Suppose \\(f\\) is differentiable along the trajectory of the gradient flow. Then for all \\(t \\in [0, T]\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\kappa t) L(0).\n\\end{align*}\\] \n\nIndeed, Lemma 1 suggests that arbitrarily small training error can be guaranteed as long as we can bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) above zero for sufficiently long enough.\n\nProof. Observe by definition that \\(L(t) = ||{\\mathbf{r}}(t) ||_2^2\\). Using Lemma 1 it follows that \\[\\begin{align*}\n        \\frac{d}{dt}||{\\mathbf{r}}(t)||^2 &=2{\\mathbf{r}}(t)^T\\frac{d {\\mathbf{r}}(t)}{dt}\n        = - 2{\\mathbf{r}}(t)^T {\\mathbf{H}}(t) {\\mathbf{r}}(t)\n       = -2 ||{\\mathbf{J}}(t) {\\mathbf{r}}(t)||^2\n       \\leq - 2\\lambda_n({\\mathbf{H}}(t)) ||r(t)||^2 \\leq -\\kappa||r(t)||^2.\n    \\end{align*}\\] As \\(||r(s)||^2\\) and \\(-\\kappa\\) are real and continuous functions of \\(s\\) for \\(s \\in [0,t]\\), then the result claimed follows from Gronwall’s inequality, \\[\\begin{align*}\n        ||{\\mathbf{r}}(t)||^2 \\leq \\exp\\left( - \\int_{0}^t \\kappa ds \\right) ||{\\mathbf{r}}(0)||^2 = \\exp(-\\kappa t) ||{\\mathbf{r}}(0)||^2.\n    \\end{align*}\\]\n\nAs a result, to prove convergence it suffices to uniformly lower bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) for all \\(t\\geq0\\). Note as \\({\\mathbf{H}}(t)\\) is a gram matrix then its eigenvalues are both real and non-negative. This observation leads to the following somewhat trivial corollary.\n\nCorollary 1 Under the same conditions as Lemma 1 we have \\(L(t) \\leq L(0)\\).\n\nBefore proceeding a small side point to make is that we actually only need to bound the smallest eigenvalue of the eigenspace of \\({\\mathbf{H}}(t)\\) in which the residue lies. To be clear, suppose \\({\\mathbf{r}}(t)\\) lies in the span of the top \\(k(t)\\) eigenvectors of \\({\\mathbf{H}}(t)\\), then it would suffice to lower bound instead \\(\\lambda_{k(t)}({\\mathbf{H}}(t))\\). However, for neural networks, and indeed many other models, analyzing the spectrum of \\({\\mathbf{H}}(t)\\) directly is difficult. In particular, before one even considers the dynamics, observe, due to the random initialization of the network parameters, that \\({\\mathbf{H}}(0)\\) is a random matrix whose distribution is typically not easily analyzed. The approach we will instead pursue is as follows: i) substitute the analysis of the eigenvalues of \\({\\mathbf{H}}(0)\\) with that of a simpler `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) (the choice of notation here will soon become clear!), which we assume for now is positive semi-definite, then ii) derive conditions to ensure that \\(\\lambda_n({\\mathbf{H}}(t))\\) remains close to \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) for all \\(t \\in [0,T]\\) where \\(T\\in \\mathbb{R}_{&gt;0}\\) is arbitrary.\n\nLemma 2 Let \\({\\mathbf{H}}_{\\infty} \\in \\mathbb{R}^{n \\times n}\\) be positive semi-definite. Given a \\(T \\in \\mathbb{R}_{&gt;0}\\), if \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\in [0, T]\\), then \\(\\lambda_n(H(t)) \\geq \\frac{\\lambda_{n}({\\mathbf{H}}_{\\infty})}{2}\\) for all \\(t \\in [0, T]\\).\n\n\nProof. For any square matrix \\({\\mathbf{A}}\\in \\mathbb{R}^{n \\times n}\\) we have \\(\\lambda_1({\\mathbf{A}}) = -\\lambda_n(-{\\mathbf{A}})\\). Therefore, and as \\({\\mathbf{H}}(t)\\) and \\({\\mathbf{H}}_{\\infty}\\) are Hermitian by construction and assumption respectively, using a Weyl inequality we have \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| = |\\lambda_n({\\mathbf{H}}(t)) + \\lambda_1(-{\\mathbf{H}}_{\\infty})| \\leq |\\lambda_1({\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty})| = ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}||.\n    \\] Therefore, from the assumptions of the lemma and using the triangle inequality, it follows that \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}|| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| +  ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}.\n    \\] Trivially the result of the lemma holds if \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_{n}({\\mathbf{H}}_{\\infty})\\), therefore assume \\(\\lambda_n({\\mathbf{H}}(t)) &lt; \\lambda_{n}({\\mathbf{H}}_{\\infty})\\): in this case rearranging the inequality derived above it follows that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}\\).\n\nBased on Lemmas Lemma 1 and Lemma 2} we therefore can use the following approach for deriving training guarantees when confronted with non-linear least squares.\n\nIdentify a suitable `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) which is positive semi-definite and is close to \\({\\mathbf{H}}(0)\\), in particular \\(||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\)\nIdentify a parameter regime which ensures the NTK remains close to its initialization, \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\).\n\nFor neural networks we will see that a good candidate for \\({\\mathbf{H}}_{\\infty}\\) is the expected value of \\({\\mathbf{H}}(0)\\) which coincides with the infinite width limit of the network. By making the width of the network sufficiently large we will prove the above conditions are satisfied. Finally note for the bound on the loss to be useful we require \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})&gt;0\\)!"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#a-recipe-for-deriving-guarantees-based-on-the-neural-tangent-kernel-ntk",
    "href": "posts/opt-guarantees-ntk/index.html#a-recipe-for-deriving-guarantees-based-on-the-neural-tangent-kernel-ntk",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "A recipe for deriving guarantees based on the Neural Tangent Kernel (NTK)",
    "text": "A recipe for deriving guarantees based on the Neural Tangent Kernel (NTK)\nThe Jacobian and its gram, \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t) \\in \\mathbb{R}^{n \\times n}\\), which is also referred to as the Neural Tangent Kernel (NTK) gram matrix, will play a critical role in what follows here and also when it comes to studying linearized neural networks. Note calling \\({\\mathbf{H}}(t)\\) the NTK or NTK gram matrix is somewhat misleading as it can be studied more generally for any sufficiently smooth model, not just neural networks! However, as it is now accepted terminology we will stick with it. The following proposition illustrates the significance of this matrix for training.\n\nProposition 1 Assume \\(f\\) is differentiable with respect to its parameters \\(\\theta \\in \\mathbb{R}^p\\) at all points along the trajectory of gradient flow. Then \\[\\begin{align*}\n    \\frac{d {\\mathbf{r}}(t)}{dt} = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] \n\n\nProof. Observe that as \\[\\begin{align*}\n        \\frac{\\partial L(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta, {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i)\n    \\end{align*}\\] then collecting terms we have \\[\n    \\nabla_{\\theta} L(\\theta(t)) =  {\\mathbf{J}}(t){\\mathbf{r}}(t).\n    \\] Noting that \\(f\\) is a function of \\(p\\) parameters which each depend on \\(t\\), then from the chain rule it follows that \\[\\begin{align*}\n        \\frac{d u_i(t)}{dt }   =  \\frac{d f(\\theta(t), {\\mathbf{x}}_i)}{dt}\n        = \\sum_{k = 1}^p \\frac{d \\theta_k}{dt} \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{\\partial \\theta_k}\n        = \\nabla_{\\theta} f(\\theta(t), {\\mathbf{x}}_i)^T  \\left(\\frac{d \\theta(t)}{dt}\\right).\n    \\end{align*}\\] Again collecting terms and substituting the expression for gradient flow given in we have \\[\\begin{align*}\n        \\frac{d {\\mathbf{u}}(t)}{dt} = {\\mathbf{J}}(t)^T \\frac{d \\theta(t)}{dt}\n        = - {\\mathbf{J}}(t)^T \\nabla_{\\theta}L(t) = - {\\mathbf{J}}(t)^T{\\mathbf{J}}(t){\\mathbf{r}}(t) = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] To finish observe \\(\\frac{d {\\mathbf{u}}(t)}{dt} = \\frac{d {\\mathbf{r}}(t)}{dt}\\).\n\nThe following lemma hints at how we might be able to use Proposition 1 to derive guarantees for training, note here we use \\(\\lambda_i({\\mathbf{A}})\\) to denote the \\(ith\\) eigenvalue of a matrix \\({\\mathbf{A}}\\in \\mathbb{C}^{n \\times n}\\) where \\(\\lambda_1({\\mathbf{A}}) \\geq \\lambda_2({\\mathbf{A}}) \\geq ... \\lambda_n({\\mathbf{A}})\\).\n\nLemma 1 For some \\(T\\in \\mathbb{R}_{\\geq 0}\\), suppose for all \\(t \\in [0,T]\\) there exists a constant \\(\\kappa \\geq 0\\) such that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\kappa}{2}\\). Suppose \\(f\\) is differentiable along the trajectory of the gradient flow. Then for all \\(t \\in [0, T]\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\kappa t) L(0).\n\\end{align*}\\] \n\nIndeed, Lemma 1 suggests that arbitrarily small training error can be guaranteed as long as we can bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) above zero for sufficiently long enough.\n\nProof. Observe by definition that \\(L(t) = ||{\\mathbf{r}}(t) ||_2^2\\). Using Lemma 1 it follows that \\[\\begin{align*}\n        \\frac{d}{dt}||{\\mathbf{r}}(t)||^2 &=2{\\mathbf{r}}(t)^T\\frac{d {\\mathbf{r}}(t)}{dt}\n        = - 2{\\mathbf{r}}(t)^T {\\mathbf{H}}(t) {\\mathbf{r}}(t)\n       = -2 ||{\\mathbf{J}}(t) {\\mathbf{r}}(t)||^2\n       \\leq - 2\\lambda_n({\\mathbf{H}}(t)) ||r(t)||^2 \\leq -\\kappa||r(t)||^2.\n    \\end{align*}\\] As \\(||r(s)||^2\\) and \\(-\\kappa\\) are real and continuous functions of \\(s\\) for \\(s \\in [0,t]\\), then the result claimed follows from Gronwall’s inequality, \\[\\begin{align*}\n        ||{\\mathbf{r}}(t)||^2 \\leq \\exp\\left( - \\int_{0}^t \\kappa ds \\right) ||{\\mathbf{r}}(0)||^2 = \\exp(-\\kappa t) ||{\\mathbf{r}}(0)||^2.\n    \\end{align*}\\]\n\nAs a result, to prove convergence it suffices to uniformly lower bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) for all \\(t\\geq0\\). Note as \\({\\mathbf{H}}(t)\\) is a gram matrix then its eigenvalues are both real and non-negative. This observation leads to the following somewhat trivial corollary.\n\nCorollary 1 Under the same conditions as Lemma 1 we have \\(L(t) \\leq L(0)\\).\n\nBefore proceeding a small side point to make is that we actually only need to bound the smallest eigenvalue of the eigenspace of \\({\\mathbf{H}}(t)\\) in which the residue lies. To be clear, suppose \\({\\mathbf{r}}(t)\\) lies in the span of the top \\(k(t)\\) eigenvectors of \\({\\mathbf{H}}(t)\\), then it would suffice to lower bound instead \\(\\lambda_{k(t)}({\\mathbf{H}}(t))\\). However, for neural networks, and indeed many other models, analyzing the spectrum of \\({\\mathbf{H}}(t)\\) directly is difficult. In particular, before one even considers the dynamics, observe, due to the random initialization of the network parameters, that \\({\\mathbf{H}}(0)\\) is a random matrix whose distribution is typically not easily analyzed. The approach we will instead pursue is as follows: i) substitute the analysis of the eigenvalues of \\({\\mathbf{H}}(0)\\) with that of a simpler `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) (the choice of notation here will soon become clear!), which we assume for now is positive semi-definite, then ii) derive conditions to ensure that \\(\\lambda_n({\\mathbf{H}}(t))\\) remains close to \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) for all \\(t \\in [0,T]\\) where \\(T\\in \\mathbb{R}_{&gt;0}\\) is arbitrary.\n\nLemma 2 Let \\({\\mathbf{H}}_{\\infty} \\in \\mathbb{R}^{n \\times n}\\) be positive semi-definite. Given a \\(T \\in \\mathbb{R}_{&gt;0}\\), if \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\in [0, T]\\), then \\(\\lambda_n(H(t)) \\geq \\frac{\\lambda_{n}({\\mathbf{H}}_{\\infty})}{2}\\) for all \\(t \\in [0, T]\\).\n\n\nProof. For any square matrix \\({\\mathbf{A}}\\in \\mathbb{R}^{n \\times n}\\) we have \\(\\lambda_1({\\mathbf{A}}) = -\\lambda_n(-{\\mathbf{A}})\\). Therefore, and as \\({\\mathbf{H}}(t)\\) and \\({\\mathbf{H}}_{\\infty}\\) are Hermitian by construction and assumption respectively, using a Weyl inequality we have \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| = |\\lambda_n({\\mathbf{H}}(t)) + \\lambda_1(-{\\mathbf{H}}_{\\infty})| \\leq |\\lambda_1({\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty})| = ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}||.\n    \\] Therefore, from the assumptions of the lemma and using the triangle inequality, it follows that \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}|| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| +  ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}.\n    \\] Trivially the result of the lemma holds if \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_{n}({\\mathbf{H}}_{\\infty})\\), therefore assume \\(\\lambda_n({\\mathbf{H}}(t)) &lt; \\lambda_{n}({\\mathbf{H}}_{\\infty})\\): in this case rearranging the inequality derived above it follows that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}\\).\n\nBased on Lemmas Lemma 1 and Lemma 2} we therefore can use the following approach for deriving training guarantees when confronted with non-linear least squares.\n\nIdentify a suitable `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) which is positive semi-definite and is close to \\({\\mathbf{H}}(0)\\), in particular \\(||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\)\nIdentify a parameter regime which ensures the NTK remains close to its initialization, \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\).\n\nFor neural networks we will see that a good candidate for \\({\\mathbf{H}}_{\\infty}\\) is the expected value of \\({\\mathbf{H}}(0)\\) which coincides with the infinite width limit of the network. By making the width of the network sufficiently large we will prove the above conditions are satisfied. Finally note for the bound on the loss to be useful we require \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})&gt;0\\)!"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#a-recipe-for-deriving-guarantees-based-on-analyzing-the-smallest-eigenvalue-of-the-neural-tangent-kernel-ntk",
    "href": "posts/opt-guarantees-ntk/index.html#a-recipe-for-deriving-guarantees-based-on-analyzing-the-smallest-eigenvalue-of-the-neural-tangent-kernel-ntk",
    "title": "Optimization guarantees via the NTK",
    "section": "A recipe for deriving guarantees based on analyzing the smallest eigenvalue of the Neural Tangent Kernel (NTK)",
    "text": "A recipe for deriving guarantees based on analyzing the smallest eigenvalue of the Neural Tangent Kernel (NTK)\nThe Jacobian and its gram, \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t) \\in \\mathbb{R}^{n \\times n}\\), which is also referred to as the Neural Tangent Kernel (NTK) gram matrix, will play a critical role in what follows here and also when it comes to studying linearized neural networks. Note calling \\({\\mathbf{H}}(t)\\) the NTK or NTK gram matrix is somewhat misleading as it can be studied more generally for any sufficiently smooth model, not just neural networks! However, as it is now accepted terminology we will stick with it. The following proposition illustrates the significance of this matrix for training.\n\nProposition 1 Assume \\(f\\) is differentiable with respect to its parameters \\(\\theta \\in \\mathbb{R}^p\\) at all points along the trajectory of gradient flow. Then \\[\\begin{align*}\n    \\frac{d {\\mathbf{r}}(t)}{dt} = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] \n\n\nProof. Observe that as \\[\\begin{align*}\n        \\frac{\\partial L(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta, {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i)\n    \\end{align*}\\] then collecting terms we have \\[\n    \\nabla_{\\theta} L(\\theta(t)) =  {\\mathbf{J}}(t){\\mathbf{r}}(t).\n    \\] Noting that \\(f\\) is a function of \\(p\\) parameters which each depend on \\(t\\), then from the chain rule it follows that \\[\\begin{align*}\n        \\frac{d u_i(t)}{dt }   =  \\frac{d f(\\theta(t), {\\mathbf{x}}_i)}{dt}\n        = \\sum_{k = 1}^p \\frac{d \\theta_k}{dt} \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{\\partial \\theta_k}\n        = \\nabla_{\\theta} f(\\theta(t), {\\mathbf{x}}_i)^T  \\left(\\frac{d \\theta(t)}{dt}\\right).\n    \\end{align*}\\] Again collecting terms and substituting the expression for gradient flow given in we have \\[\\begin{align*}\n        \\frac{d {\\mathbf{u}}(t)}{dt} = {\\mathbf{J}}(t)^T \\frac{d \\theta(t)}{dt}\n        = - {\\mathbf{J}}(t)^T \\nabla_{\\theta}L(t) = - {\\mathbf{J}}(t)^T{\\mathbf{J}}(t){\\mathbf{r}}(t) = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] To finish observe \\(\\frac{d {\\mathbf{u}}(t)}{dt} = \\frac{d {\\mathbf{r}}(t)}{dt}\\).\n\nThe following lemma hints at how we might be able to use Proposition 1 to derive guarantees for training, note here we use \\(\\lambda_i({\\mathbf{A}})\\) to denote the \\(ith\\) eigenvalue of a matrix \\({\\mathbf{A}}\\in \\mathbb{C}^{n \\times n}\\) where \\(\\lambda_1({\\mathbf{A}}) \\geq \\lambda_2({\\mathbf{A}}) \\geq ... \\lambda_n({\\mathbf{A}})\\).\n\nLemma 1 For some \\(T\\in \\mathbb{R}_{\\geq 0}\\), suppose for all \\(t \\in [0,T]\\) there exists a constant \\(\\kappa \\geq 0\\) such that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\kappa}{2}\\). Suppose \\(f\\) is differentiable along the trajectory of the gradient flow. Then for all \\(t \\in [0, T]\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\kappa t) L(0).\n\\end{align*}\\] \n\nIndeed, Lemma 1 suggests that arbitrarily small training error can be guaranteed as long as we can bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) above zero for sufficiently long enough.\n\nProof. Observe by definition that \\(L(t) = ||{\\mathbf{r}}(t) ||_2^2\\). Using Lemma 1 it follows that \\[\\begin{align*}\n        \\frac{d}{dt}||{\\mathbf{r}}(t)||^2 &=2{\\mathbf{r}}(t)^T\\frac{d {\\mathbf{r}}(t)}{dt}\n        = - 2{\\mathbf{r}}(t)^T {\\mathbf{H}}(t) {\\mathbf{r}}(t)\n       = -2 ||{\\mathbf{J}}(t) {\\mathbf{r}}(t)||^2\n       \\leq - 2\\lambda_n({\\mathbf{H}}(t)) ||r(t)||^2 \\leq -\\kappa||r(t)||^2.\n    \\end{align*}\\] As \\(||r(s)||^2\\) and \\(-\\kappa\\) are real and continuous functions of \\(s\\) for \\(s \\in [0,t]\\), then the result claimed follows from Gronwall’s inequality, \\[\\begin{align*}\n        ||{\\mathbf{r}}(t)||^2 \\leq \\exp\\left( - \\int_{0}^t \\kappa ds \\right) ||{\\mathbf{r}}(0)||^2 = \\exp(-\\kappa t) ||{\\mathbf{r}}(0)||^2.\n    \\end{align*}\\]\n\nAs a result, to prove convergence it suffices to uniformly lower bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) for all \\(t\\geq0\\). Note as \\({\\mathbf{H}}(t)\\) is a gram matrix then its eigenvalues are both real and non-negative. This observation leads to the following somewhat trivial corollary.\n\nCorollary 1 Under the same conditions as Lemma 1 we have \\(L(t) \\leq L(0)\\).\n\nBefore proceeding a small side point to make is that we actually only need to bound the smallest eigenvalue of the eigenspace of \\({\\mathbf{H}}(t)\\) in which the residue lies. To be clear, suppose \\({\\mathbf{r}}(t)\\) lies in the span of the top \\(k(t)\\) eigenvectors of \\({\\mathbf{H}}(t)\\), then it would suffice to lower bound instead \\(\\lambda_{k(t)}({\\mathbf{H}}(t))\\). However, for neural networks, and indeed many other models, analyzing the spectrum of \\({\\mathbf{H}}(t)\\) directly is difficult. In particular, before one even considers the dynamics, observe, due to the random initialization of the network parameters, that \\({\\mathbf{H}}(0)\\) is a random matrix whose distribution is typically not easily analyzed. The approach we will instead pursue is as follows: i) substitute the analysis of the eigenvalues of \\({\\mathbf{H}}(0)\\) with that of a simpler `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) (the choice of notation here will soon become clear!), which we assume for now is positive semi-definite, then ii) derive conditions to ensure that \\(\\lambda_n({\\mathbf{H}}(t))\\) remains close to \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) for all \\(t \\in [0,T]\\) where \\(T\\in \\mathbb{R}_{&gt;0}\\) is arbitrary.\n\nLemma 2 Let \\({\\mathbf{H}}_{\\infty} \\in \\mathbb{R}^{n \\times n}\\) be positive semi-definite. Given a \\(T \\in \\mathbb{R}_{&gt;0}\\), if \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\in [0, T]\\), then \\(\\lambda_n(H(t)) \\geq \\frac{\\lambda_{n}({\\mathbf{H}}_{\\infty})}{2}\\) for all \\(t \\in [0, T]\\).\n\n\nProof. For any square matrix \\({\\mathbf{A}}\\in \\mathbb{R}^{n \\times n}\\) we have \\(\\lambda_1({\\mathbf{A}}) = -\\lambda_n(-{\\mathbf{A}})\\). Therefore, and as \\({\\mathbf{H}}(t)\\) and \\({\\mathbf{H}}_{\\infty}\\) are Hermitian by construction and assumption respectively, using a Weyl inequality we have \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| = |\\lambda_n({\\mathbf{H}}(t)) + \\lambda_1(-{\\mathbf{H}}_{\\infty})| \\leq |\\lambda_1({\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty})| = ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}||.\n    \\] Therefore, from the assumptions of the lemma and using the triangle inequality, it follows that \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}|| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| +  ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}.\n    \\] Trivially the result of the lemma holds if \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_{n}({\\mathbf{H}}_{\\infty})\\), therefore assume \\(\\lambda_n({\\mathbf{H}}(t)) &lt; \\lambda_{n}({\\mathbf{H}}_{\\infty})\\): in this case rearranging the inequality derived above it follows that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}\\).\n\nBased on Lemma 1 and Lemma 2 we therefore can use the following approach for deriving training guarantees when confronted with non-linear least squares.\n\nIdentify a suitable `proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) which is positive semi-definite and is close to \\({\\mathbf{H}}(0)\\), in particular \\(||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\)\nIdentify a parameter regime which ensures the NTK remains close to its initialization, \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\).\n\nFor neural networks we will see that a good candidate for \\({\\mathbf{H}}_{\\infty}\\) is the expected value of \\({\\mathbf{H}}(0)\\) which coincides with the infinite width limit of the network. By making the width of the network sufficiently large we will prove the above conditions are satisfied. Finally note for the bound on the loss to be useful we require \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})&gt;0\\)!"
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#conclusion",
    "href": "posts/opt-guarantees-ntk/index.html#conclusion",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "Conclusion",
    "text": "Conclusion\nTheorem 1 is interesting as despite the loss function being non-linear and non-convex in the model parameters, it says for sufficiently overparameterized networks and assuming \\(\\lambda_n({\\mathbf{H}}_{\\infty}) &gt; 0\\), then given a sufficiently long training time one can achieve arbitrarily small loss using gradient flow! There are some notable limitations however, in particular the level of overparameterization is exceedingly severe and not representative of networks in practice. Moreover, in order to derive our results we had to limit the movement of each neuron throughout training! This condition requiring the NTK to remain close to its initialization, or equivalently each neuron to remain close to its initialization, is limiting as it rules out a rich feature learning regime in which the kernel (or weights adapt to the data). In this post we looked at deriving from first principles guarantees in the very simple setting of a differentiable single layer network. These ideas can be generalized to encompass more general networks and problems in terms of depth, architecture and loss functions. A nice summary and introduction to these topics can be found here https://www.benjamin-bowman.com/assets/Intro_to_NTK.pdf."
  },
  {
    "objectID": "posts/opt-guarantees-ntk/index.html#a-recipe-for-deriving-guarantees-based-on-analyzing-the-smallest-eigenvalue-of-the-ntk",
    "href": "posts/opt-guarantees-ntk/index.html#a-recipe-for-deriving-guarantees-based-on-analyzing-the-smallest-eigenvalue-of-the-ntk",
    "title": "Optimization guarantees for neural networks via the NTK",
    "section": "A recipe for deriving guarantees based on analyzing the smallest eigenvalue of the NTK",
    "text": "A recipe for deriving guarantees based on analyzing the smallest eigenvalue of the NTK\nThe Jacobian and its gram, \\({\\mathbf{H}}(t) = {\\mathbf{J}}(t)^T {\\mathbf{J}}(t) \\in \\mathbb{R}^{n \\times n}\\), which is also referred to as the Neural Tangent Kernel (NTK) gram matrix, will play a critical role in what follows here and also when it comes to studying linearized neural networks. Note calling \\({\\mathbf{H}}(t)\\) the NTK or NTK gram matrix is somewhat misleading as it can be studied more generally for any sufficiently smooth model, not just neural networks! However, as it is now accepted terminology we will stick with it. The following proposition illustrates the significance of this matrix for training.\n\nProposition 1 Assume \\(f\\) is differentiable with respect to its parameters \\(\\theta \\in \\mathbb{R}^p\\) at all points along the trajectory of gradient flow. Then \\[\\begin{align*}\n    \\frac{d {\\mathbf{r}}(t)}{dt} = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] \n\n\nProof. Observe that as \\[\\begin{align*}\n        \\frac{\\partial L(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^n \\frac{\\partial f(\\theta, {\\mathbf{x}}_i)}{ \\partial \\theta_k} (f(\\theta, {\\mathbf{x}}_i) - y_i)\n    \\end{align*}\\] then collecting terms we have \\[\n    \\nabla_{\\theta} L(\\theta(t)) =  {\\mathbf{J}}(t){\\mathbf{r}}(t).\n    \\] Noting that \\(f\\) is a function of \\(p\\) parameters which each depend on \\(t\\), then from the chain rule it follows that \\[\\begin{align*}\n        \\frac{d u_i(t)}{dt }   =  \\frac{d f(\\theta(t), {\\mathbf{x}}_i)}{dt}\n        = \\sum_{k = 1}^p \\frac{d \\theta_k}{dt} \\frac{\\partial f(\\theta(t), {\\mathbf{x}}_i)}{\\partial \\theta_k}\n        = \\nabla_{\\theta} f(\\theta(t), {\\mathbf{x}}_i)^T  \\left(\\frac{d \\theta(t)}{dt}\\right).\n    \\end{align*}\\] Again collecting terms and substituting the expression for gradient flow we have \\[\\begin{align*}\n        \\frac{d {\\mathbf{u}}(t)}{dt} = {\\mathbf{J}}(t)^T \\frac{d \\theta(t)}{dt}\n        = - {\\mathbf{J}}(t)^T \\nabla_{\\theta}L(t) = - {\\mathbf{J}}(t)^T{\\mathbf{J}}(t){\\mathbf{r}}(t) = - {\\mathbf{H}}(t) {\\mathbf{r}}(t).\n    \\end{align*}\\] To finish observe \\(\\frac{d {\\mathbf{u}}(t)}{dt} = \\frac{d {\\mathbf{r}}(t)}{dt}\\).\n\nThe following lemma hints at how we might be able to use Proposition 1 to derive guarantees for training, note here we use \\(\\lambda_i({\\mathbf{A}})\\) to denote the \\(ith\\) eigenvalue of a matrix \\({\\mathbf{A}}\\in \\mathbb{C}^{n \\times n}\\) where \\(\\lambda_1({\\mathbf{A}}) \\geq \\lambda_2({\\mathbf{A}}) \\geq ... \\lambda_n({\\mathbf{A}})\\).\n\nLemma 1 For some \\(T\\in \\mathbb{R}_{\\geq 0}\\), suppose for all \\(t \\in [0,T]\\) there exists a constant \\(\\kappa \\geq 0\\) such that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\kappa}{2}\\). Suppose \\(f\\) is differentiable along the trajectory of the gradient flow. Then for all \\(t \\in [0, T]\\) \\[\\begin{align*}\n         L(t) \\leq \\exp(- \\kappa t) L(0).\n\\end{align*}\\] \n\nIndeed, Lemma 1 suggests that arbitrarily small training error can be guaranteed as long as we can bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) above zero for sufficiently long enough.\n\nProof. Observe by definition that \\(L(t) = ||{\\mathbf{r}}(t) ||_2^2\\). Using Lemma 1 it follows that \\[\\begin{align*}\n        \\frac{d}{dt}||{\\mathbf{r}}(t)||^2 &=2{\\mathbf{r}}(t)^T\\frac{d {\\mathbf{r}}(t)}{dt}\n        = - 2{\\mathbf{r}}(t)^T {\\mathbf{H}}(t) {\\mathbf{r}}(t)\n       = -2 ||{\\mathbf{J}}(t) {\\mathbf{r}}(t)||^2\n       \\leq - 2\\lambda_n({\\mathbf{H}}(t)) ||r(t)||^2 \\leq -\\kappa||r(t)||^2.\n    \\end{align*}\\] As \\(||r(s)||^2\\) and \\(-\\kappa\\) are real and continuous functions of \\(s\\) for \\(s \\in [0,t]\\), then the result claimed follows from Gronwall’s inequality, \\[\\begin{align*}\n        ||{\\mathbf{r}}(t)||^2 \\leq \\exp\\left( - \\int_{0}^t \\kappa ds \\right) ||{\\mathbf{r}}(0)||^2 = \\exp(-\\kappa t) ||{\\mathbf{r}}(0)||^2.\n    \\end{align*}\\]\n\nAs a result, to prove convergence it suffices to uniformly lower bound the smallest eigenvalue of \\({\\mathbf{H}}(t)\\) for all \\(t\\geq0\\). Note as \\({\\mathbf{H}}(t)\\) is a gram matrix then its eigenvalues are both real and non-negative. This observation leads to the following somewhat trivial corollary.\n\nCorollary 1 Under the same conditions as Lemma 1 we have \\(L(t) \\leq L(0)\\).\n\nBefore proceeding a small side point to make is that we actually only need to bound the smallest eigenvalue of the eigenspace of \\({\\mathbf{H}}(t)\\) in which the residue lies. To be clear, suppose \\({\\mathbf{r}}(t)\\) lies in the span of the top \\(k(t)\\) eigenvectors of \\({\\mathbf{H}}(t)\\), then it would suffice to lower bound instead \\(\\lambda_{k(t)}({\\mathbf{H}}(t))\\). However, for neural networks, and indeed many other models, analyzing the spectrum of \\({\\mathbf{H}}(t)\\) directly is difficult. In particular, before one even considers the dynamics, observe, due to the random initialization of the network parameters, that \\({\\mathbf{H}}(0)\\) is a random matrix whose distribution is typically not easily analyzed. The approach we will instead pursue is as follows: i) substitute the analysis of the eigenvalues of \\({\\mathbf{H}}(0)\\) with that of a simpler ‘proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) (the choice of notation here will soon become clear!), which we assume for now is positive semi-definite, then ii) derive conditions to ensure that \\(\\lambda_n({\\mathbf{H}}(t))\\) remains close to \\(\\lambda_n({\\mathbf{H}}_{\\infty})\\) for all \\(t \\in [0,T]\\) where \\(T\\in \\mathbb{R}_{&gt;0}\\) is arbitrary.\n\nLemma 2 Let \\({\\mathbf{H}}_{\\infty} \\in \\mathbb{R}^{n \\times n}\\) be positive semi-definite. Given a \\(T \\in \\mathbb{R}_{&gt;0}\\), if \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)||, ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\) for all \\(t \\in [0, T]\\), then \\(\\lambda_n(H(t)) \\geq \\frac{\\lambda_{n}({\\mathbf{H}}_{\\infty})}{2}\\) for all \\(t \\in [0, T]\\).\n\n\nProof. For any square matrix \\({\\mathbf{A}}\\in \\mathbb{R}^{n \\times n}\\) we have \\(\\lambda_1({\\mathbf{A}}) = -\\lambda_n(-{\\mathbf{A}})\\). Therefore, and as \\({\\mathbf{H}}(t)\\) and \\({\\mathbf{H}}_{\\infty}\\) are Hermitian by construction and assumption respectively, using a Weyl inequality we have \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| = |\\lambda_n({\\mathbf{H}}(t)) + \\lambda_1(-{\\mathbf{H}}_{\\infty})| \\leq |\\lambda_1({\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty})| = ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}||.\n    \\] Therefore, from the assumptions of the lemma and using the triangle inequality, it follows that \\[\n    |\\lambda_n({\\mathbf{H}}(t)) - \\lambda_n({\\mathbf{H}}_{\\infty})| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}_{\\infty}|| \\leq ||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| +  ||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}.\n    \\] Trivially the result of the lemma holds if \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\lambda_{n}({\\mathbf{H}}_{\\infty})\\), therefore assume \\(\\lambda_n({\\mathbf{H}}(t)) &lt; \\lambda_{n}({\\mathbf{H}}_{\\infty})\\): in this case rearranging the inequality derived above it follows that \\(\\lambda_n({\\mathbf{H}}(t)) \\geq \\frac{\\lambda_n({\\mathbf{H}}_{\\infty})}{2}\\).\n\nBased on Lemma 1 and Lemma 2 we therefore can use the following approach for deriving training guarantees when confronted with non-linear least squares.\n\nIdentify a suitable ‘proxy’ matrix \\({\\mathbf{H}}_{\\infty}\\) which is positive semi-definite and is close to \\({\\mathbf{H}}(0)\\), in particular \\(||{\\mathbf{H}}(0) - {\\mathbf{H}}_{\\infty}|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\)\nIdentify a parameter regime which ensures the NTK remains close to its initialization, \\(||{\\mathbf{H}}(t) - {\\mathbf{H}}(0)|| \\leq \\lambda_n({\\mathbf{H}}_{\\infty})/4\\).\n\nFor neural networks we will see that a good candidate for \\({\\mathbf{H}}_{\\infty}\\) is the expected value of \\({\\mathbf{H}}(0)\\) which coincides with the infinite width limit of the network. By making the width of the network sufficiently large we will prove the above conditions are satisfied. Finally note for the bound on the loss to be useful we require \\(\\lambda_{n}({\\mathbf{H}}_{\\infty})&gt;0\\)!"
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html",
    "href": "posts/opt-guarantees-perceptron/index.html",
    "title": "Global optimization guarantees for shallow neural networks via the Perceptron algorithm",
    "section": "",
    "text": "In this post we will look at how the proof of convergence for the Perceptron algorithm can be used to derive strong training guarantees for a shallow neural network assuming linearly separable data."
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#introduction",
    "href": "posts/opt-guarantees-perceptron/index.html#introduction",
    "title": "Global optimization guarantees for shallow neural networks via the Perceptron algorithm",
    "section": "Introduction",
    "text": "Introduction\nFor neural networks deriving training guarantees for reaching global optima is challenging. In another blog post we showed how such guarantees can be achieved by ensuring the network in question is sufficiently wide (this is the NTK approach). However, the typical width requirements are often unrealistic and moreover are designed in order to keep the parameters within some ball of their initialization throughout training and therefore cannot really explain the rich feature learning observed in practice. In this post we remove any width requirements at the price of having to impose significant structure on the target function and data. In particular we consider the problem of learning a binary classifier \\(f_{\\theta}:\\mathbb{R}^d \\rightarrow \\{\\pm 1 \\}\\) on linearly separable data. To this end let \\((x_i, y_i)_{i \\in [n]}\\) denote the training data where \\(x_i \\in \\mathbb{R}^d\\) are the input features and \\(y_i \\in \\{ \\pm 1\\}\\) the output labels. If the problem is linearly separable then there exists a \\(u \\in \\mathbb{R}^d\\) such that \\(\\text{sign}(\\langle a, x_i \\rangle)= y_i\\), or equivalently \\(y_i\\langle a, x_i \\rangle &gt; 0\\) for all \\(i \\in [n]\\). We assume the input data is bounded, i.e., \\(|| x_i || \\leq R\\) for all \\(i \\in [n]\\). Letting \\(f(x; \\theta)\\) denote the map of a shallow neural network our goal is to understand if gradient descent can find parameters \\(\\theta\\) such that \\(sign(f(x_i;\\theta))=y_i\\) for all \\(i \\in [n]\\). To achieve this we deploy a neat trick based on the Perceptron algorithm which can be seen in (Brutzkus et al. 2018). Our presentation will most closely follow that given in (Karhadkar et al. 2024)."
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#setting",
    "href": "posts/opt-guarantees-perceptron/index.html#setting",
    "title": "Optimization guarantees for neural networks on linearly seperable data",
    "section": "Setting",
    "text": "Setting\nIn this post we consider the problem of learning a binary classifier \\(f:\\mathbb{R}^d \\rightarrow \\{\\pm 1 \\}\\). To this end let \\((x_i, y_i)_{i \\in [n]}\\) denote the training data where \\(x_i \\in \\mathbb{R}^d\\) are the input features and \\(y_i \\in \\{ \\pm 1\\}\\) the output labels. Lets assume the problem is linearly separable, in particular this means that there exists some \\(u \\in \\mathbb{R}^d\\) such that \\(\\text{sign}(\\langle a, x_i \\rangle)= y_i\\), or equivalently \\(y_i\\langle a, x_i \\rangle &gt; 0\\) for all \\(i \\in [n]\\).\n\nDefinition 1  \n\n\nLemma 1 XXX\n\n\nProof. XXX"
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#on-the-convergence-of-the-perceptron-algorithm-for-linearly-seperable-data",
    "href": "posts/opt-guarantees-perceptron/index.html#on-the-convergence-of-the-perceptron-algorithm-for-linearly-seperable-data",
    "title": "Optimization guarantees for neural networks on linearly seperable data",
    "section": "On the convergence of the Perceptron algorithm for linearly seperable data",
    "text": "On the convergence of the Perceptron algorithm for linearly seperable data\nBefore we consider a shallow neural network, let’s first recall the Perceptron algorithm.\n\nInputs: \\((x_i, y_i)_{i \\in [n]}\\)\nAlgorithm:\n\nInitialize \\(w(0) = 0_d\\), \\(t = 1\\)\nWhile there exists a \\(\\pi(t) \\in [n]\\) s.t. \\(y_{\\pi(t)} \\langle w(t), x_{\\pi(t)} \\rangle \\leq 0\\) do\n\n\\(w(t) = w(t-1) + y_{\\pi(t)}x_{\\pi(t)}\\)\n\\(t = t + 1\\)\n\nreturn \\(w(t)\\)\n\n\nIn the above the map \\(\\pi: \\mathbb{N}\\rightarrow [n]\\) selects a point to update at each iteration. Furthermore observe that at each iteration the Perceptron algorithm improves its performance on at least one feature-label pair in the training set: indeed, \\(y_{\\pi(t)} \\langle w(t), x_{\\pi(t)} \\rangle = y_{\\pi(t)}\\langle w(t-1), x_{\\pi(t)} \\rangle + 1\\).\nFor linearly separable data it is possible to show that the Perceptron algorithm converges to a global minimizer. Recall linearly separable means there exists a \\(u \\in \\mathbb{R}^d\\), \\(||u ||=1\\) such that \\(y_i \\langle x_i, u \\rangle &gt;0\\) for all \\(i \\in [n]\\) (the requirement that \\(u\\) is unit norm is made for convenience as we can always re-scale by dividing by \\(||u ||\\)). Let \\(\\gamma(u)\\) denote the margin of \\(u\\), i.e., the minimum distance of a point in the data sample to the decision boundary of \\(u\\). Geometrically, as the decision boundary of the linear classifier \\(u\\) is the vector subspace orthogonal to \\(u\\) then the shortest distance from \\(x_i\\) to this decision boundary is simply \\(|\\langle x_i, u \\rangle|\\). Let \\[\n\\gamma(u) := \\min_{i \\in [n]} |\\langle x_i, u \\rangle|,\n\\] denote the margin of the linear classifier \\(u\\). Without loss of generality we may as well consider the linear classifier with the maximum margin: to this end moving forward we let \\[\n\\gamma := \\max_{||u ||=1} \\min_{i \\in [n]} |\\langle x_i, u \\rangle|\n\\] denote the max margin and \\(u\\) the max margin classifier. For linearly separable data it is possible to show that the Perceptron algorithm terminates after a finite number of steps and successfully classifies all points in the training sample.\n\nLemma 1 If \\((x_i, y_i)_{i \\in [n]}\\) is linearly separable then the Perceptron algorithm terminates after at most \\(T \\leq \\frac{R^2}{\\gamma^2}\\) iterations and \\[ y_i \\langle x_{i} , w(T) \\rangle &gt; 0 \\] for all \\(i \\in [n]\\).\n\n\nProof. The key idea of the proof is to show that \\(| \\langle w(t) , u \\rangle |^2\\) grows faster than \\(|| w(t) | |^2\\). By the Cauchy-Schwarz (CS) inequality, for any iteration \\(t \\geq 1\\) it must hold that\n\\[\n  |\\langle w(t), u \\rangle|^2 \\leq || w(t) ||^2.\n\\] Therefore, there must exist an iteration \\(T\\) after which no further updates take place: indeed, if this was not true then for some sufficiently large iteration the CS inequality would be invalid which is clearly a contradiction. To lower bound \\(| \\langle w(t) , u \\rangle |^2\\) , observe \\[\n\\begin{align*}\n  \\langle w(t), u \\rangle &\\geq \\langle w(t-1), u \\rangle + y_{\\pi(t)} \\langle x_{\\pi(t)} , u \\rangle\\\\\n  & \\geq \\langle w(t-1), u \\rangle + \\gamma\\\\\n  & \\geq \\langle w(t-2), u \\rangle + 2 \\gamma\\\\\n  & \\geq t \\gamma.\n\\end{align*}\n\\] To upper bound \\(||w(t)||^2\\) note \\[\n\\begin{align*}\n  ||w(t)||^2  &=  ||w(t-1) +  y_{\\pi(t)}  x_{\\pi(t)}||^2 \\\\\n  &= || w(t-1) ||^2 + 2y_{\\pi(t)} \\langle x_{\\pi(t)}, w(t-1) \\rangle + ||x_{\\pi(t)}||^2\\\\\n  & \\leq || w(t-1) ||^2 + ||x_{\\pi(t)}||^2\\\\\n  & \\leq|| w(t-1) ||^2 + R^2\\\\\n  & \\leq || w(t-1) ||^2 +  2R^2\\\\\n  & \\leq t R^2.\n\\end{align*}\n\\] Let \\(T\\) denote the iteration at which the Perceptron algorithm terminates, \\(T = \\infty\\) indicates the Perceptron algorithm never terminates. Then by the CS inequality \\[\nT^2 \\gamma^2 \\leq |\\langle w_t, u \\rangle|^2 \\leq || w_t ||^2 \\leq TR^2\n\\] Rearranging then \\(T\\leq \\frac{R^2}{\\gamma^2}\\) as claimed."
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#before-we-consider-a-shallow-neural-network-lets-first-recall-the-perceptron-algorithm.",
    "href": "posts/opt-guarantees-perceptron/index.html#before-we-consider-a-shallow-neural-network-lets-first-recall-the-perceptron-algorithm.",
    "title": "Optimization guarantees for neural networks on linearly seperable data",
    "section": "Before we consider a shallow neural network, let’s first recall the Perceptron algorithm.",
    "text": "Before we consider a shallow neural network, let’s first recall the Perceptron algorithm.\n\nDefinition 1  \n\n\nLemma 1 XXX\n\n\nProof. XXX"
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#from-the-perceptron-algorithm-to-training-a-shallow-neural-network",
    "href": "posts/opt-guarantees-perceptron/index.html#from-the-perceptron-algorithm-to-training-a-shallow-neural-network",
    "title": "Optimization guarantees for neural networks on linearly seperable data",
    "section": "From the Perceptron algorithm to training a shallow neural network",
    "text": "From the Perceptron algorithm to training a shallow neural network\n\nDefinition 1"
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#from-the-perceptron-algorithm-to-shallow-neural-network",
    "href": "posts/opt-guarantees-perceptron/index.html#from-the-perceptron-algorithm-to-shallow-neural-network",
    "title": "Optimization guarantees for neural networks on linearly seperable data",
    "section": "From the Perceptron algorithm to shallow neural network",
    "text": "From the Perceptron algorithm to shallow neural network\nConsider now a shallow neural network \\[\nf(x;W,v) = v^T \\sigma(Wx) = \\sum_{j=1}^{2m} v_j \\sigma( \\langle w_j, x \\rangle)\n\\] where \\(W \\in \\mathbb{R}^{2m \\times d}\\) is the weight matrix of the first layer, \\(w_j\\) is the \\(j\\)th row of \\(W\\), \\(v \\in \\mathbb{R}^{2m}\\) is a vector of output weights and \\(v_j\\) is the \\(j\\)th entry of \\(v\\). We focus on the Leaky-ReLU non-linearity \\(\\sigma(z) = \\max \\{ \\alpha z, z \\}\\) for \\(\\alpha \\in (0,1)\\). Consider learning the parameters \\(W\\) and \\(v\\) using gradient descent (GD) with the hinge loss (recall the hinge loss is defined as \\(\\ell(z) = \\max \\{0 , 1 - z \\}\\)). Using the minimum norm subgradient (as is standard in most automatic differentiation software) at zero we let \\(\\ell'(z) = -1\\) for \\(z &lt; 1\\) and is \\(0\\) otherwise, equivalently we can write this as \\(\\ell'(z) = - \\mathbb{1}(z&lt;1)\\). Similarly \\(\\sigma'(z) = \\alpha\\) for \\(z \\leq 0\\) and is \\(1\\) otherwise. A straightforward calculation gives \\[\n\\begin{align*}\n&\\frac{\\partial f}{\\partial w_j}(x;W,v) = v_j \\sigma'(\\langle w_j, x \\rangle) x,\\\\\n&\\frac{\\partial f}{\\partial v_j}(x;W,v) =\\sigma(\\langle w_j, x \\rangle).\\\\\n\\end{align*}\n\\] Consider training the network with gradient descent in order to minimize the loss \\(L(W,v) = \\sum_{i=1}^n \\ell(y_i f(x_i, W,v))\\): as \\[\n\\frac{\\partial L}{\\partial \\theta} = \\sum_{i \\in [n]} \\ell'(y_i f(x_i; W, v))y_i \\frac{\\partial f}{\\partial \\theta}(x_i;W,v)\n\\] then the gradient updates for each neuron \\(j \\in [2m]\\) are \\[\n\\begin{align*}\n& w_j(t) =  w_j(t-1) + \\eta_w \\sum_{i =1}^{n} \\mathbb{1}[y_i f(x_i; W(t-1), v(t-1))&lt;1]  v_j(t-1) \\sigma'(\\langle w_j(t-1), x_i \\rangle) y_i x_i,\\\\\n& v_j(t) = v_j(t-1) + \\eta_v \\sum_{i =1}^{n} \\mathbb{1}[y_i f(x_i; W(t-1), v(t-1))&lt;1] \\sigma(\\langle w_j(t-1), x_i \\rangle),\n\\end{align*}\n\\] where \\(\\eta_w, \\eta_v &gt; 0\\) are the learning rates for the inner and outer weights respectively. Inspecting these equations we see that shallow networks trained with hinge loss and GD have a similar update rule to the Perceptron algorithm! Inspired by this observation we pursue the idea of comparing the growth rate of \\(| \\langle v_j(t) w_j(t), u \\rangle |^2\\) versus \\(|| v_j(t)w_j(t) ||^2\\) for each \\(j \\in [2m]\\). In particular, analogous to studying the growth of \\(\\langle w(t), u \\rangle\\) for the Perceptron algorithm, we instead study \\[\n  A(t) = \\sum_{j=1}^{2m}\\langle v_j(t) w_j(t), u \\rangle.\n\\] Here we use A for alignment and \\(u\\) again denotes the unit norm weights of the max margin linear classifier. Similarly, instead of considering \\(||w(t)||^2\\) we study \\[\n  F(t) = || diag(v(t)) W(t) ||_F^2\n\\] (we use F for Frobenius) where \\(diag(v(t))\\) is a diagonal square matrix with \\([diag(v(t))]_{jj} = v_j(t)\\). Again by the CS inequality and assuming the data is linearly separable with max margin unit vetor \\(u\\), then it must hold that \\[\nA^2(t) = |\\langle vec(diag(v(t)) W(t)), u \\otimes 1_{2m} \\rangle|^2 \\leq F(t) 2m.\n\\]\nTherefore, if we can show that \\(F(t)\\) grows slower than \\(A(t)\\) under the outlined gradient update rule there must be a finite iteration after which the update is zero and the parameters no longer change. We will do this by lower bounding \\(A(t)\\) and upper bounding \\(F(t)\\) in terms of the total number times each point in the training set contributes to the update of the weights at time \\(t\\), which we denote \\(U(t) = \\sum_{\\tau=1}^{t} |\\mathcal{F}(\\tau)|\\). This is useful as we can bound the number of updates \\(T\\) by considering the setting where only one point contributes to the update at each time step, leading to the bound \\(T \\leq U(t)\\).\n\nTraining with frozen outer weights\nFirst we consider a simplified setting where we freeze the outer weights by setting \\(\\eta_v = 0\\): in this setting we are able to prove analogous results to the Perceptron setting.\n\nLemma 2 Suppose the training data \\((x_i, y_i)_{i\\in [n]}\\) is linearly separable and assume \\(\\eta_v = 0\\), \\(\\eta_w \\leq \\frac{1}{nR}\\), \\(v_j(0) = (-1)^j\\) and \\(||w_j(0) || \\leq \\lambda_w\\) for all \\(j \\in [2m]\\) and $ _w {1, }$. Then gradient descent will terminate after \\[\nT &lt; \\frac{1}{\\eta_w \\alpha \\gamma }\\left(1 + \\frac{10}{\\alpha \\gamma} \\right)\n\\] iterations, at which point the network will have achieved zero training loss.\n\n\nProof. First we lower bound A(t). Let \\(\\mathcal{F}(t) = \\{ i \\in [n] \\; : \\; y_i f(x_i; \\theta(t -1)) &lt; 1\\}\\) denote the set of training points that have not achieved zero hinge loss at the previous iterate. Alternatively we can view this as the set of points that participate in the update at iteration \\(t\\). Recalling the gradient update rule, and using \\(\\sigma'_{ji}(t) := \\sigma'(\\langle w_j(t), x_i \\rangle)\\) for typographical ease, then \\[\n\\begin{align*}\n  w_j(t) =  w_j(t-1) + \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} (-1)^j \\sigma'_{ji}(t-1) y_i x_i.\n\\end{align*}\n\\] Recalling the update rule, as $y_i x_i, u $ and \\(\\sigma'_{ji}(t) \\geq \\alpha &gt;0\\) then \\[\n\\begin{align*}\n  A(t) &= \\sum_{j=1}^{2m} (-1)^j\\langle w_j(t), u \\rangle \\\\\n  &= A(t-1) + \\eta_w \\sum_{j=1}^{2m} \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sigma'_{ji}(t) y_i \\langle x_i, u\\rangle \\\\\n  & \\geq  A(t-1) + \\eta_w \\gamma \\alpha \\sum_{j=1}^{2m} \\sum_{i \\in \\mathcal{F}(t)}^{n} 1\\\\\n  & = A(t-1) + \\eta_w \\gamma \\alpha 2m |\\mathcal{F}(t)|\\\\\n  & = A(0) + \\eta_w \\gamma \\alpha 2m \\sum_{\\tau = 1}^{t} |\\mathcal{F}(\\tau)|.\n\\end{align*}\n\\] Under the assumptions of the lemma \\[\n\\begin{align*}\nA(0) &= \\sum_{j=1}^{2m} (-1)^j\\langle w_j(0), u \\rangle\\\\\n& \\geq  - \\sum_{j=1}^{2m} | \\langle w_j(0), u \\rangle| \\\\\n& \\geq - 2m \\lambda_w\n\\end{align*}\n\\] Therefore, defining \\(U(t) = \\sum_{\\tau = 1}^{t} |\\mathcal{F}(\\tau)|\\) we gave \\[\n  A(t) \\geq -2m \\lambda_w + 2m \\eta_w \\alpha \\gamma U(t).\n\\] Before proceeding it is worth reflecting on the fact we need \\(\\alpha&gt;0\\) to make this bound useful. To extend this technique to Leaky-ReLU networks one might instead explore bounding for each neuron \\(j \\in [2m]\\) \\(| \\mathcal{F}(t) \\cap \\mathcal{A}_j(t) |\\), where \\(\\mathcal{A}_j(t) = \\{i \\in [n]: \\; \\langle w_j(t), x_i \\rangle &gt; 0 \\}\\). For the upper bound on F(t) we plug in the GD update rule. \\[\n\\begin{align*}\nF(t)\n&= \\sum_{j=1}^{2m} ||w_j(t) ||^2\\\\\n&= \\sum_{j=1}^{2m} || w_j(t-1) + \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} (-1)^j \\sigma'_{ji}(t-1) y_i x_i ||^2\\\\\n&= \\sum_{j=1}^{2m}||w_j(t-1) ||^2 + 2\\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sum_{j=1}^{2m}(-1)^j\\sigma'_{ji}(t-1)y_i \\langle w_j(t-1),  x_i \\rangle \\\\\n&+\\eta_w^2 \\sum_{j =1}^{2m} \\sum_{i,k \\in \\mathcal{F}(t)}^{n}  (-1)^{2j}\\sigma'_{ji}(t-1)\\sigma'_{ki}(t-1)\\langle x_i, x_k \\rangle.\n\\end{align*}\n\\] The first term in the above is simply \\(F(t-1)\\). The second can be simplified by observing that \\(\\sigma(z) = \\sigma'(z) z\\) for a piecewise linear function \\(\\sigma\\). Therefore \\[\n\\begin{align*}\n  \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sum_{j=1}^{2m}(-1)^j\\sigma'_{ji}(t-1) \\langle w_j(t-1),  x_i \\rangle &= \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sum_{j=1}^{2m}(-1)^j\\sigma(\\langle w_j(t-1),  x_i \\rangle)\\\\\n  &= \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} y_i f(x_i; W(t-1))\\\\\n  & &lt; \\eta_w |\\mathcal{F}(t)|,\n\\end{align*}\n\\] where the final inequality follows from the fact that \\(i \\in \\mathcal{F}(t)\\) implies \\(y_i f(x_i; W(t-1))&lt;1\\). The third term in the previous bound on \\(F(t)\\) simplifies from the fact that \\(\\sigma_{ji}(t-1)\\leq 1\\), \\(|| x_i || \\leq R\\) and $ _w /nR^2 $. As a result, and as \\(|\\mathcal{F}(t)| \\leq n\\), we have \\[\n\\eta_w^2 \\sum_{j =1}^{2m} \\sum_{i,k \\in \\mathcal{F}(t)}^{n}  (-1)^{2j}\\sigma'_{ji}(t-1)\\sigma'_{ki}(t-1)\\langle x_i, x_k \\rangle \\leq 2m\\eta_w^2 R^2 |\\mathcal{F}(t)|^2 \\leq 2m \\eta_w |\\mathcal{F}(t)|\n\\] Note the bound placed on the step-size means we can avoid having to work with \\(|\\mathcal{F}(t)|^2\\). Proceeding, it follows that \\[\n\\begin{align*}\nF(t) &&lt; F(t-1) + (2 + 2m )\\eta_w |\\mathcal{F}(t)|\\\\\n& \\leq F(t-1) + 4m \\eta_w |\\mathcal{F}(t)|\\\\\n&&lt; F(0) + 4 m\\eta_w \\sum_{\\tau=1}^t |\\mathcal{F}(t)|\\\\\n& = F(0) + 4 m\\eta_w  U(t)\\\\\n& = \\sum_{j=1}^{2m} ||w_j(0)||^2 + 4 m\\eta_w U(t)\\\\\n& \\leq 2m \\lambda_w + 4 m\\eta_w U(t).\n\\end{align*}\n\\] By the CS inequality and dividing both sides by \\(4m^2\\) then \\[\n\\frac{A^2(t)}{4m^2} \\leq \\frac{F(t)}{2m}.\n\\] Plugging in the bounds for \\(A(t)\\) and \\(F(t)\\), using \\(\\lambda_w \\leq 1\\) and rearranging produces the following sequence of inequalities, \\[\n\\begin{align*}\n(\\eta_w \\alpha \\gamma U(t) - \\lambda_w)^2 &&lt; \\lambda_w + 2 \\eta_w U(t)\\\\\n\\eta_w^2 \\alpha^2 \\gamma^2U^2(t) - 2 \\lambda_w \\eta_w \\alpha \\gamma U(t) + \\lambda_w^2 &&lt; \\lambda_w + 2 \\eta_w U(t)\\\\\n\\eta_w \\alpha^2 \\gamma^2U^2(t) - 2 (\\lambda_w \\alpha \\gamma +1)  U(t) - \\frac{\\lambda_w(1-\\lambda_w )}{\\eta_w} &&lt; 0,\\\\\n  \\eta_w \\alpha^2 \\gamma^2U^2(t) - 2 (\\lambda_w \\alpha \\gamma +1)  U(t) - \\frac{\\lambda_w}{\\eta_w} &&lt; 0.\n\\end{align*}\n\\] Applying the quadratic formula for \\(U(t)\\) and using \\(\\lambda_w \\leq \\text{min}(\\frac{1}{\\alpha \\gamma}, 1)\\) then \\[\n\\begin{align*}\n2\\eta_w \\alpha^2 \\gamma^2 U(t) &&lt; 4 (\\lambda_w \\alpha \\gamma +1)^2 + 2\\sqrt{ (\\lambda_w \\alpha \\gamma +1)^2 + \\lambda_w \\alpha^2 \\gamma^2}\\\\\n& \\leq 16 + \\sqrt{4 + \\alpha^2 \\gamma^2}\\\\\n& = 16 + \\sqrt{(2 + \\alpha \\gamma)^2 - 4 \\alpha \\gamma}\\\\\n& \\leq 18 + \\alpha \\gamma.\n\\end{align*}\n\\] Therefore \\[\nU(t) &lt; \\frac{1}{\\eta_w \\alpha \\gamma }\\left(1 + \\frac{10}{\\alpha \\gamma} \\right).\n\\] To conclude, observe if \\(T\\) is the final iterate then \\[\n  T = \\sum_{\\tau = 1}^{T} 1 \\leq \\sum_{\\tau = 1}^{T} |\\mathcal{F}(\\tau)| = U(T) &lt;\\frac{1}{\\eta_w \\alpha \\gamma }\\left(1 + \\frac{10}{\\alpha \\gamma} \\right).\n\\]\n\nA few reflections on this result are as follows.\n\nComparing the bound for shallow network with bound for Perceptron: as a sense check note both bounds depend on \\(\\gamma^{-2}\\) and \\(R^2\\) (in the case of the shallow network this is through \\(\\eta_w\\)). In addition, for the shallow network we see that a smaller \\(\\alpha\\), i.e., a Leaky ReLU activation closer to ReLU, we get a weaker bound, while at the other end if we chose a linear activation we would get a bound which is very similar! Perhaps the key difference is the dependence on \\(n\\): in particular through \\(\\eta_w\\) the upper bound on the number of iterations of GD scales proportional to \\(n\\) while for the bound for the Perceptron has no dependence on \\(n\\). Given that we use full-batch GD versus single batch SGD for the Perceptron one might expect the bound for the shallow network to be proportional somehow to \\(1/n\\). Instead the upper bound grows proportional to \\(n\\) for the shallow network! This arises as a result of two artefacts of the proof: first, to bound \\(T\\) in terms of \\(U(t)\\) we used the lower bound \\(|\\mathcal{F}(t)| \\geq 1\\) for all \\(t \\leq T\\), i.e., we assume only one point is involved in the update of the parameters at each time step when it could be as large as \\(n\\)! Second, we removed a factor of \\(|\\mathcal{F}(t)|\\) by using a stepsize which is proportional to \\(1/n\\). It is worth highlighting though that the bound for the Perceptron is in terms of the number of non-zero updates as opposed to the number of updates. One could use exactly the same proof to bound the number of non-zero iterations of mini-batch SGD of size \\(k \\in [n]\\) say which would only require the \\(\\eta_w\\) proportional to \\(1/k\\).\nWidth of the network does not matter: the particular choice of \\(m\\) does not actually matter in terms of impacting convergence and the upper bound on the number of iterations. This should not be overly surprising as the target function is linear and could be solved by a single neuron!\nAll neurons are treated the same: the bounds we have written down are derived without having to study the individual dynamics of neurons, or even sub-groups of neurons. Equivalently put, we did not need to study the activation patterns of each neuron on different data points throughout training and how this drives the dynamics. Analysis of activation patterns is in general quite challenging and current works in this regard typically require assumptions such as the input features being nearly orthogonal, see e.g., (George et al. 2023).\nDoes not work for ReLU: we require \\(\\alpha&gt;0\\) as otherwise the lower bound we derived for \\(A(t)\\) does not grow with \\(t\\). This condition means that our results do not cover ReLU. In order to extend to ReLU a similar technique could be deployed if one where to consider instead bounding \\(|\\mathcal{F}(t) \\cap \\mathcal{A}(t)|\\) instead of \\(|\\mathcal{F}(t)|\\). For ReLU networks it is worth highlighting that points which lie in the intersection of each neuron’s inactive halfspace are zeroed by the network and cannot be fitted: this property clearly introduces sub-optimal stationary points! By comparison, Leaky ReLU networks never zero an input unless they are exactly orthogonal to the weight vectors of every neuron, which is a null set.\nScale of the initialization \\(\\lambda_w\\): the condition $_w $ primarily used to simplify the upper bound on \\(F(t)\\). Inspecting the lower bound on \\(A(t)\\) and the upper bound on \\(F(t)\\) then both become weaker as \\(\\lambda_w\\) increases. Moreover, inspecting the quadratic we form in \\(U(t)\\) then for sufficiently large \\(\\lambda_w\\) the roots may be complex. As there are no real roots we can conclude that there is no \\(U(t)\\) for which the quadratic inequality is satisfied and therefore it does not provide an upper bound on \\(U(t)\\) which we can use to bound \\(T\\). Note from this we cannot conclude that GD never terminates, only that we cannot use this technique to prove convergence. In practice and for linearly seperable data I would be confident convergence would happen for large \\(\\lambda_w\\), it would just take longer.\nStep-size \\(\\eta_w\\): the condition that \\(\\eta_w \\leq 1/nR^2\\) simplifies the upper bound on \\(F(t)\\), in particular without this assumption the upper bound would depend on \\(\\sum_{\\tau=1}^{t}|\\mathcal{F}(t)|^2\\) instead of \\(U(t)\\). The dependence of \\(\\eta_w\\) on \\(R^2\\) is primarily for convenience in simplifying the expressions encountered in the proof and is not necessary. Supposing this condition is satisfied then the result agrees with our intuition: the smaller the step size the larger upper bound, which in turn suggests GD will take longer to converge. If \\(\\eta_w\\) is very large, in particular relative to \\(\\lambda_w\\), then the inequality \\(A(t)^2 \\leq 2m F(t)\\) approximately reduces to \\[\n\\left(\\sum_{\\tau=1}^{t} |\\mathcal{F}(t)| \\right)^2 \\leq C \\sum_{\\tau=1}^{t} |\\mathcal{F}(t)|^2.\n\\] This suggests that for \\(\\eta_w\\) much larger than \\(\\lambda_w\\) then we could in theory fit the data in just a few steps! This is a bit surprising as typically convergence is contingent on a small enough step-size… we explore the setting of small \\(\\lambda_w\\) and large \\(\\eta_w\\) in more detail below.\n\n\nDefinition 1"
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#convergence-of-the-perceptron-algorithm-for-linearly-seperable-data",
    "href": "posts/opt-guarantees-perceptron/index.html#convergence-of-the-perceptron-algorithm-for-linearly-seperable-data",
    "title": "Global optimization guarantees for shallow neural networks via the Perceptron algorithm",
    "section": "Convergence of the perceptron algorithm for linearly seperable data",
    "text": "Convergence of the perceptron algorithm for linearly seperable data\nBefore we consider a shallow neural network, let’s first recall the Perceptron algorithm.\n\nInputs: \\((x_i, y_i)_{i \\in [n]}\\)\nAlgorithm:\n\nInitialize \\(w(0) = 0_d\\), \\(t = 1\\)\nWhile there exists a \\(\\pi(t) \\in [n]\\) s.t. \\(y_{\\pi(t)} \\langle w(t), x_{\\pi(t)} \\rangle \\leq 0\\) do\n\n\\(w(t) = w(t-1) + y_{\\pi(t)}x_{\\pi(t)}\\)\n\\(t = t + 1\\)\n\nreturn \\(w(t)\\)\n\n\nIn the above the map \\(\\pi: \\mathbb{N}\\rightarrow [n]\\) selects a point to update at each iteration. Furthermore observe that at each iteration the Perceptron algorithm improves its performance on at least one feature-label pair in the training set as \\(y_{\\pi(t)} \\langle w(t), x_{\\pi(t)} \\rangle = y_{\\pi(t)}\\langle w(t-1), x_{\\pi(t)} \\rangle + 1\\).\nFor linearly separable data it is possible to show the Perceptron algorithm converges to a global minimizer. Recall that linearly separable means there exists a \\(u \\in \\mathbb{R}^d\\), \\(||u ||=1\\) such that \\(y_i \\langle x_i, u \\rangle &gt;0\\) for all \\(i \\in [n]\\) (the requirement that \\(u\\) is unit norm is made for convenience as we can always re-scale by dividing by \\(||u ||\\)). As the decision boundary of the linear classifier \\(u\\) is the vector subspace orthogonal to \\(u\\) then the shortest distance from \\(x_i\\) to this decision boundary is simply \\(|\\langle x_i, u \\rangle|\\). Let \\[\n\\gamma(u) := \\min_{i \\in [n]} |\\langle x_i, u \\rangle|,\n\\] denote the margin of the linear classifier \\(u\\). Without loss of generality we may as well consider the linear classifier with the maximum margin: to this end moving forward we let \\[\n\\gamma := \\max_{||u ||=1} \\min_{i \\in [n]} |\\langle x_i, u \\rangle|\n\\] denote the max margin and \\(u\\) the max margin classifier. For linearly separable data it is possible to show that the Perceptron algorithm terminates after a finite number of steps and successfully classifies all points in the training sample.\n\nLemma 1 If \\((x_i, y_i)_{i \\in [n]}\\) is linearly separable then the Perceptron algorithm terminates after at most \\(T \\leq \\frac{R^2}{\\gamma^2}\\) iterations and \\[ y_i \\langle x_{i} , w(T) \\rangle &gt; 0 \\] for all \\(i \\in [n]\\).\n\n\nProof. The key idea of the proof is to show that \\(| \\langle w(t) , u \\rangle |^2\\) grows faster than \\(|| w(t) | |^2\\). By the Cauchy-Schwarz (CS) inequality, for any iteration \\(t \\geq 1\\) it must hold that\n\\[\n  |\\langle w(t), u \\rangle|^2 \\leq || w(t) ||^2.\n\\] Therefore, there must exist an iteration \\(T\\) after which no further updates take place: if this was not true this would imply for some sufficiently large iteration that the CS inequality is invalid! To lower bound \\(| \\langle w(t) , u \\rangle |^2\\) observe \\[\n\\begin{align*}\n  \\langle w(t), u \\rangle &\\geq \\langle w(t-1), u \\rangle + y_{\\pi(t)} \\langle x_{\\pi(t)} , u \\rangle\\\\\n  & \\geq \\langle w(t-1), u \\rangle + \\gamma\\\\\n  & \\geq \\langle w(t-2), u \\rangle + 2 \\gamma\\\\\n  & \\geq t \\gamma.\n\\end{align*}\n\\] To upper bound \\(||w(t)||^2\\) note \\[\n\\begin{align*}\n  ||w(t)||^2  &=  ||w(t-1) +  y_{\\pi(t)}  x_{\\pi(t)}||^2 \\\\\n  &= || w(t-1) ||^2 + 2y_{\\pi(t)} \\langle x_{\\pi(t)}, w(t-1) \\rangle + ||x_{\\pi(t)}||^2\\\\\n  & \\leq || w(t-1) ||^2 + ||x_{\\pi(t)}||^2\\\\\n  & \\leq|| w(t-1) ||^2 + R^2\\\\\n  & \\leq || w(t-1) ||^2 +  2R^2\\\\\n  & \\leq t R^2.\n\\end{align*}\n\\] Let \\(T\\) denote the iteration at which the Perceptron algorithm terminates where \\(T = \\infty\\) indicates the Perceptron algorithm never terminates. Then by the CS inequality \\[\nT^2 \\gamma^2 \\leq |\\langle w_t, u \\rangle|^2 \\leq || w_t ||^2 \\leq TR^2\n\\] which is finite! Rearranging then \\(T\\leq \\frac{R^2}{\\gamma^2}\\) as claimed."
  },
  {
    "objectID": "posts/opt-guarantees-perceptron/index.html#from-the-perceptron-to-shallow-neural-networks",
    "href": "posts/opt-guarantees-perceptron/index.html#from-the-perceptron-to-shallow-neural-networks",
    "title": "Global optimization guarantees for shallow neural networks via the Perceptron algorithm",
    "section": "From the perceptron to shallow neural networks",
    "text": "From the perceptron to shallow neural networks\nConsider now a shallow neural network \\[\nf(x;W,v) = v^T \\sigma(Wx) = \\sum_{j=1}^{2m} v_j \\sigma( \\langle w_j, x \\rangle)\n\\] where \\(W \\in \\mathbb{R}^{2m \\times d}\\) is the weight matrix of the first layer, \\(w_j\\) is the \\(j\\)th row of \\(W\\), \\(v \\in \\mathbb{R}^{2m}\\) is the vector of output weights and \\(v_j\\) is the \\(j\\)th entry of \\(v\\). We focus on the Leaky-ReLU non-linearity \\(\\sigma(z) = \\max \\{ \\alpha z, z \\}\\) for some \\(\\alpha \\in (0,1)\\). Consider learning the parameters \\(W\\) and \\(v\\) using gradient descent (GD) with the hinge loss (recall the hinge loss is defined as \\(\\ell(z) = \\max \\{0 , 1 - z \\}\\)). Using the minimum norm subgradient (as is standard in most automatic differentiation software) at zero we let \\(\\ell'(z) = -1\\) for \\(z &lt; 1\\) and is \\(0\\) otherwise, equivalently we can write this as \\(\\ell'(z) = - \\mathbb{1}(z&lt;1)\\). Similarly \\(\\sigma'(z) = \\alpha\\) for \\(z \\leq 0\\) and is \\(1\\) otherwise. A straightforward calculation gives \\[\n\\begin{align*}\n&\\frac{\\partial f}{\\partial w_j}(x;W,v) = v_j \\sigma'(\\langle w_j, x \\rangle) x,\\\\\n&\\frac{\\partial f}{\\partial v_j}(x;W,v) =\\sigma(\\langle w_j, x \\rangle).\\\\\n\\end{align*}\n\\] Consider training the network with gradient descent in order to minimize the loss \\(L(W,v) = \\sum_{i=1}^n \\ell(y_i f(x_i, W,v))\\): as \\[\n\\frac{\\partial L}{\\partial \\theta} = \\sum_{i \\in [n]} \\ell'(y_i f(x_i; W, v))y_i \\frac{\\partial f}{\\partial \\theta}(x_i;W,v)\n\\] then the gradient updates for each neuron \\(j \\in [2m]\\) are \\[\n\\begin{align*}\n& w_j(t) =  w_j(t-1) + \\eta_w \\sum_{i =1}^{n} \\mathbb{1}[y_i f(x_i; W(t-1), v(t-1))&lt;1]  v_j(t-1) \\sigma'(\\langle w_j(t-1), x_i \\rangle) y_i x_i,\\\\\n& v_j(t) = v_j(t-1) + \\eta_v \\sum_{i =1}^{n} \\mathbb{1}[y_i f(x_i; W(t-1), v(t-1))&lt;1] \\sigma(\\langle w_j(t-1), x_i \\rangle),\n\\end{align*}\n\\] where \\(\\eta_w, \\eta_v &gt; 0\\) are the learning rates for the inner and outer weights respectively. Inspecting these equations we see that shallow networks trained with hinge loss and GD have a similar update rule to the Perceptron algorithm! Inspired by this observation we pursue the idea of comparing the growth rate of \\(| \\langle v_j(t) w_j(t), u \\rangle |^2\\) versus \\(|| v_j(t)w_j(t) ||^2\\) for each \\(j \\in [2m]\\). In particular, analogous to studying the growth of \\(\\langle w(t), u \\rangle\\) for the Perceptron algorithm, we instead study \\[\n  A(t) = \\sum_{j=1}^{2m}\\langle v_j(t) w_j(t), u \\rangle.\n\\] Here we use A for alignment and \\(u\\) again denotes the unit norm weights of the max margin linear classifier. Similarly, instead of considering \\(||w(t)||^2\\) we study \\[\n  F(t) = || diag(v(t)) W(t) ||_F^2\n\\] (we use F for Frobenius) where \\(diag(v(t))\\) is a diagonal square matrix with \\([diag(v(t))]_{jj} = v_j(t)\\). Again by the CS inequality and assuming the data is linearly separable with max margin unit vetor \\(u\\), then it must hold that \\[\nA^2(t) = |\\langle vec(diag(v(t)) W(t)), u \\otimes 1_{2m} \\rangle|^2 \\leq F(t) 2m.\n\\]\nTherefore, if we can show that \\(F(t)\\) grows slower than \\(A(t)\\) under the outlined gradient update rule there must be a finite iteration after which the update is zero and the parameters no longer change. We will do this by lower bounding \\(A(t)\\) and upper bounding \\(F(t)\\) in terms of the total number of times each point in the training set contributes to the update of the weights at time \\(t\\), which we denote \\(U(t) = \\sum_{\\tau=1}^{t} |\\mathcal{F}(\\tau)|\\). This is useful as we can bound the number of updates \\(T\\) by considering the setting where only one point contributes to the update at each time step, leading to the bound \\(T \\leq U(t)\\).\n\nTraining with frozen outer weights\nFirst we consider a simplified setting where we freeze the outer weights by setting \\(\\eta_v = 0\\): in this setting we are able to prove analogous results to the Perceptron setting.\n\nLemma 2 Suppose the training data \\((x_i, y_i)_{i\\in [n]}\\) is linearly separable, assume \\(\\eta_v = 0\\), \\(\\eta_w \\leq \\frac{1}{nR^2}\\), \\(v_j(0) = (-1)^j\\) and \\(||w_j(0) || \\leq \\lambda_w\\) for all \\(j \\in [2m]\\). Then gradient descent will terminate after \\[\nT &lt; \\frac{2 (\\lambda_w \\alpha \\gamma +1)}{ \\eta_w \\alpha^2 \\gamma^2}\n\\] iterations, at which point the network will have achieved zero training (hinge) loss.\n\n\nProof. First we lower bound \\(A(t)\\). Let \\(\\mathcal{F}(t) = \\{ i \\in [n] \\; : \\; y_i f(x_i; \\theta(t -1)) &lt; 1\\}\\) denote the set of training points that do not have zero hinge loss at the previous iterate \\(t-1\\) (alternatively we can view this set as the points that participate in the update of the weights at the current iterate \\(t\\)). For convenience using \\(\\sigma'_{ji}(t) := \\sigma'(\\langle w_j(t), x_i \\rangle)\\) then recalling the gradient update rule we have \\[\n\\begin{align*}\n  w_j(t) =  w_j(t-1) + \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} (-1)^j \\sigma'_{ji}(t-1) y_i x_i.\n\\end{align*}\n\\] As \\(y_i \\langle x_i, u \\rangle \\geq \\gamma\\) and \\(\\sigma'_{ji}(t) \\geq \\alpha &gt;0\\) then \\[\n\\begin{align*}\n  A(t) &= \\sum_{j=1}^{2m} (-1)^j\\langle w_j(t), u \\rangle \\\\\n  &= A(t-1) + \\eta_w \\sum_{j=1}^{2m} \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sigma'_{ji}(t) y_i \\langle x_i, u\\rangle \\\\\n  & \\geq  A(t-1) + \\eta_w \\gamma \\alpha \\sum_{j=1}^{2m} \\sum_{i \\in \\mathcal{F}(t)}^{n} 1\\\\\n  & = A(t-1) + \\eta_w \\gamma \\alpha 2m |\\mathcal{F}(t)|\\\\\n  & = A(0) + \\eta_w \\gamma \\alpha 2m \\sum_{\\tau = 1}^{t} |\\mathcal{F}(\\tau)|.\n\\end{align*}\n\\] Under our assumptions on the initialization scale \\[\n\\begin{align*}\nA(0) &= \\sum_{j=1}^{2m} (-1)^j\\langle w_j(0), u \\rangle\\\\\n& \\geq  - \\sum_{j=1}^{2m} | \\langle w_j(0), u \\rangle| \\\\\n& \\geq - 2m \\lambda_w.\n\\end{align*}\n\\] Therefore, defining \\(U(t) = \\sum_{\\tau = 1}^{t} |\\mathcal{F}(\\tau)|\\) we have \\[\n  A(t) \\geq -2m \\lambda_w + 2m \\eta_w \\alpha \\gamma U(t).\n\\] Before proceeding it is worth emphasizing that we need \\(\\alpha&gt;0\\) to make this bound non-trivial and increasing in \\(t\\). To extend this technique to ReLU networks one might instead explore bounding for each neuron \\(j \\in [2m]\\) the set \\(| \\mathcal{F}(t) \\cap \\mathcal{A}_j(t) |\\), where \\(\\mathcal{A}_j(t) = \\{i \\in [n]: \\; \\langle w_j(t), x_i \\rangle &gt; 0 \\}\\).\nFor the upper bound on \\(F(t)\\) we again plug in the GD update rule, \\[\n\\begin{align*}\nF(t)\n&= \\sum_{j=1}^{2m} ||w_j(t) ||^2\\\\\n&= \\sum_{j=1}^{2m} || w_j(t-1) + \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} (-1)^j \\sigma'_{ji}(t-1) y_i x_i ||^2\\\\\n&= \\sum_{j=1}^{2m}||w_j(t-1) ||^2 + 2\\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sum_{j=1}^{2m}(-1)^j\\sigma'_{ji}(t-1)y_i \\langle w_j(t-1),  x_i \\rangle \\\\\n&+\\eta_w^2 \\sum_{j =1}^{2m} \\sum_{i,k \\in \\mathcal{F}(t)}^{n}  (-1)^{2j}\\sigma'_{ji}(t-1)\\sigma'_{ki}(t-1)\\langle x_i, x_k \\rangle.\n\\end{align*}\n\\] The first term in the above is simply \\(F(t-1)\\). The second term can be simplified by observing that \\(\\sigma(z) = \\sigma'(z) z\\) for a piecewise linear function \\(\\sigma\\). Therefore \\[\n\\begin{align*}\n  \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sum_{j=1}^{2m}(-1)^j\\sigma'_{ji}(t-1) \\langle w_j(t-1),  x_i \\rangle &= \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} \\sum_{j=1}^{2m}(-1)^j\\sigma(\\langle w_j(t-1),  x_i \\rangle)\\\\\n  &= \\eta_w \\sum_{i \\in \\mathcal{F}(t)}^{n} y_i f(x_i; W(t-1))\\\\\n  & &lt; \\eta_w |\\mathcal{F}(t)|,\n\\end{align*}\n\\] where the final inequality follows from the fact that \\(i \\in \\mathcal{F}(t)\\) implies \\(y_i f(x_i; W(t-1))&lt;1\\). Finally, the third term in the previous bound on \\(F(t)\\) can be simplified using the inequalities \\(\\sigma_{ji}(t-1)\\leq 1\\), \\(||x_i||\\leq R\\), \\(\\eta_w\\leq1/nR^2\\), \\(|\\mathcal{F}(t)| \\leq n\\), which imply \\[\n\\eta_w^2 \\sum_{j =1}^{2m} \\sum_{i,k \\in \\mathcal{F}(t)}^{n}  (-1)^{2j}\\sigma'_{ji}(t-1)\\sigma'_{ki}(t-1)\\langle x_i, x_k \\rangle \\leq 2m\\eta_w^2 R^2 |\\mathcal{F}(t)|^2 \\leq 2m \\eta_w |\\mathcal{F}(t)|.\n\\] Note the bound placed on the step-size means we avoid having to work with \\(|\\mathcal{F}(t)|^2\\). Proceeding, it follows that \\[\n\\begin{align*}\nF(t) &&lt; F(t-1) + (2 + 2m )\\eta_w |\\mathcal{F}(t)|\\\\\n& \\leq F(t-1) + 4m \\eta_w |\\mathcal{F}(t)|\\\\\n&&lt; F(0) + 4 m\\eta_w \\sum_{\\tau=1}^t |\\mathcal{F}(t)|\\\\\n& = F(0) + 4 m\\eta_w  U(t)\\\\\n& = \\sum_{j=1}^{2m} ||w_j(0)||^2 + 4 m\\eta_w U(t)\\\\\n& \\leq 2m \\lambda_w^2 + 4 m\\eta_w U(t).\n\\end{align*}\n\\] By the CS inequality and dividing both sides by \\(4m^2\\) then \\[\n\\frac{A^2(t)}{4m^2} \\leq \\frac{F(t)}{2m}.\n\\] Plugging in the bounds for \\(A(t)\\) and \\(F(t)\\), using \\(\\lambda_w \\leq 1\\) and rearranging produces the following sequence of inequalities, \\[\n\\begin{align*}\n(\\eta_w \\alpha \\gamma U(t) - \\lambda_w)^2 &&lt; \\lambda_w^2 + 2 \\eta_w U(t)\\\\\n\\eta_w^2 \\alpha^2 \\gamma^2U^2(t) - 2 \\lambda_w \\eta_w \\alpha \\gamma U(t) + \\lambda_w^2 &&lt; \\lambda_w^2 + 2 \\eta_w U(t)\\\\\n\\eta_w \\alpha^2 \\gamma^2U^2(t)  &&lt; 2 (\\lambda_w \\alpha \\gamma +1)  U(t) ,\\\\\nU(t) &\\leq \\frac{2 (\\lambda_w \\alpha \\gamma +1)}{ \\eta_w \\alpha^2 \\gamma^2}.\n\\end{align*}\n\\] To conclude, observe if \\(T\\) is the final iterate then \\[\n  T = \\sum_{\\tau = 1}^{T} 1 \\leq \\sum_{\\tau = 1}^{T} |\\mathcal{F}(\\tau)| = U(T) &lt;\\frac{2 (\\lambda_w \\alpha \\gamma +1)}{ \\eta_w \\alpha^2 \\gamma^2}.\n\\]\n\nA few reflections on this result are as follows.\n\nComparing the bound for shallow network with bound for Perceptron: as a sense check note both bounds have a term proportional to \\(\\gamma^{-2}\\) and \\(R^2\\) (observe in the case of the shallow network this is through \\(\\eta_w\\)). In addition, for the shallow network we see that a smaller \\(\\alpha\\), i.e., a Leaky ReLU activation closer to ReLU, we get a weaker bound, while at the other end if we chose a linear activation we would get a bound which is very similar! Perhaps the key difference is that through \\(\\eta_w\\) the upper bound on the number of iterations of GD scales proportional to \\(n\\) while the bound for the Perceptron has no dependence on \\(n\\). Given that we use full-batch GD versus single batch SGD for the Perceptron one might expect the bound for the shallow network to be proportional somehow to \\(1/n\\), however instead the upper bound grows proportional to \\(n\\) for the shallow network! This arises as a result of two artefacts of the proof: first, to bound \\(T\\) in terms of \\(U(t)\\) we use the lower bound \\(|\\mathcal{F}(t)| \\geq 1\\) for all \\(t \\leq T\\), i.e., we assume only one point is involved in the update of the parameters at each time step when it could be as large as \\(n\\). Second, we removed a factor of \\(|\\mathcal{F}(t)|\\) by using a step-size which is proportional to \\(1/n\\). It is worth highlighting that the bound for the Perceptron is in terms of the number of non-zero updates as opposed to the number of updates. One could use exactly the same proof to bound the number of non-zero iterations of mini-batch SGD of size \\(k \\in [n]\\) say which would only require the \\(\\eta_w\\) proportional to \\(1/k\\) and would thereby remove the dependence on \\(n\\).\nWidth of the network does not matter: the particular choice of \\(m\\) does not actually matter in terms of impacting the upper bound on the number of iterations. This should not perhaps be overly surprising as the target function is linear and could be solved by a single neuron!\nAll neurons are treated the same: the bounds we have written down are derived without having to study the individual dynamics of neurons, or even sub-groups of neurons. Equivalently put, we used the same bound for every neuron and every time step and avoided considering the activation patterns of each neuron on different data points at different time steps. Analysis of activation patterns is in general quite challenging and current works in this regard typically require additional assumptions on the data, such as the input features being nearly orthogonal, see e.g., (George et al. 2023).\nDoes not work for ReLU: we require \\(\\alpha&gt;0\\) as otherwise the lower bound we derived for \\(A(t)\\) does not grow with \\(t\\). This condition means that our results do not cover ReLU. In order to extend to ReLU a similar technique could be deployed if one where to consider instead bounding \\(|\\mathcal{F}(t) \\cap \\mathcal{A}(t)|\\) instead of \\(|\\mathcal{F}(t)|\\). For ReLU networks it is worth highlighting that points which lie in the intersection of each neuron’s inactive halfspace are zeroed by the network and cannot be fitted: this property clearly introduces sub-optimal stationary points! By comparison, Leaky ReLU networks never zero an input unless they are exactly orthogonal to the weight vectors of every neuron, which is a null set.\nScale of the initialization \\(\\lambda_w\\): for linearly separable data the scale of the initialization does not impact convergence, however a larger initialization scale increases the upper bound on \\(U(t)\\) which perhaps suggests that it slows down convergence.\nStep-size \\(\\eta_w\\): the condition \\(\\eta_w \\leq 1/nR^2\\) simplifies the upper bound on \\(F(t)\\), in particular without this assumption the upper bound would depend on \\(\\sum_{\\tau=1}^{t}|\\mathcal{F}(t)|^2\\) instead of \\(U(t)\\). The dependence of \\(\\eta_w\\) on \\(R^2\\) is primarily for convenience in simplifying the expressions encountered in the proof and is not necessary. Observing the role of the step-size in the upper bound then, and agreeing with our intuition, the smaller the step size the larger upper bound, which in turn perhaps suggests GD will take longer to converge. If \\(\\eta_w\\) is sufficiently large relative to \\(\\alpha\\), \\(\\gamma\\) and \\(\\lambda_w\\) then the inequality \\(A(t)^2 \\leq 2m F(t)\\) can be reduced to the form \\[\n\\left(\\sum_{\\tau=1}^{t} |\\mathcal{F}(t)| \\right)^2 \\leq C \\sum_{\\tau=1}^{t} |\\mathcal{F}(t)|^2\n\\] for some constant \\(C\\) independent of \\(n\\) and \\(t\\). As the right-hand-side grows linearly with \\(t\\) while the left-hand-side quadratically then there must be some finite \\(T\\) at which the inequality no longer holds. At this point GD must have terminated, despite the large step-size!\n\n\n\nTraining both the inner and outer layers\nA natural question one might ask is what happens if you unfreeze the outer layer weights, i.e., \\(\\eta_u \\geq 0\\)? Practically we should expect nothing to change (you can try this experimentally if you like!), however from the perspective of deriving guarantees our job becomes harder. The first challenge is actually deriving bounds for \\(A(t)\\) and \\(F(t)\\) which are non-trivial: generally given bounded data it will be possible to upper bound at least crudely \\(F(t)\\), however a positive lower bound on \\(A(t)\\) we will shortly observe is trickier. Second, even if we are able to derive upper and lower bounds this technique only works if the lower bound on \\(A(t)\\) grows faster than \\(F(t)\\) with \\(t\\).\nConsider the lower bound on \\(A(t)\\): for convenience we use the notation \\(\\phi_{ji}(t) = \\phi(\\langle w_j(t), x_i \\rangle)\\) where \\(\\phi \\in \\{\\sigma, \\sigma' \\}\\), from the update rules we derived previously we have the following. \\[\n\\begin{align*}\nA(t+1) &= \\sum_{j=1}^{2m}v_j(t+1) \\langle w_j(t+1) ,u \\rangle\\\\\n& = \\sum_{j=1}^{2m} \\left( v_j(t) + \\eta_v \\sum_{i \\in \\mathcal{F}(t)} \\sigma_{ji}(t) \\right) \\left(\\langle w_j(t) + \\eta_w \\sum_{k \\in \\mathcal{F}(t)} v_j(t) \\sigma_{jk}'(t)y_k x_k  , u \\rangle \\right)\\\\\n& = \\sum_{j=1}^{2m} \\left( v_j(t) + \\eta_v \\sum_{i \\in \\mathcal{F}(t)} \\sigma_{ji}(t) \\right) \\left(\\langle w_j(t), u \\rangle + \\eta_w \\sum_{k \\in \\mathcal{F}(t)} v_j(t) \\sigma_{jk}'(t) y_k \\langle x_k ,u \\rangle \\right)\\\\\n& = \\sum_{j=1}^{2m} v_j(t) \\langle w_j(t), u \\rangle\\\\\n&+ \\eta_w \\sum_{k \\in \\mathcal{F}(t)} \\sum_{j=1}^{2m}v_j^2(t) \\sigma_{jk}'(t) y_k \\langle x_k ,u \\rangle\\\\\n&+ \\eta_v \\sum_{i \\in \\mathcal{F}(t)} \\sum_{j=1}^{2m} \\sigma_{ji}(t) \\langle w_j(t), u \\rangle \\\\\n&+ \\eta_v \\eta_w \\sum_{i,k \\in \\mathcal{F}(t)} \\sum_{j=1}^{2m} v_j(t) \\sigma_{ji}(t) \\sigma_{jk}'(t) y_k \\langle x_k ,u \\rangle.\n\\end{align*}\n\\] The first of the above four terms is simply \\(A(t)\\), while the second term is positive as \\[\n\\eta_w \\sum_{k \\in \\mathcal{F}(t)} \\sum_{j=1}^{2m}v_j^2(t) \\sigma_{jk}'(t) y_k \\langle x_k ,u \\rangle \\geq ||v(t) ||^2 \\alpha \\gamma.\n\\] However, bounding the other two terms from below is challenging and requires a more fine-grained understanding of the dynamics of each neuron. Note in the case where the outer weights was frozen we found ourselves in the easy position where we could apply the same bound to each neuron at each time step. In this sense we were actually able to treat all neurons identically as we could uniformly (both in time and neuron index) bound the alignment with \\(u\\) as well as the growth of the norm."
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html",
    "href": "posts/landscape-quadratic-net/index.html",
    "title": "A simple neural network with a benign loss landscape",
    "section": "",
    "text": "We look at an example of a simple network whose loss landscape, albeit non-convex, is relatively benign from an optimization perspective. This post is based on Theorem 2.1 of (Soltanolkotabi, Javanmard, and Lee 2019)."
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#introduction",
    "href": "posts/landscape-quadratic-net/index.html#introduction",
    "title": "A simple neural network with a benign loss landscape",
    "section": "Introduction",
    "text": "Introduction\nA surprising aspect when it comes to training neural networks is that first order optimization methods appear to be pretty effective despite the fact that the loss landscape they traverse is typically non-convex. A nice line of work, in particular (Lee et al. 2016), (Du et al. 2017) and related, state results under mild conditions on the initialization and objective function to the effect that if gradient descent converges then it converges to a local minimum almost surely. However, although roughly speaking perturbed versions of gradient descent are able to escape saddles, there is still the question of why do first order methods not get stuck in bad local minima? A popular hypothesis for explaining this is that at least for some networks, e.g., those which are sufficiently overparameterized, then local minima are sufficiently `rare’ versus global minima that they are unlikely to be encountered during training.\nHere we look at a simple example which illustrates a particularly strong flavor of this idea: in short, for a sufficiently wide, shallow network with quadratic activations then for general data it can be shown that all local minima are global and have zero loss (Theorem 2.1 of (Soltanolkotabi, Javanmard, and Lee 2019))!"
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#setup",
    "href": "posts/landscape-quadratic-net/index.html#setup",
    "title": "An example of a benign loss landscape for a neural network",
    "section": "Setup",
    "text": "Setup\nConsider training sample of \\(n\\) feature-label pairs \\((x_i, y_i)_{i \\in [n]}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\mathbb{R}\\). Let \\(f:\\mathbb{R}^d \\times \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}\\) be a neural network defined as \\[\nf(x;W) = \\frac{1}{2} \\sum_{j=1}^{2m}(-1)^j (w_j^Tx)^2\n\\] where \\(w_j\\) denotes the \\(j\\)th row of \\(W\\). We seek to minimize the squared error loss of this network over the training sample \\(L:\\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}\\), \\[\nL(W) = \\frac{1}{2} \\sum_{i =1}^n ( f(x_i, W) - y_i )^2.\n\\] Observe that \\(L\\) is twice differentiable, we denote the gradient of \\(L\\) as \\(\\nabla L : \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2md}\\) and the Hessian as \\(\\nabla^2L: \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2md \\times 2md}\\). Defining \\(r_i(W):= f(x_i;W) - y_i \\in \\mathbb{R}\\), then for any \\(W \\in \\mathbb{R}^{2m \\times d}\\) the gradient of \\(L\\) at \\(W\\) can be written as \\[\n\\nabla L(W) = \\sum_{i=1}^n r_i(W) \\nabla_W f(x_i; W).\n\\] As \\(\\frac{\\partial f(x_i; W)}{\\partial w_r} = (-1)^r (w_r^T x_i) x_i\\), then letting \\(D \\in \\mathbb{R}^{2m \\times 2m}\\) be a diagonal matrix with non-zero entries \\(D_jj = (-1)^j\\) we have \\[\n\\begin{align*}\n\\nabla_W f(x_i; W) = (DW x_i) \\otimes x_i.\n\\end{align*}\n\\] Therefore \\[\n\\begin{align*}\n\\nabla L(W) &= \\sum_{i=1}^n r_i(W) \\nabla_W f(x_i; W)\\\\\n&= \\sum_{i=1}^n r_i(W) (DW x_i) \\otimes x_i.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#all-local-minima-are-global",
    "href": "posts/landscape-quadratic-net/index.html#all-local-minima-are-global",
    "title": "An example of a benign loss landscape for a neural network",
    "section": "All local minima are global",
    "text": "All local minima are global\nThe following is a rephrased version of Theorem 2.1 (Soltanolkotabi, Javanmard, and Lee 2019).\n\nTheorem 1 If \\(m \\geq d\\) then the following three properties hold for \\(L(W)\\).\n\nAll local minima are global: if \\(\\nabla L(W^*)=0\\) and \\(\\nabla^2L(W^*) \\succcurlyeq 0\\) then \\(L(W^*) \\leq L(W)\\) for all \\(W \\in \\mathbb{R}^{2m \\times d}\\).\nAll saddle points have a direction of strictly negative curvature\nIf \\(n \\lesssim d^2\\) then for almost all datasets any global optimum \\(W^*\\) of \\(L\\) satisfies \\(L(W^*)=0\\).\n\n\n\nProof. We start by proving points 1 and 2. Note as the loss is twice differentiable any local minima or saddle must satisfy the following conditions \\[\n  \\begin{align*}\n    \\nabla L (W) &= 0,\\\\\n     \\nabla^2L(W) &\\succcurlyeq 0.\n  \\end{align*}\n  \\] We proceed to show that any \\(W\\) satisfying these conditions must be a global minimum which in turn implies there are no spurious local minima or non-strict saddles."
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#setup-notation-and-useful-identities",
    "href": "posts/landscape-quadratic-net/index.html#setup-notation-and-useful-identities",
    "title": "An example of a benign loss landscape for a neural network",
    "section": "Setup, notation and useful identities",
    "text": "Setup, notation and useful identities\nBefore we proceed some small points on notation [WIP]…\n\nWe use \\(A \\otimes B\\) denote the Kronecker product between matrices \\(A\\) and \\(B\\).\nWe use \\(A \\star B\\) denote the Khatri-Rao product between matrices \\(A\\) and \\(B\\).\n\nConsider training sample of \\(n\\) feature-label pairs \\((x_i, y_i)_{i \\in [n]}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\mathbb{R}\\). Let \\(f:\\mathbb{R}^d \\times \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}\\) be a neural network defined as \\[\nf(x;W) = \\frac{1}{2} \\sum_{j=1}^{2m}(-1)^j (w_j^Tx)^2\n\\] where \\(w_j\\) denotes the \\(j\\)th row of \\(W\\). We seek to minimize the squared error loss of this network over the training sample \\(L:\\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}\\), \\[\nL(W) = \\frac{1}{2} \\sum_{i =1}^n ( f(x_i, W) - y_i )^2.\n\\] Observe that \\(L\\) is twice differentiable, we denote the gradient of \\(L\\) as \\(\\nabla L : \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2md}\\) and the Hessian as \\(\\nabla^2L: \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2md \\times 2md}\\).\n\nFirst order derivatives\nDefining \\(r_i(W):= f(x_i;W) - y_i \\in \\mathbb{R}\\), then for any \\(W \\in \\mathbb{R}^{2m \\times d}\\) the matrix-scalar derivative \\(\\partial_W L: \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2m \\times d}\\) is \\[\n\\partial_W L(W) = \\sum_{i=1}^n r_i(W) \\partial_W f(x_i; W).\n\\] As \\(\\frac{\\partial f(x_i; W)}{\\partial w_r} = (-1)^r (w_r^T x_i) x_i\\), then letting \\(D \\in \\mathbb{R}^{2m \\times 2m}\\) be a diagonal matrix with non-zero entries \\(D_jj = (-1)^j\\) we have \\[\n\\begin{align*}\n\\partial_W f(x_i; W) = DW x_i x_i^T.\n\\end{align*}\n\\] Therefore \\[\n\\begin{align*}\n\\partial_W L(W) &= \\sum_{i=1}^n r_i(W) \\partial_W f(x_i; W)\n= DW \\sum_{i=1}^n r_i(W)  x_i x_i^T.\n\\end{align*}\n\\] Similarly as \\(\\nabla_W f(x; W) = (DW x_i) \\otimes x_i\\) \\[\n\\begin{align*}\n\\nabla L(W) = \\sum_{i=1}^n r_i(W) \\nabla_W f(x_i; W)\n&= \\sum_{i=1}^n r_i(W) (DW x_i) \\otimes x_i,\n\\end{align*}\n\\] which is just the flattened or vectorized way of writing \\(\\partial_W L\\). We can also write this in terms of the Jacobian \\(J_F(W)\\in \\mathbb{R}^{2md \\times n}\\) of the network on the training sample: if \\(F(W) := [f(x_1; W), f(x_2;W)... f(x_n; W) ]^T \\in \\mathbb{R}^{n}\\) then\n\\[\n\\begin{align*}\nJ_F(W) &:= \\nabla_W F(W)\\\\\n&= [\\nabla_W f(x_1; W),  \\nabla_W f(x_2; W)... \\nabla_W f(x_n; W)]\\\\\n&= [DWx_1 \\otimes x_1, DWx_2 \\otimes x_2... DWx_n \\otimes x_n] \\\\\n&= DW \\star X\n\\end{align*}\n\\]\n\n\nSecond order derivatives\nTurning our attention to the Hessian,"
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#preliminaries",
    "href": "posts/landscape-quadratic-net/index.html#preliminaries",
    "title": "A simple neural network with a benign loss landscape",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nPoints on notation\nGiven a matrix \\(A \\in \\mathbb{R}^{m \\times d}\\) then \\(a \\in \\mathbb{R}^{d}\\) denotes its \\(i\\)th row. We denote the entry at the \\(i\\)th row and \\(j\\)th column either as \\([A]_{ij}\\) or \\(a_{ij}\\). Given a scalar function \\(f:\\mathbb{R}^{m \\times d} \\rightarrow \\mathbb{R}\\) we use \\(\\partial_A f(A) \\in \\mathbb{R}^{m \\times d}\\) to denote the matrix-scalar derivative of \\(f\\) with respect to \\(A\\), in particular \\([\\partial_A f(A)]_{ij} = \\frac{\\partial f(A)}{\\partial A_ij}\\). Given a matrix \\(A \\in \\mathbb{R}^{m \\times d}\\) we use \\(vec(A) \\in \\mathbb{R}^{md}\\) to denote the vectorized or flattened version of \\(A\\). Given a matrix or vector \\(A \\in \\mathbb{R}^{m \\times d}\\) and a scalar function \\(f(A)\\) then we use \\(\\nabla_A f(A)\\) to denote the gradient, equivalently \\(\\nabla_A f(A) = \\partial_{vec(A)} f(A)\\). We use \\(A \\otimes B\\) denote the Kronecker product between matrices \\(A\\) and \\(B\\). We use \\(A \\star B\\) denote the Khatri-Rao product between matrices \\(A\\) and \\(B\\). If it is not clear from the context what the dimensions or shape of a tensor are we will use a subscript to clarify, this is most common when working with the 0 tensor.\n\n\nData, network and loss\nConsider a training sample of \\(n\\) input-label pairs \\((x_i, y_i)_{i \\in [n]}\\) where \\(x_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\mathbb{R}\\), we use \\(X \\in \\mathbb{R}^{d \\times n}\\) to denote the matrix of training inputs stored column-wise. Let \\(f:\\mathbb{R}^d \\times \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}\\) be a neural network defined as \\[\nf(x;W) = \\frac{1}{2} \\sum_{j=1}^{2m}(-1)^j (w_j^Tx)^2\n\\] where \\(w_j\\) denotes the \\(j\\)th row of \\(W\\). We seek to minimize the squared error loss \\(L:\\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}\\) of this network over the training sample, \\[\nL(W) = \\frac{1}{2} \\sum_{i =1}^n ( f(x_i, W) - y_i )^2.\n\\] Observe that \\(L\\) is twice differentiable, we denote the gradient of \\(L\\) as \\(\\nabla L : \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2md}\\) and the Hessian as \\(\\nabla^2L: \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2md \\times 2md}\\).\n\n\nFirst order derivatives\nLet \\(r(W) \\in \\mathbb{R}^n\\) be the residual vector with entries \\(r_i(W):= f(x_i;W) - y_i\\), then for any \\(W \\in \\mathbb{R}^{2m \\times d}\\) the matrix-scalar derivative \\(\\partial_W L: \\mathbb{R}^{2m \\times d} \\rightarrow \\mathbb{R}^{2m \\times d}\\) is \\[\n\\partial_W L(W) = \\sum_{i=1}^n r_i(W) \\partial_W f(x_i; W).\n\\] As \\(\\frac{\\partial f(x_i; W)}{\\partial w_r} = (-1)^r (w_r^T x_i) x_i\\), then letting \\(D \\in \\mathbb{R}^{2m \\times 2m}\\) be a diagonal matrix with non-zero entries \\(D_{jj} = (-1)^j\\) we have \\[\n\\begin{align*}\n\\partial_W f(x_i; W) = DW x_i x_i^T.\n\\end{align*}\n\\] Therefore \\[\n\\begin{align*}\n\\partial_W L(W) &= \\sum_{i=1}^n r_i(W) \\partial_W f(x_i; W)\n= DW \\sum_{i=1}^n r_i(W)  x_i x_i^T.\n\\end{align*}\n\\] Similarly as \\(\\nabla_W f(x; W) = (DW x_i) \\otimes x_i\\) \\[\n\\begin{align*}\n\\nabla L(W) = \\sum_{i=1}^n r_i(W) \\nabla_W f(x_i; W)\n&= \\sum_{i=1}^n r_i(W) (DW x_i) \\otimes x_i.\n\\end{align*}\n\\] We can also write this in terms of the Jacobian \\(J_F(W)\\in \\mathbb{R}^{2md \\times n}\\) of the network on the training sample: if \\(F(W) := [f(x_1; W), f(x_2;W)... f(x_n; W) ]^T \\in \\mathbb{R}^{n}\\) then\n\\[\n\\begin{align*}\nJ_F(W) &:= \\nabla_W F(W)\\\\\n&= [\\nabla_W f(x_1; W),  \\nabla_W f(x_2; W)... \\nabla_W f(x_n; W)]\\\\\n&= [DWx_1 \\otimes x_1, DWx_2 \\otimes x_2... DWx_n \\otimes x_n] \\\\\n&= DWX \\star X.\n\\end{align*}\n\\] As a result \\[\n\\nabla L(W) = J_F(W)r(W).\n\\]\n\n\nSecond order derivatives\nTurning our attention to the Hessian, observe as \\(L(W) = \\frac{1}{2} \\sum_{i=1}^n r_i(W)^2\\) then \\[\n\\begin{align*}\n\\frac{\\partial^2 L(W)}{\\partial w_{rj} w_{lk}} &= \\frac{\\partial}{\\partial w_{rj}} \\sum_{i=1}^n r_i(W) \\frac{\\partial f(x_i; W)}{\\partial w_{lk}}\\\\\n&= \\sum_{i=1}^n \\frac{\\partial f(x_i; W)}{\\partial w_{rj}} \\frac{\\partial f(x_i; W)}{\\partial w_{lk}} +  \\sum_{i=1}^n r_i(W) \\frac{\\partial^2 f(x_i; W)}{\\partial w_{rj}\\partial w_{lk}}\n\\end{align*}\n\\] Recall \\(\\frac{\\partial f(x_i;W)}{\\partial w_{rj}} = (-1)^r (w_r^T x_i) x_{ij}\\), as a result for \\(r \\neq l\\) then \\(\\frac{\\partial^2 f(x_i;W)}{\\partial w_{rj} \\partial w_{lk}} = 0\\) while \\(\\frac{\\partial^2 f(x_i;W)}{\\partial w_{rj} \\partial w_{rk}} = (-1)^r x_{ij} x_{ik}\\). Therefore \\[\n\\frac{\\partial^2 L(W)}{\\partial w_{rj} w_{lk}} = \\sum_{i=1}^n (-1)^{r + l} (w_r^T x_i) (w_l^T x_i) x_{ij}x_{ik} + \\mathbb{1}_{r = l} r_i(W) (-1)^r x_{ij} x_{ik}.\n\\] As a result \\[\n\\frac{\\partial^2 L(W)}{\\partial w_{r} w_{l}} = \\sum_{i=1}^n (-1)^{r + l} (w_r^T x_i) (w_l^T x_i) x_{i}x_{i}^T + \\mathbb{1}_{r = l} r_i(W) (-1)^r x_{i} x_{i}^T.\n\\] Based on this observe \\[\n\\begin{align*}\n  &vec(U)^T \\nabla_W^2 L(W) vec(U) \\\\\n  &= \\sum_{r=1}^{2m}\\sum_{l=1}^{2m} u_r^T \\frac{\\partial^2 L(W)}{\\partial w_{r} w_{l}} u_l\\\\\n  &= \\sum_{r=1}^{2m}\\sum_{l=1}^{2m} \\left( \\sum_{i=1}^n (-1)^{r + l} (w_r^T x_i) (w_l^T x_i) (u_r^T x_{i})(x_{i}^Tu_l) + \\mathbb{1}_{r = l} r_i(W) (-1)^r (u_r^T x_{i})(x_{i}^Tu_l) \\right)\\\\\n  &=  \\sum_{i=1}^n \\sum_{r=1}^{2m}(-1)^r (w_r^T x_i) (u_r^T x_{i}) \\sum_{l=1}^{2m}(-1)^l(x_{i}^Tu_l)(w_l^T x_i) + \\sum_{i=1}^n r_i(W)  \\sum_{r=1}^{2m}(-1)^r (u_r^T x_{i}) \\sum_{l=1}^{2m} \\mathbb{1}_{r = l} (x_{i}^Tu_l) \\\\\n  &= \\sum_{i=1}^n \\left(\\sum_{r=1}^{2m}(-1)^r (w_r^T x_i) (u_r^T x_{i})\\right)^2 + \\sum_{i=1}^n r_i(W)  \\sum_{r=1}^{2m}(-1)^r (u_r^T x_{i})^2 \\\\\n  &= \\sum_{i=1}^n \\left( x_i^T W^T D U x_i\\right)^2 + \\sum_{i=1}^n r_i(W) x_i^T U^T D U x_i.\n\\end{align*}\n\\]\n\n\nRecap on stationary points\n\nA stationary point \\(W\\) of \\(L\\) satisfies \\(\\nabla L(W) = 0\\). A stationary point is either a local minimum, global minimum or saddle.\nThe stationary points of a sufficiently smooth function can be classified based on their local curvature / second order information.\n\nIf \\(\\nabla^2 L(W) \\succ 0\\) then is an (isolated) local minimum.\nIf \\(\\nabla^2 L(W) \\prec 0\\) then is an (isolated) local maximum.\nif \\(\\nabla^2 L(W) \\succcurlyeq 0\\) then could either be a saddle or a (non-isolated) local minimum,\nif \\(\\nabla^2 L(W) \\preccurlyeq 0\\) then could be either a saddle or a (non-isolated) local maximum.\nIf \\(\\nabla^2 L(W)\\) has both positive and negative eigenvalues then is a saddle point.\n\nIn keeping with convention we define a strict saddle as any point having at least one direction of negative curvature, i.e., \\(\\lambda_{min}(\\nabla^2 L(W)) &lt; 0\\). Roughly speaking strict saddles are nice as GD can escape them almost surely (Lee et al. 2016), (Du et al. 2017) given a random initialization."
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#all-local-minima-are-global-and-all-saddles-are-strict-saddles",
    "href": "posts/landscape-quadratic-net/index.html#all-local-minima-are-global-and-all-saddles-are-strict-saddles",
    "title": "A simple neural network with a benign loss landscape",
    "section": "All local minima are global and all saddles are strict saddles",
    "text": "All local minima are global and all saddles are strict saddles\nThe following is a rephrased version of Theorem 2.1 (Soltanolkotabi, Javanmard, and Lee 2019).\n\nTheorem 1 If \\(m \\geq d\\) then the following three properties hold for \\(L(W)\\).\n\nAll local minima are global and all saddle points are strict saddle points.\nIn addition if \\(d \\geq 3\\) and \\(n \\lesssim d^2\\) then for almost all \\(X\\in \\mathbb{R}^{d \\times n}\\) any global optimum \\(W^*\\) of \\(L\\) satisfies \\(L(W^*)=0\\).\n\n\nThe outline of the proof for Theorem 1 is as follows.\n\nAny stationary point satisfies \\(DW \\sum_{i=1}^n r_i(W) x_i x_i^T = 0\\).\nIf in addition \\(\\nabla L(W)=0\\) and \\(\\nabla^2L(W) \\succcurlyeq 0\\) then this implies \\(\\sum_{i=1}^n r_i(W) x_i x_i^T = 0\\).\nBy Lemma 1 if \\(\\sum_{i=1}^n r_i(W) x_i x_i^T = 0\\) then \\(W\\) is a global minimizer.\nFurthermore if \\(W\\) is a stationary point and \\(J_F(W)\\) is full rank then as \\(\\nabla L(W) = J_F(W) r(W) = 0\\) this implies \\(r(W) = 0\\) and therefore \\(W\\) is also global minimizer with \\(0\\) loss.\n\n\nSupporting Lemmas\n\nCertificate for global optimality\nThe following lemma gives a sufficient condition for global optimality. Abstracting the argument, this is achieved by decomposing the loss into the composition of two functions, \\(L(W) = L_G(G(W))\\), we can therefore interpret \\(L\\) as a particular restriction of \\(L_G\\). Note that any global minimizer of \\(L\\) must also be a global minimizer of \\(L_G\\). This allows us to substitute the problem of finding a global certificate for \\(L\\) (which might be hard!) with the problem of finding a global certificate for \\(L_G\\) (which hopefully is a lot easier…). In the setting studied here we can relax the problem by using a more arbitrary quadratic form instead of a neural network with quadratic activations. Moreover the proxy loss \\(L_G\\) is convex in the parameters of this quadratic form and hence a global certificate for \\(L_G\\) in this setting is easy to derive!\n\nLemma 1 If \\(\\tilde{W} \\in \\mathbb{R}^{2m \\times d}\\) satisfies \\[\n  \\sum_{i=1}^n r_i(\\tilde{W}) x_i x_i^T = 0\n\\] then \\(\\tilde{W}\\) is a global minimizer of \\(L(W)\\).\n\n\nProof. First of all we note the obvious: a shallow network with quadratic activations is a quadratic form with respect to the input data, indeed \\(f(x;W) = x^TW^TDWx\\). As a result we can re-write the condition of the lemma as \\[\n\\sum_{i=1}^n r_i(\\tilde{W}) x_i x_i^T = \\sum_{i=1}^n (x_i^T \\tilde{W}^T D \\tilde{W} x_i - y_i)x_i x_i^T =0.\n\\] We now relax the problem by instead considering a predictor \\(g(x;M) = x^T M x\\) instead of \\(f(x;W)\\) and look at the squared error in this context, \\[\n\\begin{align*}\nL_g(M) &:= \\frac{1}{2} \\sum_{i=1}^n (g(x_i, M) - y_i)^2.\n\\end{align*}\n\\]\nAs \\(g\\) is affine in \\(M\\) then \\((g(x; M) - y)^2\\) is a composition of a convex function with an affine function and therefore is convex. It follows that \\(L_g\\) is differentiable and convex as it is a positively weighted sum of convex, differentiable functions. As a result any global minimizer \\(M^*\\) of \\(L_g\\) satisfies \\(\\nabla L_g(M^*)= 0_{2md}\\) or equivalently \\(\\partial_M L_g(M^*)= 0_{2m \\times d}\\), which in turn implies \\[\n\\sum_{i=1}^n(g(x_i, M^*) - y_i) x_i x_i^T = 0.\n\\] Now consider the square matrix \\(\\tilde{W}^T D \\tilde{W}\\): based on the above observation if \\[\n  \\sum_{i=1}^n(g(x_i, \\tilde{W}^T D \\tilde{W}) - y_i) x_i x_i^T  = \\sum_{i=1}^n(x_i^T \\tilde{W}^T D \\tilde{W} x_i - y_i) x_i x_i^T= 0\n\\] then \\(\\tilde{W}^T D \\tilde{W}\\) is a global minimizer of \\(L_g\\). By construction \\(L(W) = L_g(W^T D W)\\) for all \\(W \\in \\mathbb{R}^{2m \\times d}\\), therefore if \\(\\tilde{W}^T D \\tilde{W}\\) is a global minimizer of \\(L_g\\) then \\(\\tilde{W}\\) is a global minimzer of \\(L\\) as \\[\nL(\\tilde{W}) = L_g(\\tilde{W}^T D \\tilde{W}) \\leq L_g(W^T D W) = L(W)\n\\] for all \\(W \\in \\mathbb{R}^{2m \\times d}\\).\n\n\n\nProving \\(X \\star X\\) is full rank almost everywhere\nWe will show all local minima satisfy the condition \\((X \\star X)r(W) = 0_{d^2}\\). Assuming \\(d^2 \\geq n\\) then \\(X \\star X \\in \\mathbb{R}^{d^2 \\times n}\\) is a tall matrix. If \\(rank(X \\star X) = n\\) then this implies \\(null(X \\star X) = \\emptyset\\) and therefore \\((X \\star X)r(W) = 0_{d^2}\\) implies \\(r(W) = 0\\) which in turn implies \\(W\\) is a global minimum. Let \\(A \\in \\mathbb{R}^{m \\times n}\\) be a rectangular matrix where without loss of generality we assume \\(m\\geq n\\) (note the column and row rank are equal and therefore we can always instead study \\(A^T\\) if is this is not true). \\(A\\) is full rank if and only if there exists a square submatrix formed by selecting a subset of size \\(n\\) out of \\(m\\) possible rows which is full rank. Therefore \\(A\\) is not full rank, or singular, if and only if all possible square submatrices formed by selecting \\(n\\) out of \\(m\\) possible rows are not full rank. Recall also a square matrix \\(B \\mathbb{R}^{n \\times n}\\) is singular if and only its determinant is zero. Recall the determinant of \\(B\\) is a polynomial in the entries of the \\(B\\) and for any non-zero polynomial its zero-set has Lebesgue measure 0. To abstract the argument we are about to present let \\(G:\\mathbb{R}^p \\rightarrow \\mathbb{R}^{m \\times n}\\) with \\(m \\geq n\\) output matrices whose entries are polynomials of the elements of the input \\(\\theta\\). Consider the Lebesgue product measure on \\(\\mathbb{R}^p\\), which we denote \\(\\mu\\), and for \\(S \\subseteq [m]\\) let \\(G(\\theta)_S)\\) denote submatrix of \\(G(\\theta)\\) formed by taking only those rows with index in \\(S\\). The kind of result we are after is often derived using the following argument.\n\nDefine \\(p(\\theta) = \\sum_{S \\subset [n], |S| = n} det(G(\\theta)_S)\\).\nAs \\(p\\) is a sum of polynomials of the elements of \\(G(\\theta)\\) and the elements of \\(G(\\theta)\\) are themselves polynomial in \\(\\theta\\) then \\(p\\) is polynomial in \\(\\theta\\).\nBy construction \\(rank(G(\\theta)) &lt; n\\) if and only if \\(p(\\theta) = 0\\).\nLet \\(Z(p) = \\{ \\theta \\in \\mathbb{R}^p: p(\\theta) = 0 \\}\\) be the zero set of \\(p\\). If there exists a \\(\\theta\\) such that \\(p(\\theta) \\neq 0\\) then \\(\\mu(Z(p)) = 0\\).\n\nWe now apply this argument to our setting. It is worth remarking that in Theorem 2.1 of (Soltanolkotabi, Javanmard, and Lee 2019) it is stated that \\(n\\geq d\\) but this is not necessary (which should be intuitive!).\n\nLemma 2 If \\(d \\geq 3\\) and \\(n \\lesssim d^2\\) then for almost every \\(X \\in \\mathbb{R}^{d \\times n}\\) we have \\(rank(X \\star X) = n\\).\n\n\nProof. By the argument above it suffices to find just one \\(X\\) such that \\(rank(X \\star X) = n\\). We consider two cases, \\(n\\leq d\\) and \\(d &lt; n \\leq cd^2\\) for some constant \\(c \\in (0,1]\\). For the first case, consider \\(X = [e_1, e_2... e_n]\\) where \\(e_i\\) is the standard basis with a one at index \\(i\\) and zero everywhere else. Then \\(X \\star X = [e_1, e_2... e_n]\\) (here the \\(e_i\\) are being used interchangeably to represent the standard basis vectors in both \\(\\mathbb{R}^d\\) and \\(\\mathbb{R}^{d^2})\\) which clearly is rank \\(n\\). For the other case, consider \\(X\\) with unique columns of the form \\(e_i + e_j\\) for \\(i,j \\in [n]\\) with \\(i &lt;j\\). Let \\(p(i,j) = d(i-1) +j\\). Note \\([(e_i + e_j) \\otimes (e_i + e_j)]_{p(k,l)} = 1\\) if and only if \\(k=l\\) and \\(j = l\\). Denoting \\(S = \\{e_i + e_j: \\; 1\\leq i &lt; j \\leq n \\}\\) then any \\(X\\) formed from \\(n\\) of the \\(|S| = (d - 1)(d-2)/2\\) elements of \\(S\\) generates an \\(X \\star X\\) which is full rank. Note for \\(|S| &gt;0\\) we require \\(d \\geq 3\\).\n\n\n\n\nProof of Theorem 1\n\nProof. Note as the loss is twice differentiable any local minima or (non-strict) saddle must satisfy the following conditions, \\[\n  \\begin{align*}\n    \\nabla L (W) &= 0,\\\\\n     \\nabla^2L(W) &\\succcurlyeq 0.\n  \\end{align*}\n  \\] For Statement 1 it suffices to show any \\(W\\) satisfying these conditions in turn satisfies \\[\n\\sum_{i=1}^n r_i(W) x_i x_i^T = 0\n\\] and therefore by Lemma 1 must be a global minimum. If \\(W\\) is a stationary point then \\[\n\\begin{align*}\n\\nabla L(W) = DW \\sum_{i=1}^n r_i(W) x_i x_i^T = 0\n\\end{align*}\n\\] Recall \\(DW \\in \\mathbb{R}^{2m \\times d}\\) and \\(m \\geq d\\). If \\(DW\\) is full rank then it has a left inverse, i.e., there exists a matrix \\(H\\) such that \\(HDW = I\\). As a result, if \\(W\\) is stationary and \\(DW\\) is full rank then \\[\nHDW\\sum_{i=1}^n r_i(W) x_i x_i^T = \\sum_{i=1}^n r_i(W) x_i x_i^T = 0.\n\\] Suppose instead that \\(DW\\) is not full rank. As \\(W\\) is either a local minimum or a non-strict saddle then \\(\\nabla^2L(W) \\succcurlyeq 0\\) which in turn implies \\[\n\\sum_{i=1}^n \\left( x_i^T D W^T U x_i\\right)^2 + \\sum_{i=1}^n r_i(W) x_i^T U^T D U x_i \\geq 0\n\\] for any \\(U \\in \\mathbb{R}^{2m \\times d}\\). Consider \\(U = ab^T\\) for some \\(a \\in R^{2m}\\), \\(b \\in \\mathbb{R}^{d}\\). Then \\[\n\\begin{align*}\n\\sum_{i=1}^n \\left( x_i^T  W^TD U x_i\\right)^2 + \\sum_{i=1}^n r_i(W) x_i^T U^T D U x_i & = \\sum_{i=1}^n \\left( x_i^T  W^TD a b^T x_i\\right)^2 + \\sum_{i=1}^n r_i(W) x_i^T b a^T D a b^T x_i\\\\\n&= \\sum_{i=1}^n \\left( x_i^T  W^TD a b^T x_i\\right)^2 + (a^T D a) \\sum_{i=1}^n r_i(W) x_i^T b  b^T x_i\\\\\n&= \\sum_{i=1}^n \\left( x_i^T  W^T D a b^T x_i\\right)^2 + (a^T D a) b^T \\left(\\sum_{i=1}^n r_i(W) x_i x_i^T\\right) b\\\\\n& \\geq 0.\n\\end{align*}\n\\] Let \\(E(W^TD), O(W^T D) \\in \\mathbb{R}^{m \\times d}\\) denote the submatrices of \\(W^T d\\) formed by taking only the even and odd columns. Again as \\(DW\\) is not full rank it must hold that both \\(E(DW)\\) and \\(O(DW)\\) are not full rank. Therefore let \\(a_1 \\in \\mathbb{R}^{2m}\\) be a vector whose even indexed elements are all zero and whose odd indexed elements form a subvector which lies in the nullspace of \\(O(DW)\\). Likewise, let \\(a_1 \\in \\mathbb{R}^{2m}\\) be a vector whose odd indexed elements are all zero and whose even indexed elements form a subvector which lies in the nullspace of \\(E(DW)\\). Then \\(a_1, a_2 \\in null(W^T D)\\) and as a result the left-hand-side of the above inequality simplifies significantly, \\[\n\\begin{align*}\n\\left(a_1^TDa_1 \\right) b^T \\left(\\sum_{i=1}^n r_i(W) x_i x_i^T\\right) b &\\geq 0\\\\\n\\left(a_2^TDa_2 \\right) b^T \\left(\\sum_{i=1}^n r_i(W) x_i x_i^T\\right) b &\\geq 0\n\\end{align*}\n\\] for all \\(b \\in \\mathbb{R}^d\\). Furthermore \\[\n\\begin{align*}\na_1^T D a_1  &= -\\sum_{i} a_{1i}^2 &lt; 0\\\\\na_2^T D a_2  &= \\sum_{i} a_{2i}^2 &gt; 0\n\\end{align*}\n\\] and therefore \\[\n\\begin{align*}\n\\left(a_1^TDa_1 \\right) b^T \\left(\\sum_{i=1}^n r_i(W) x_i x_i^T\\right) b & \\leq 0,\\\\\n\\left(a_2^TDa_2 \\right) b^T \\left(\\sum_{i=1}^n r_i(W) x_i x_i^T\\right) b & \\geq  0\n\\end{align*}\n\\] for all \\(b \\in \\mathbb{R}^d\\). This is possible if and only if \\(\\sum_{i=1}^n r_i(W) x_i x_i^T = 0\\). This concludes the proof of Statement 1.\nFor Statement 2, observe \\[\n  \\begin{align*}\n    \\sum_{i=1}^n r_i(W) x_i x_i^T = 0_{d \\times d}\n  \\end{align*}\n\\] is equivalent to \\[\n(X \\star X)r(W) = 0_{d^2},\n\\] recall we use subscripts to clarify the shape of the tensor in question. It suffices to prove that \\((X \\star X) \\in \\mathbb{R}^{d^2 \\times n}\\) is full rank as this would then imply \\(r(W) = 0\\), or equivalently or \\(f(x_i, W) = y_i\\) for all \\(i \\in [n]\\). As long as \\(d\\geq 3\\) and \\(n \\lesssim d^2\\) then this is true for almost every \\(X\\in \\mathbb{R}^{d \\times n}\\) as per Lemma 2."
  },
  {
    "objectID": "posts/landscape-quadratic-net/index.html#discussion",
    "href": "posts/landscape-quadratic-net/index.html#discussion",
    "title": "A simple neural network with a benign loss landscape",
    "section": "Discussion",
    "text": "Discussion\nIn this post we have illustrated under relatively mild conditions that a shallow network with quadratic activations has a benign loss landscape in the sense that all global minima are local and all saddles are strict saddles. Interestingly there are analogues with deep linear networks for which likewise all local minima are global, however not all saddles are strict. One of the key tricks we saw as per Lemma 1 was to find a proxy problem for which global certificates are easy to derive and whose global minimizers are a super set of the global minimizers of the network loss. It would be interesting to see where and how this idea could be extended to other settings."
  },
  {
    "objectID": "projects.html#research-themes",
    "href": "projects.html#research-themes",
    "title": "Research",
    "section": "Research themes",
    "text": "Research themes\nMy research roughly speaking falls into the following two categories. Note an up-to-date list of my publications can be found on Google scholar, linked on the home page.\n\nTheoertical foundations of deep learning\nThe success of many of today’s cutting edge systems rely on variants of deep neural networks. These models are typically highly overparameterized and are trained by minimizing a non-convex and often non-smooth objective using only first order information with little if any explicit form of regularization. Furthermore the amount of data available for training is often relatively small relative to the dimension of the data. As a result the success of these models appears to contradict conventional machine learning wisdom. Examples of specific topics include identifying transitions between benign, tempered and no-overfitting outcomes, characterizing local minima in the loss landscape of neural networks, analysis of the spectrum of the NTK to gain insight into the role of overparameterization and implicit versus explicit forms of regularization resulting from the interplay of initialization, learning algorithm, architecture and data.\n\n\nImproving deep learning algorithms\nExample topics include understanding transitions between benign, tempered and no-overfitting, the loss landscape of neural networks, implicit forms of regularization and the role of overparameterization in training and generalization.\nExample topics include principled design of network architecture (in particular role of the activation function), sparse / compressed networks and novel regularization strategies."
  },
  {
    "objectID": "projects.html#perspective",
    "href": "projects.html#perspective",
    "title": "Research",
    "section": "",
    "text": "I use maths to better understand and improve the performance of algorithms for machine learning and data science. I am particularly interested in deep learning for two reasons: first because it offers state-of-the-art performance across so many important applications (e.g., natural language processing, computer vision, drug prediction etc.), and second because the fact that it works so well challenges a number of aspects of conventional machine learning wisdom. I think mathematics will allow us to not only better understand deep learning, thereby helping us extract and generalize principles for successful learning systems, but will also play a crucial role in making them safer, more reliable and less costly to train and use in terms of time, energy and memory."
  },
  {
    "objectID": "projects.html#key-themes",
    "href": "projects.html#key-themes",
    "title": "Research",
    "section": "Key themes",
    "text": "Key themes\nMy research roughly speaking falls into the following two categories. An up-to-date list of my publications can be found on Google scholar.\n\nTheoertical foundations of deep learning\nThe success of many of today’s cutting edge systems rely on variants of deep neural networks. These models are typically highly overparameterized and are trained by minimizing a non-convex and often non-smooth objective using only first order information with little if any explicit form of regularization. Furthermore the amount of data available for training is often relatively small relative to the dimension of the data. As a result the success of these models appears to contradict conventional machine learning wisdom. Examples of specific topics include identifying transitions between benign, tempered and no-overfitting outcomes, characterizing local minima in the loss landscape of neural networks, analysis of the spectrum of the NTK to gain insight into the role of overparameterization and implicit versus explicit forms of regularization resulting from the interplay of initialization, learning algorithm, architecture and data.\n\n\nImproving the efficiency of deep learning algorithms\nToday’s massive, cutting edge deep learning based systems are enormously expensive to train and use. Algorithmic innovation based on intuition gained from mathematical theory has the potential to significantly lower costs in terms of memory, compute time and energy as well as unlock new applications. Examples of specific topics include principles for architecture design, including role of the activation function, how to initialize neural networks for efficient forward and backward propagation early during training as well as encouraging the selection of sparsifiable / compressible networks through regularization."
  },
  {
    "objectID": "projects.html#high-level-perspective",
    "href": "projects.html#high-level-perspective",
    "title": "Research",
    "section": "",
    "text": "I use maths to better understand and improve the performance of algorithms for machine learning and data science, notably deep learning. I am particularly interested in deep learning for two reasons: first because it offers state-of-the-art performance across so many important applications (e.g., natural language processing, computer vision, drug prediction etc.), and second because the fact that it works so well challenges a number of aspects of conventional machine learning wisdom. I think mathematics will allow us to not only better understand deep learning, thereby helping us extract and generalize principles for successful learning systems, but will also play a crucial role in making such systems safer, more reliable and less costly to train and use in terms of time, energy and memory."
  },
  {
    "objectID": "projects.html#themes",
    "href": "projects.html#themes",
    "title": "Research",
    "section": "Themes",
    "text": "Themes\nTo date my research roughly speaking falls into the following two categories. An up-to-date list of my publications can be found on Google scholar.\n\nTheoretical foundations of deep learning\nThe success of many of today’s cutting edge machine learning systems rely on variants of deep neural networks. These models are typically highly overparameterized and are trained by minimizing a non-convex, often non-smooth objective using only first order information with little if any explicit form of regularization. Furthermore, the amount of data available for training is often relatively small relative to the dimension of the data. As a result, the success of these models appears to contradict certain aspects of conventional machine learning wisdom and there is a need for reconcilliation.\nExamples of specific topics include identifying transitions between benign, tempered and no-overfitting outcomes, characterizing local minima in the loss landscape of neural networks, analysis of the spectrum of the NTK to gain insight into the role of overparameterization and implicit versus explicit forms of regularization resulting from the interplay of initialization, learning algorithm, architecture and data.\n\n\nImproving the efficiency of deep learning algorithms\nToday’s massive, cutting edge deep learning based systems are enormously expensive to train and use. Algorithmic innovation based on intuition gained from mathematical theory has the potential to significantly lower costs in terms of memory, compute time and energy as well as unlock new applications.\nExamples of specific topics include principles for architecture design (including the role of the activation function), how to initialize neural networks for efficient forward and backward propagation early during training as well as encouraging the selection of sparsifiable / compressible networks through regularization."
  },
  {
    "objectID": "posts/flatness1/index.html",
    "href": "posts/flatness1/index.html",
    "title": "On some elementary properties of Hopfield networks",
    "section": "",
    "text": "In this post we derive some basic mathematical properties of asynchronous Hopfield networks from first principles.\nHopfield networks where introduced in 1982 by John Hopfield as a model for biological computation, they are also often viewed as the precursor to Boltzmann machines. The purpose of this post is to derive some of their basic mathematical properties and results in a simple setting: namely binary (as opposed to polar) Hopfield networks equipped with an asynchronous update rule trained with a minimum probability flow objective. The content of this post is based primarily on (Hillar and Marzen 2017) as well as discussions with Chris Hillar.\nDisclaimer: this post is not at all intended as a full survey or review of Hopfield networks and there are doubtless many important topics, ideas and references that have been completely omitted."
  },
  {
    "objectID": "posts/flatness1/index.html#introduction",
    "href": "posts/flatness1/index.html#introduction",
    "title": "Some elementary properties of Hopfield networks",
    "section": "Introduction",
    "text": "Introduction\nHopfield networks were popularized in …"
  },
  {
    "objectID": "posts/flatness1/index.html#perturbation-perspective-flat-minimizers-correspond-to-smooth-solutions---mathematical-characterization-based-on-trace-consider-smooth-function-look-at-2nd-order-taylor-series-expansion-when-trace-is-small-error-difference-in-loss-between-points-is-also-small.---bound-on-trace-in-turn-bounds-the-gradient-of-the-model-on-the-training-data.---if-the-function-is-sufficientlysmooth-then-can-bound-the-gradient-everywhere-and-therefore-conclude-that-is-smooth-everywhere.",
    "href": "posts/flatness1/index.html#perturbation-perspective-flat-minimizers-correspond-to-smooth-solutions---mathematical-characterization-based-on-trace-consider-smooth-function-look-at-2nd-order-taylor-series-expansion-when-trace-is-small-error-difference-in-loss-between-points-is-also-small.---bound-on-trace-in-turn-bounds-the-gradient-of-the-model-on-the-training-data.---if-the-function-is-sufficientlysmooth-then-can-bound-the-gradient-everywhere-and-therefore-conclude-that-is-smooth-everywhere.",
    "title": "Hopfield networks: basic properties and learning with Minimum Energy Flow (MEF)",
    "section": "Perturbation perspective: flat minimizers correspond to smooth' solutions - Mathematical characterization based on trace: consider smooth function, look at 2nd order Taylor series expansion, when trace is small error difference in loss between points is also small. - Bound on trace in turn bounds the gradient of the model on the training data. - If the function is sufficientlysmooth’ then can bound the gradient everywhere and therefore conclude that is smooth everywhere.",
    "text": "Perturbation perspective: flat minimizers correspond to smooth' solutions - Mathematical characterization based on trace: consider smooth function, look at 2nd order Taylor series expansion, when trace is small error difference in loss between points is also small. - Bound on trace in turn bounds the gradient of the model on the training data. - If the function is sufficientlysmooth’ then can bound the gradient everywhere and therefore conclude that is smooth everywhere.\n\nexample: polynomial regression with square loss"
  },
  {
    "objectID": "posts/flatness1/index.html#perturbation-perspective-flat-minimizers-are-robust-to-perturbations-of-the-input",
    "href": "posts/flatness1/index.html#perturbation-perspective-flat-minimizers-are-robust-to-perturbations-of-the-input",
    "title": "Hopfield networks: basic properties and learning with Minimum Energy Flow (MEF)",
    "section": "Perturbation perspective: flat minimizers are robust to perturbations of the input",
    "text": "Perturbation perspective: flat minimizers are robust to perturbations of the input"
  },
  {
    "objectID": "posts/flatness1/index.html#bayesian-perspective-flat-minimizers-correspond-to",
    "href": "posts/flatness1/index.html#bayesian-perspective-flat-minimizers-correspond-to",
    "title": "Hopfield networks: basic properties and learning with Minimum Energy Flow (MEF)",
    "section": "Bayesian perspective: flat minimizers correspond to",
    "text": "Bayesian perspective: flat minimizers correspond to"
  },
  {
    "objectID": "posts/flatness1/index.html#information-theory-perspective-flat-minimizers-are-compressible",
    "href": "posts/flatness1/index.html#information-theory-perspective-flat-minimizers-are-compressible",
    "title": "Hopfield networks: basic properties and learning with Minimum Energy Flow (MEF)",
    "section": "Information theory perspective: flat minimizers are compressible",
    "text": "Information theory perspective: flat minimizers are compressible"
  },
  {
    "objectID": "posts/flatness1/index.html#definition-and-feedforward-dynamics",
    "href": "posts/flatness1/index.html#definition-and-feedforward-dynamics",
    "title": "Some elementary properties of Hopfield networks",
    "section": "Definition and feedforward dynamics",
    "text": "Definition and feedforward dynamics\n\nEnergy function\nIn its most elementary form, a Hopfield network is a parameterized model which uses recurrence dynamics to map binary vectors to other binary vectors. Let \\(\\text{Symm}_0(d)\\subset \\mathbb{R}^{n \\times n}\\) be the set of symmetric \\(n \\times n\\) matrices whose diagonal entries are zero. To define a Hopfield network we introduce the function \\(E:\\{0,1\\}^n \\times \\text{Symm}_0(n) \\times \\mathbb{R}^d \\rightarrow \\{0,1\\}^n\\), defined as \\[\nE(x; \\theta) = -\\frac{1}{2} x^T W x + h^Tx\n\\] where for convenience we use \\(\\theta = (W, h)\\). We often refer to \\(E\\) as the `energy’ due to its use in modelling the energy configuration of ferromagnetic materials (see the Lenz-Ising model): roughly speaking, \\(x\\) encodes the magnetic state of an object, \\(W\\) captures the magnetic properties of the object, namely the magnetic interaction / coupling between different atoms, and \\(h\\) models the effects of an external magnetic field. For what follows it will prove useful to analyze the difference in the energy of two states that differ by at most one bit.\n\n\nRecurrence dynamics\nThe network part of ‘Hopfield network’ arises by considering a network of \\(n\\) nodes where each node can store the value 0 or 1. The state of a network is therefore described by the binary vector encoding the value of each node. Now we introduce certain recurrence dynamics which allow the value of each node (and hence the state of the network) to evolve in time: to clarify some of the notation that follows, we denote the \\(i\\)th entry of a vector \\(x\\) as \\(x_i\\), the \\(j\\)th row of \\(W\\) as \\(w_j\\) and use \\({\\mathbb{1}}\\) to denote the indicator function. Given an input vector \\(x \\in \\{ 0,1 \\}^n\\) we generate a sequence of binary vectors \\((x(k))_{k \\in {\\mathbb{Z}}_{\\geq 0}}\\) by setting \\(x(0) = x\\) and adopting at each subsequent iteration the following node-wise update rule. \\[\nx_i(k) = \\begin{cases}\n&{\\mathbb{1}}(\\frac{1}{2}w_i^Tx(k-1)&gt; h_i), \\quad k \\; \\text{mod} \\; i = 0,\\\\\n&x_i(k-1), \\quad \\text{otherwise}.\\\\\n\\end{cases}\n\\] In particular, at each iteration the value of exactly one node is updated: this update is based on i) the connection of the node to other nodes (as encoded by \\(w_i\\)), the current value of the other nodes (as encoded by \\(x(k-1)\\)) and the inherent tendency of the node to activate (captured by the \\(h_i\\), which acts as a threshold). This update is referred to as asynchronous due to the fact that at each iteration only one node is updated while the rest remain unchanged, in addition, while here we only consider the case \\(w_{ii}=0\\) for all \\(i \\in [n]\\) one can also consider \\(w_{ii}\\geq 0\\).\nWe will not show that for any binary input vector the above recurrence relation converges to a fixed point or attractor after a finite number of steps. To this end the following will prove useful.\n\nProposition 1 Suppose \\(x,x' \\in \\{0,1 \\}^n\\) differ by exactly one bit, meaning there exists an \\(i \\in [n]\\) such that \\(x_j \\neq x_j'\\) iff \\(j = i\\). Then \\[\nE(x; \\theta ) - E(x'; \\theta ) = (1 - 2x_i)\\left(\\frac{1}{2}w_i^Tx(k)- h_i\\right)\n\\]\n\n\nProof. As \\(x_i \\neq x_i'\\) then \\(x_i'- x_i = 1 - 2x_i\\) and \\[\n\\begin{align*}\nE(x; \\theta ) - E(x'; \\theta ) = &= -\\frac{1}{2} \\sum_{l,j \\in [n]}W_{lj}(x_l x_j - x_l' x_j') + \\sum_{l \\in [n]}h_l(x_l - x_l')\\\\\n  &= -\\frac{1}{2} \\sum_{j \\in [n]} W_{ji}(x_j x_i - x_j' x_i') + h_i(x_i - x_i')\\\\\n  &= ( x_i'- x_i)  \\left(\\frac{1}{2} \\sum_{j \\in [n]} W_{ji}x_j - h_i\\right)\\\\\n  & = (1 - 2x_i)\\left(\\frac{1}{2}w_i^Tx- h_i\\right).\n\\end{align*}\n\\]\n\nFurthermore, an important property of the described dynamics is that an update never results in an increase in energy.\n\nProposition 2 Let \\(\\Delta x(k+1) := x(k+1) - x(k)\\) and \\(\\theta = (W,h)\\), where \\(W \\in \\text{Sym}_0(n)\\) and \\(h \\in \\mathbb{R}^n\\). For all \\(k \\geq 0\\) we have \\(E(x(k+1); \\theta ) \\leq E(x(k); \\theta)\\). Furthermore, if \\(\\Delta x_i(k+1) =1\\) then \\(E(x(k+1); \\theta ) &lt; E(x(k); \\theta)\\).\n\n\nProof. By construction there exists an \\(i \\in [n]\\) such that \\((k+1) \\; \\text{mod} \\; i = 0\\), moreover for all \\(l \\neq i\\) we have \\(x_l(k+1) = x_l(k)\\). Let \\(\\Delta x_i(k+1) := x_i(k+1) - x_i(k)\\), clearly then \\(\\Delta x(k+1) = \\Delta x_i(k+1)\\). As a result \\[\n\\begin{align*}\n  E(x(k+1); \\theta) - E(x(k); \\theta) &= -\\frac{1}{2} \\sum_{l,j \\in [n]}W_{lj}(x_l(k+1) x_j(k+1) - x_l(k) x_j(k)) + \\sum_{l \\in [n]}h_l(x_l(k+1) - x_l(k))\\\\\n  &= -\\frac{1}{2} \\sum_{j \\in [n]} W_{ji}(x_j(k+1) x_i(k+1) - x_j(k) x_i(k)) + h_i(x_i(k+1) - x_i(k))\\\\\n  &= - ( x_i(k+1) - x_i(k))  \\left(\\frac{1}{2} \\sum_{j \\in [n]} W_{ji}x_j(k) - h_i\\right)\\\\\n  & = - \\Delta x_i(k) \\left(\\frac{1}{2}w_i^Tx(k)- h_i\\right).\n\\end{align*}\n\\] If \\(\\frac{1}{2}w_i^Tx(k-1)&gt; h_i\\) then \\(x_i(k+1) = 1\\) and therefore \\(\\Delta x_i(k+1) \\geq 0\\), as a result \\(E(x(k+1); \\theta) - E(x(k) \\leq 0\\). If \\(\\frac{1}{2}w_i^Tx(k-1)\\leq h_i\\) then \\(x_i(k+1) = 1\\) and therefore \\(\\Delta x_i(k+1) \\leq 0\\) and again it follows that \\(E(x(k+1); \\theta) - E(x(k) \\leq 0\\). Now if \\(\\Delta x(k+1) = \\Delta x_i(k+1) \\neq 0\\) then \\(\\Delta x_i(k+1) \\in \\{-1,1 \\}\\). If \\(\\Delta x_i(k+1) = -1\\) then \\(x_i(k+1) = 0\\), implying \\(x_i(k) = 1\\)\n\nUsing these two lemmas we can now prove convergence after a finite number of iterations.\n\nProposition 3 For any \\(x \\in \\{0,1\\}^n\\) there exists a \\(K \\in {\\mathbb{Z}}_{\\geq 0}\\), \\(z \\in \\{0,1\\}^n\\) such that \\(x(k) = z\\) for all \\(k \\geq K\\).\n\n\nProof. Consider the subset \\(S(x) \\subset \\{0,1\\}^n\\), defined as the set of states which are revisited indefinitely by the recurrence dynamics given input/ initial state \\(x\\), \\[\nS(x) = \\{ z \\in \\{0,1\\}^n: \\forall K \\geq 0 \\; \\exists k \\geq K \\; s.t. \\; x(k) = z \\}.\n\\] We denote its compliment \\(S^c = \\{ 0,1\\}^n \\backslash S\\). If \\(z \\in \\{ 0,1\\}^n \\notin S(x)\\) then there exists an iteration \\(K(z,x) \\geq 0\\) such that for all \\(k &gt; K(z,x)\\) we have \\(x(k) \\neq z\\). Define \\(K(x) = \\max_{z \\in S^c} K(z,x) + 1\\), then by construction for all \\(k\\geq K(x)\\) we have \\(x(k) \\in S(x)\\). It suffices to show for any arbitrary \\(x \\in \\{0,1 \\}^n\\) that \\(|S(x)| = 1\\), i.e., for each \\(x\\) there exists a unique attractor. Proceeding by contradiction, suppose there exists an \\(x\\in \\{ 0,1\\}^n\\) such that \\(|S(x)| \\geq 2\\). Suppose \\(k \\geq K(x)\\) and let \\(k'&gt;k\\) denote the first iteration after \\(k\\) such that \\(x(k+l) \\neq x(k)\\), note such an iteration must exist as otherwise \\(|S(x)| = 1\\). By construction \\(x(k), x(k') \\in S(x)\\) and there exists an \\(i\\in[n]\\) such that \\(x_j(k) \\neq x_j(k')\\) iff \\(i \\neq j\\). By Proposition 1 it follows that \\[\nE(z; \\theta ) - E(z'; \\theta ) = (1 - 2z_i)\\left(\\frac{1}{2}w_i^Tz- h_i\\right) = 0\n\\]\nBy Proposition Proposition 2 we know that energy cannot increase between two updates, this implies \\(E(z_1; \\theta ) = E(x_2; \\theta)\\) for any \\(z_1, z_2 \\in S\\).\nIf the network does not change over \\(n\\) iterations or more then it must have reached a fixed point as no subsequent changes will occur.\nClearly as \\(n\\) is finite so are the number of distinct states, indeed, trivially there are only \\(2^n\\) of them. In particular, after \\(n2^n\\) iterations either convergence has occurred or every state must have been visited at least once. Define the subset \\(S \\subset \\{0,1\\}^n\\) as\nAs a result, the described recurrence dynamics fail to converge if and only if some subset of states (at least two of them) are indefinitely revisited. In particular, after \\(n2^n\\) iterations either convergence has occurred or every state must have been visited at least once. As a result, for convergence not to occur there must exist some subset \\(S \\subset \\{0,1\\}^n\\), \\(|S|\\geq 2\\), such that for any \\(K \\in {\\mathbb{Z}}_{\\geq n2^n}\\) and \\(x' \\in S\\) there exists a \\(k \\geq K\\) for which \\(x(k) = x'\\). We proceed by contradiction and suppose such a subset of states exists."
  },
  {
    "objectID": "posts/flatness1/index.html#training-via-minimum-energy-flow-mef",
    "href": "posts/flatness1/index.html#training-via-minimum-energy-flow-mef",
    "title": "Some elementary properties of Hopfield networks",
    "section": "Training via Minimum Energy Flow (MEF)",
    "text": "Training via Minimum Energy Flow (MEF)\nTraining a Boltzmann network can perhaps best be described as configuring the parameters in order that a given set of target binary vectors are the fixed points of the recurrence dynamics. There are a number of ways a Boltzmann network can be trained, here we only present one method from (Hillar, Sohl-Dickstein, and Koepsell 2015) based on minimum probability flows (Sohl-Dickstein, Battaglino, and DeWeese 2011). To this end, let \\(\\mathcal{T}\\subset \\{ 0,1\\}^n\\) denote the set of target binary vectors we want to store / memorize in our Hopfield network, furthermore for any \\(x \\in \\{0,1 \\}^n\\) let \\(\\mathcal{N}(x) = \\{x' \\in \\{0,1 \\}^n: \\sum_{i=1}^n |x_i - x_i'| = 1 \\}\\) denote the set of binary vectors which are a hamming distance of exactly one away from \\(x\\). Now consider the following loss \\[\nL(\\theta) = \\sum_{x \\in \\mathcal{T}} \\sum_{x' \\in \\mathcal{N}(x)} \\exp\\left( \\frac{E(x;\\theta ) - E(x'; \\theta)}{2} \\right)\n\\] Intuitively such an objective makes sense for the following reasons.\n\nThe fixed points of the asynchronous dynamics for a set of parameters \\(\\theta\\) correspond to those states \\(x \\in \\{0,1\\}^n\\) which are local minima, i.e., \\(E(x; \\theta) \\leq E(x';\\theta)\\) for all \\(x' \\in \\mathcal{N}(x)\\).\nTherefore, for a fixed set of states \\(\\mathcal{T}\\subset \\{ 0,1\\}^n\\) we want to choose \\(\\theta\\) so that for all \\(x \\in \\mathcal{T}\\) then \\(x\\) is a local minima.\nBy inspection the loss \\(L\\) is minimized, when possible, by choosing \\(\\theta\\) such that for all \\(x \\in \\mathcal{T}\\) then \\(E(x'; \\theta)\\) is made as large as possible relative to \\(E(x; \\theta)\\) for all \\(x' \\in \\mathcal{N}(x)\\). Moreover, again by inspection if \\(L(\\theta) &lt; 1\\) then it must hold that \\(\\exp\\left( \\frac{E(x;\\theta ) - E(x'; \\theta)}{2} \\right)&lt;1\\) and therefore \\(E(x; \\theta) \\leq E(x';\\theta)\\) for all \\(x \\in \\mathcal{T}\\) and \\(x' \\in \\mathcal{N}(x)\\).\n\nIn order to motivate what follows, we now introduce a Gibbs measure / Boltzmann distribution associated with \\(E\\). This describes the probability \\(p_{\\theta}\\) of being in a given state \\(x \\in \\{0,1 \\}^d\\), \\[\np_{\\theta}(x) = \\frac{\\exp(-E(x;\\theta))}{\\sum_{x' \\in \\{0,1\\}^n}\\exp(-E(x';\\theta)) }\n\\] In addition to being nice to manipulate and work with, probability distributions of this form are often quite practically relevant: in particular there is an equivalence between Gibbs measures and markov random fields, which are popular for modelling both physical and information systems. In addition, and for the specific energy function stated above, \\(p_{\\theta}\\) is the maximum entropy distribution over binary vectors given both first and second order statistics (mean and variance). In short then \\(p_{\\theta}\\) obeys the maximum entropy: if we want to model a target distribution over binary vectors and only know its mean and covariance, then using \\(p_{\\theta}\\) is in some sense the best choice if we want to maximize our uncertainty about the data beyond these two statistics."
  },
  {
    "objectID": "posts/flatness1/index.html#what-is-a-hopfield-network-and-how-does-it-work",
    "href": "posts/flatness1/index.html#what-is-a-hopfield-network-and-how-does-it-work",
    "title": "On some elementary properties of Hopfield networks",
    "section": "What is a Hopfield network and how does it work?",
    "text": "What is a Hopfield network and how does it work?\nIn its most elementary form, a Hopfield network \\(H_{\\theta}:\\{0,1 \\}^n \\rightarrow \\{0,1 \\}^n\\) is a parameterized model which maps binary vectors onto binary vectors. The parameters of a Hopfield network are used to define a function referred to as the energy function of the network: this energy function combined with a recurrence relation allows one to define an input-output map for a Hopfield network.\n\nEnergy function\nLet \\(\\text{Symm}_0(d)\\subset \\mathbb{R}^{n \\times n}\\) be the set of symmetric \\(n \\times n\\) matrices whose diagonal entries are zero. The energy function \\(E:\\{0,1\\}^n \\times \\text{Symm}_0(n) \\times \\mathbb{R}^d \\rightarrow \\{0,1\\}^n\\) of a Hopfield network is defined as \\[\nE(x; \\theta) = -\\frac{1}{2} x^T W x + h^Tx,\n\\] note for convenience we use \\(\\theta = (W, h)\\). We often refer to \\(E\\) as the ‘energy’ due to its use in modelling the energy configuration of ferromagnetic materials (see the Lenz-Ising model): roughly speaking we can view \\(x\\) as encoding the magnetic state of an object, \\(W\\) as capturing the magnetic properties of the object (namely the magnetic interaction or coupling between different atoms or nodes) and \\(h\\) as modelling the effect of an external magnetic field. In order to connect with probabilistic models we can also think about \\(E\\) generating a probability distribution over the different possible states of the system states.\n\n\nRecurrence dynamics\nConsider a network of \\(n\\) nodes where each node can store the value 0 or 1, the state of this network is therefore described by an \\(n\\)-dimensional binary vector. To turn this network into a Hopfield network we add recurrence dynamics which allow the value of each node (and hence the state of the network) to evolve in time. In what follows we denote the \\(i\\)th entry of a vector \\(x\\) as \\(x_i\\), the \\(j\\)th row of \\(W\\) as \\(W_j\\), and use \\({\\mathbb{1}}(\\omega)\\) to denote the indicator function which is one if the event \\(\\omega\\) is true and zero otherwise. Given an input vector \\(x \\in \\{ 0,1 \\}^n\\) the network generates a sequence of binary vectors \\((x(k))_{k \\in {\\mathbb{Z}}_{\\geq 0}}\\) as follows: let \\(x(0) = x\\), then for all \\(k\\geq 1\\) and \\(i\\in[n]\\) \\[\nx_i(k) = \\begin{cases}\n&{\\mathbb{1}}(W_i^Tx(k-1)&gt; h_i), \\quad k \\; \\text{mod} \\; i = 0,\\\\\n&x_i(k-1), \\quad \\text{otherwise}.\\\\\n\\end{cases}\n\\] In particular, at each iteration the value at most one node can change: whether this update occurs or not is based on i) the connection of the node to other nodes in the network as encoded by \\(W_i\\), the current value of the other nodes as encoded by \\(x(k-1)\\), and the inherent tendency of the node to activate which is captured by the \\(h_i\\). This update is referred to as asynchronous due to the fact that different nodes cannot be updated during the same iteration.\n\n\nDefining the input-output map of a Hopfield network: convergence of asynchronus recurrence dynamics to a fixed point\nHow does a Hopfield network compute? Initialized with a given input, the above recurrence relation, or update rule, is iterated until a fixed point is hit. This fixed point is the binary vector outputted by the Hopfield network for a given input. There are a number of interpretations of this: one way is to think of these fixed points as ’memories, the network therefore associates each input with a memory thereby implementing a form of associative memory. Alternatively, one can think of all inputs that converge to the same fixed point, or attractor, as being noisy versions of the same underlying binary vector. Under this interpretation the network is performing a de-noising or error correction function. However, for any of this to make sense or be practical, we need to prove that for any input binary input vector the aforementioned recurrence relation converges to a fixed point after a finite number of steps. To this end, it will first prove useful to analyze the difference in energy of two states that differ by at most one bit, or equivalently, have a hamming distance of one.\n\nProposition 1 Suppose \\(x,x' \\in \\{0,1 \\}^n\\) differ by exactly one bit, meaning there exists an \\(i \\in [n]\\) such that \\(x_j \\neq x_j'\\) iff \\(j = i\\). Then \\[\nE(x; \\theta ) - E(x'; \\theta ) = (1 - 2x_i)\\left(W_i^Tx - h_i\\right).\n\\]\n\n\nProof. First note as \\(x_i \\neq x_i'\\) then \\(x_i'- x_i = 1 - 2x_i\\). In addition, note that \\(x_jx_l = x_lx_j\\) if \\(l,j \\neq i\\) and also recall \\(W_{ii}=0\\). As a result using the symmetry of \\(W\\) and the fact that \\(W_{ii}=0\\) we have \\[\n\\begin{align*}\nE(x; \\theta ) - E(x'; \\theta ) = &= -\\frac{1}{2} \\sum_{l,j \\in [n]}W_{lj}(x_l x_j - x_l' x_j') + \\sum_{l \\in [n]}h_l(x_l - x_l')\\\\\n  &= -\\frac{1}{2} \\left(\\sum_{j \\in [n]} W_{ji}(x_j x_i - x_j' x_i') +  \\sum_{l \\in [n]} W_{il}(x_i x_l - x_i' x_l')\\right)+ h_i(x_i - x_i')\\\\\n  &= - \\sum_{j \\in [n]} W_{ij}(x_j x_i - x_j' x_i') + h_i(x_i - x_i') \\\\\n  &= ( x_i'- x_i)  \\left( \\sum_{j \\in [n], j \\neq i} W_{ij}x_j - h_i\\right)\\\\\n  & = (1 - 2x_i)\\left(W_i^Tx- h_i\\right).\n\\end{align*}\n\\] QED.\n\nNow we are ready to establish a fundamental property: each update or change to the state either keeps the energy the same or decreases it!\n\nProposition 2 For any given \\(k \\geq 0\\) let \\(i \\in [n]\\) be such that \\(k +1 \\; \\text{mod} \\; i = 0\\). At iteration \\(k+1\\) of the recurrence dynamics one of the following three cases must hold.\n\nNo update: if \\(x(k+1) = x(k)\\) then \\(E(x(k+1); \\theta ) = E(x(k); \\theta)\\).\n\\(i\\)th bit is turned off: if \\(x(k+1)=0\\) and \\(x(k)=1\\) then \\(E(x(k+1); \\theta ) \\leq E(x(k); \\theta)\\), note equality occurs if and only if \\(W_i^T x(k) = h_i\\).\n\\(i\\)th bit is turned on: if \\(x(k+1)=1\\) and \\(x(k)=0\\) then \\(E(x(k+1); \\theta ) &lt; E(x(k); \\theta)\\).\n\n\n\nProof. Case 1) is trivial. For 2) and 3), if \\(x(k+1) \\neq x(k)\\) then as per the update rule the hamming distance between \\(x(k+1)\\) and \\(x(k)\\) must be exactly one. Applying Proposition 1, as well as noting that \\(W_{ii} = 0\\) implies \\(W_i^T x(k+1) = W_i^Tx(k)\\), then \\[\n\\begin{align*}\nE(x(k+1); \\theta) - E(x(k); \\theta) = (1 - 2x_i(k+1))\\left(W_i^T x(k)- h_i\\right)\n\\end{align*}\n\\] Focusing on case 2), if \\(x_i(k+1) = 0\\) then by the update rule this implies \\(W_i^T x(k) - h_i \\leq 0\\) and \\(1 - 2x_i(k+1) = 1\\), as a result \\[\nE(x(k+1); \\theta) - E(x(k); \\theta) \\leq 0.\n\\] If \\(x_i(k+1) = 1\\) then by the update rule this implies \\(W_i^T x(k) - h_i &gt; 0\\) and \\(1 - 2x_i(k+1) = -1\\), this implies \\[\nE(x(k+1); \\theta) - E(x(k); \\theta) &lt; 0.\n\\] QED.\n\nThe fact that the energy does not increase will be crucial for proving convergence, indeed, without this property it is possible for the state of the network to oscillate in time and fail to converge. In light of this we now make a few remarks concerning the structure of \\(W\\).\n\nSymmetry: without symmetry the there can be updates resulting in an increase in energy. To see this suppose for ease that the diagonals of \\(W\\) are still zero, then for two states \\(x,x'\\) with a hamming distance of one and letting \\(\\tilde{W}_i \\in \\{ 0,1\\}^n\\) match the \\(i\\)th column of \\(W\\), we have \\[\n\\begin{align*}\nE(x; \\theta ) - E(x'; \\theta )\n&= - \\sum_{j \\in [n], j \\neq i} \\frac{W_{ij} + W_{ji}}{2}(x_j x_i - x_j' x_i') + h_i(x_i - x_i') \\\\\n&= ( x_i'- x_i)  \\left( \\sum_{j \\in [n]} \\frac{W_{ij} + W_{ji}}{2}x_j - h_i\\right)\\\\\n& = (1 - 2x_i)\\left(\\frac{1}{2}(W_i + \\tilde{W}_i)^T x- h_i\\right).\n\\end{align*}\n\\] One can easily construct examples where an update leads to an increase in energy in this setting: for example, consider \\(\\tilde{W}_i = -3W_i\\), then \\[\n\\begin{align*}\nE(x(k+1); \\theta ) - E(x(k); \\theta )\n& = -(1 - 2x_i(k+1))\\left(W_i^T x(k)- h_i\\right).\n\\end{align*}\n\\] Now if \\(x_i(k+1) = 1\\) then \\(W_i^T x(k)- h_i&gt;0\\) and so \\(E(x(k+1); \\theta ) &gt; E(x(k); \\theta )\\).\nZero valued self-connections: suppose \\(W\\) is still symmetric but there may be some \\(i \\in [n]\\) such that \\(W_{ii}\\neq 0\\). If \\(x_i(k) = 0\\) then there is no difference versus the update with \\(W_{ii}=0\\). However, if \\(x_i(k)=1\\) then, keeping the other parameters the same, \\(W_{ii}&gt;0\\) means \\(W_i^T x(k)\\) is bigger which makes it harder for the the \\(i\\)th bit to change to \\(0\\). Similarly, if \\(W_{ii}&lt;0\\) then \\(W_i^Tx(k)\\) is smaller which makes it easier for the \\(i\\)th bit to flip back to \\(0\\). Intuitively we therefore might expect \\(W_{ii}&gt;0\\) to make states with many ones more sticky while \\(W_{ii}&lt;0\\) to make states with many ones less sticky (here sticky means harder to leave). Considering the energy change across an iteration, for \\(W_{ii}\\neq 0\\) we have \\[\n\\begin{align*}\nE(x(k+1); \\theta ) - E(x(k); \\theta ) &= (1 - 2x_i(k+1))\\left(W_i^T x(k)  - h_i - W_{ii}x_i(k))\\right)\\\\\n\\end{align*}\n\\] From this we can again construct a scenario where the energy increases across an iteration: suppose \\(x_i(k+1) = 0\\) and \\(x_i(k)=1\\), then \\(W_i^T x(k) - h_i \\leq 0\\) and therefore \\[\nE(x(k+1); \\theta ) - E(x(k); \\theta ) = - |W_i^T x(k) - h_i| - W_{ii}.\n\\] If \\(W_{ii} &lt; - | W_i^T x(k) - h_i|\\) then \\(E(x(k+1); \\theta ) &gt; E(x(k); \\theta )\\). Negative valued self-connections or diagonal entries can therefore potentially lead to increases in the energy, oscillations in the dynamics and a failure to converge. Positive entries on the diagonal do not have this problem but on the other hand encourage states with a higher density to be attractors. As this is somewhat arbitrary and perhaps even downright problematic for certain applications (if we want to store say a sparse binary vector as a memory) it is easier to consider zero valued self-connections.\n\nWe are now ready to prove the desired convergence result for the asynchronous recurrence dynamics. This result follows from Proposition 2, indeed, as the energy cannot increase the only way the dynamics can fail to converge is if the network state indefinitely moves around some subset of states all of which have same energy. The trick to proving convergence then is to show that such dynamics are impossible: indeed, by Proposition 2 the only way the network state can change but the energy remain the same is if a bit is switched off, however, this contradicts the idea we can revisit a state or oscillate between states!\n\nProposition 3 For any \\(x \\in \\{0,1\\}^n\\) there exists a \\(K \\in {\\mathbb{Z}}_{\\geq 0}\\) and an \\(x^* \\in \\{0,1\\}^n\\) such that \\(x(k) = x^*\\) for all \\(k \\geq K\\).\n\n\nProof. Consider the subset \\(S(x) \\subset \\{0,1\\}^n\\), defined as the set of states which are revisited indefinitely by the recurrence dynamics for a given input or initial state \\(x\\), \\[\nS(x) = \\{ z \\in \\{0,1\\}^n: \\forall K \\geq 0 \\; \\exists k \\geq K \\; s.t. \\; x(k) = z \\}.\n\\] Clearly as \\(x(k) \\in \\{0,1\\}^n\\) then \\(|S(x)| \\geq 1\\). We first we prove that \\(E(z_1; \\theta) = E(z_2; \\theta)\\) for all \\(z_1,z_2 \\in S(x)\\). Proceeding by contradiction, assume that there exists a pair \\(z_1, z_2 \\in S(x)\\) such that \\(E(z_1; \\theta) &lt; E(z_2; \\theta)\\). Let \\(k \\geq 0\\) be an iteration where \\(x(k) = z_1\\), then by the construction of \\(S(x)\\) there exists some \\(k'&gt;k\\) such that \\(x(k') = z_2\\). By assumption this implies \\(E(x(k); \\theta) &lt; E(x(k'); \\theta)\\), however, this is a contradiction as by iterating Proposition 2 we have for any \\(k \\geq 0\\) the sequence of inequalities \\(E(x(k); \\theta) \\leq E(x(k+1); \\theta) \\leq ...\\leq E(x(k'); \\theta)\\). As a result for any pair \\(z_1, z_2 \\in S(x)\\) we have \\(E(z_1; \\theta) = E(z_2; \\theta)\\).\nIt suffices to show for any arbitrary \\(x \\in \\{0,1 \\}^n\\) that \\(|S(x)| = 1\\). Consider the compliment \\(S^c(x) = \\{ 0,1\\}^n \\backslash S(x)\\): if \\(z \\in \\{ 0,1\\}^n \\in S^c(x)\\) then there exists an iteration \\(K(z,x) \\geq 0\\) such that for all \\(k &gt; K(z,x)\\) we have \\(x(k) \\neq z\\). Define \\(K(x) = \\max_{z \\in S^c} K(z,x) + 1\\) which by definition must be finite, then by construction for all \\(k\\geq K(x)\\) we have \\(x(k) \\in S(x)\\). Let \\[\n\\mathcal{K}(x) = \\{k \\in {\\mathbb{Z}}_{\\geq K(x)}: x(k+1) \\neq x(k) \\}\n\\] If \\(|S(x)| \\geq 2\\) then there does not exists an \\(M \\in {\\mathbb{Z}}_{\\geq 0}\\) such that \\(|\\mathcal{K}| \\leq M\\), indeed, if the number of updates (i.e., iteratations where a bit change occurs) is bounded then it must follow that \\(|S(x)| =1\\)! Let \\(k \\in \\mathcal{K}(x)\\): by definition then \\(x(k+1), x(k) \\in S(x)\\), which implies \\(E(x(k+1); \\theta) = E(x(k), \\theta)\\), and by the update rule there exists an \\(i \\in [n]\\) such that \\(x_j(k) \\neq x_j(k'+1)\\) iff \\(i \\neq j\\). By Proposition 2 equality can can only hold if \\(x_i(k)=1\\) and \\(x_i(k+1) = 0\\): to be clear, this therefore implies at any iteration \\(k \\in \\mathcal{K}(x)\\) that a bit must be turned off. However, there are only \\(n\\) bits in total which implies \\(|\\mathcal{K}(x)| \\leq n\\) which in turn contradicts \\(|S(x)| \\geq 2\\). As a result we conclude \\(|S(x)|=1\\) and convergence occurs after \\(K(x)&lt; \\infty\\) iterations. QED.\n\nIn light of Proposition 3, then given a set of parameters \\(\\theta = (W, h)\\), where \\(W \\in \\text{Symm}_0(n)\\) and \\(h \\in \\mathbb{R}^d\\), for any \\(x \\in \\{0,1 \\}^n\\) there exists a \\(K \\in {\\mathbb{Z}}_{\\geq 0}\\) and an \\(x^* \\in \\{0,1 \\}^n\\) such that \\(x(k) = x^*\\) for all \\(k \\geq K\\). We define the input-output map of the Hopfield network \\(H_{\\theta}: \\{0,1\\}^n \\rightarrow \\{ 0,1\\}^n\\) parameterized by \\(\\theta\\) as \\(H_{\\theta}(x) = x^*\\). As a small remark, for synchronous updates it is bit trickier to define the input-output map as their dynamics do not necessarily converge to a fixed point, indeed, they can also converge to limit cycle of length 2 (Bruck 1990)!"
  },
  {
    "objectID": "posts/flatness1/index.html#training-a-hopfield-network-via-a-minimum-probability-flow-mpf-approach",
    "href": "posts/flatness1/index.html#training-a-hopfield-network-via-a-minimum-probability-flow-mpf-approach",
    "title": "On some elementary properties of Hopfield networks",
    "section": "Training a Hopfield network via a minimum probability flow (MPF) approach",
    "text": "Training a Hopfield network via a minimum probability flow (MPF) approach\nTraining a Boltzmann network can perhaps best be described as configuring the parameters in order that a given set of target binary vectors are the fixed points of the recurrence dynamics described above. Let \\(\\mathcal{T}\\subset \\{ 0,1\\}^n\\) denote the set of target binary vectors we want to store in our Hopfield network with input-output map \\(H_{\\theta}\\): we say that \\(H_{\\theta}\\) memorizes \\(\\mathcal{T}\\) if \\(x = H_{\\theta}(x)\\) for all \\(x \\in \\mathcal{T}\\). There are a number of ways a Hopfield network can be trained to memorize a target set, here we present just one method taken from (Hillar, Sohl-Dickstein, and Koepsell 2015), which is based on minimum probability flows (Sohl-Dickstein, Battaglino, and DeWeese 2011). let \\(\\mathcal{N}(x) = \\{x' \\in \\{0,1 \\}^n: \\sum_{i=1}^n |x_i - x_i'| = 1 \\}\\) denote the set of binary vectors which are a hamming distance of exactly one away from \\(x\\) and define \\(\\Theta = \\text{Symm}_0(n) \\times \\mathbb{R}^d\\). Now consider the loss function \\(L: \\Theta \\rightarrow \\mathbb{R}_{&gt;0}\\), defined as \\[\nL(\\theta) = \\sum_{x \\in \\mathcal{T}} \\sum_{x' \\in \\mathcal{N}(x)} \\exp\\left( \\frac{E(x;\\theta ) - E(x'; \\theta)}{2} \\right).\n\\] We refer to this as the minimum probability flow (MPF) loss, which is motivated by considering the KL-divergence between the data distribution and the gibbs measure associated with \\(E\\) (see (Sohl-Dickstein, Battaglino, and DeWeese 2011), (Hillar, Sohl-Dickstein, and Koepsell 2015) ). Intuitively such a loss makes sense for the following reasons.\n\nThe fixed points of the asynchronous dynamics for a set of parameters \\(\\theta\\) correspond to those states \\(x \\in \\{0,1\\}^n\\) must be local minima, i.e., \\(E(x; \\theta) \\leq E(x';\\theta)\\) for all \\(x' \\in \\mathcal{N}(x)\\). However, not all local minima are guaranteed to be fixed points, in particular, as per Proposition 2 it is possible to move to a state with the same energy by turning a bit off. However, if a state is a strict local minimum of the energy function this is sufficient to ensure it is fixed point: note we call a state \\(x \\in \\{ 0,1\\}^n\\) a strict local minimum if \\(E(x; \\theta) &lt; E(x';\\theta)\\) for all \\(x' \\in \\mathcal{N}(x)\\). Following this, we say \\(H_{\\theta}\\) strictly memorizes \\(\\mathcal{T}\\) if \\(x\\) is a strict local minimum for all \\(x \\in \\mathcal{T}\\).\nTherefore, to ensure our Hopfield network memorizes \\(\\mathcal{T}\\subset \\{ 0,1\\}^n\\) it suffices to choose \\(\\theta\\) such that \\(x\\) is a strict local minimum of \\(E\\) for all \\(x \\in \\mathcal{T}\\).\nBy inspection, the loss \\(L\\) is small whenever \\(\\theta\\) is such that \\(E(x'; \\theta)\\) is large relative to \\(E(x; \\theta)\\) for all \\(x \\in \\mathcal{T}\\) and \\(x' \\in \\mathcal{N}(x)\\). Moreover, again by inspection if \\(L(\\theta) &lt; 1\\) then it must hold that \\(\\exp\\left( \\frac{E(x;\\theta ) - E(x'; \\theta)}{2} \\right)&lt;1\\) and therefore \\(E(x; \\theta) &lt; E(x';\\theta)\\) for all \\(x \\in \\mathcal{T}\\) and \\(x' \\in \\mathcal{N}(x)\\). To be clear, \\(L(\\theta) &lt; 1\\) is a sufficient condition to conclude \\(H_{\\theta}\\) strictly memorizes \\(\\mathcal{T}\\).\n\nDepending on the nature of \\(\\mathcal{T}\\) it may not be possible to store all of its elements as strict local minima of a Hopfield network with a given number of nodes \\(n\\). However, whenever this is possible we can find the appropriate parameters by minimizing \\(L\\). The following proposition highlights that the MPF loss \\(L\\) is amenable to optimization.\n\nProposition 4 The MPF loss \\(L\\) defined for a set \\(\\mathcal{T}\\subset \\{0,1 \\}^n\\) has the following properties.\n\n\\(L\\) is infinitely differentiable and convex with respect to the parameters.\nSuppose there exist parameters \\(\\hat{\\theta}\\) such that \\(\\mathcal{T}\\) is strictly memorized by \\(H_{\\hat{\\theta}}\\), if \\(\\theta^* \\in \\text{inf}_{\\theta \\in \\Theta} L(\\theta)\\) then \\(\\mathcal{T}\\) is also strictly memorized by \\(H_{\\theta^*}\\).\n\n\n\nProof. Statement 1) is easy, the exponential function is infinitely differentiable and convex in its exponent and \\(E(x;\\theta)\\) is affine in the parameters \\(\\theta\\). The composition of a smooth, convex function with an affine function is smooth and convex, furthermore a positively weighted sum of smooth convex functions is also smooth and convex. Turning our attention to 2), for any \\(\\theta \\in \\Theta\\) let \\[\n\\delta(\\theta) := \\max_{x \\in \\mathcal{T}} \\max_{x' \\in \\mathcal{N}(x)} \\left(\\frac{E(x;\\theta) -  E(x';\\theta)}{2} \\right).\n\\] Note by construction \\[\nL(\\theta) \\leq n|\\mathcal{T}|\\exp(\\delta(\\theta)).\n\\] Moreover, as the energy function is a linear in the parameters we have for any positive scalar \\(\\alpha \\in \\mathbb{R}_{&gt;0}\\) that \\(\\delta(\\alpha \\theta) = \\alpha \\delta( \\theta)\\). By assumption there exist parameters \\(\\hat{\\theta} \\in \\Theta\\) such that for all \\(x \\in \\mathcal{T}\\) we have \\(E(x;\\hat{\\theta}) &lt; E(x';\\hat{\\theta})\\) for all \\(x' \\in \\mathcal{N}(x)\\), thus \\(\\delta(\\hat{\\theta})&lt; 0\\). Let \\(\\alpha' = -\\delta^{-1}(\\hat{\\theta})\\ln(2n|\\mathcal{T}|)\\), then by the definition of \\(\\theta^*\\) being a minimizer of \\(L\\) we have \\[\nL(\\theta^*) \\leq L(\\alpha' \\hat{\\theta}) \\leq n|\\mathcal{T}|\\exp(\\alpha' \\delta( \\hat{\\theta}))= \\frac{1}{2}.\n\\] As previously observed, \\(L(\\theta^*)&lt;1\\) is sufficient to conclude that \\(H_{\\theta^*}\\) strictly memorizes \\(\\mathcal{T}\\). As a quick aside, note if we instead assume that \\(H_{\\hat{\\theta}}\\) only memorizes \\(\\mathcal{T}\\), then it is possible that \\(\\delta(\\hat{\\theta})=0\\) and this argument would breakdown! QED.\n\nSo far so good, but even if we can minimize the MPF loss to ensure a set of target states is strictly memorized by a Hopfield network, then, and as with any machine learning model, what guarantees do we have that the model will perform well on other inputs? In particular, even if all target states are fixed points it may not be the case that all fixed points are a target state. We refer to fixed points outside the target set as spurious as they artefacts arising during training. Furthermore note that if a spurious fixed point has a large basin of attraction then many inputs will be mapped to an output which potentially has no practical meaning or interpretation. In addition, even if we avoid spurious fixed points we still it seems have few controls over the basins of attraction of our targets, i.e., which inputs are mapped to which target states."
  },
  {
    "objectID": "posts/flatness1/index.html#summary",
    "href": "posts/flatness1/index.html#summary",
    "title": "On some elementary properties of Hopfield networks",
    "section": "Summary",
    "text": "Summary\nIn this post we have shown how the input-output map of a Hopfield network can be defined using an asynchronous recurrence relation, and also how a Hopfield network can be trained so that certain target states correspond to the fixed points of the dynamics!"
  }
]