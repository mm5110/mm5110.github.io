<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michael Murray">
<meta name="dcterms.date" content="2024-06-27">

<title>Home - A simple neural network with a benign loss landscape</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html" rel="" target="">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html" rel="" target="">
 <span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A simple neural network with a benign loss landscape</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Neural Networks</div>
                <div class="quarto-category">Optimization</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Michael Murray </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 27, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link" data-scroll-target="#preliminaries">Preliminaries</a>
  <ul class="collapse">
  <li><a href="#points-on-notation" id="toc-points-on-notation" class="nav-link" data-scroll-target="#points-on-notation">Points on notation</a></li>
  <li><a href="#data-network-and-loss" id="toc-data-network-and-loss" class="nav-link" data-scroll-target="#data-network-and-loss">Data, network and loss</a></li>
  <li><a href="#first-order-derivatives" id="toc-first-order-derivatives" class="nav-link" data-scroll-target="#first-order-derivatives">First order derivatives</a></li>
  <li><a href="#second-order-derivatives" id="toc-second-order-derivatives" class="nav-link" data-scroll-target="#second-order-derivatives">Second order derivatives</a></li>
  <li><a href="#recap-on-stationary-points" id="toc-recap-on-stationary-points" class="nav-link" data-scroll-target="#recap-on-stationary-points">Recap on stationary points</a></li>
  </ul></li>
  <li><a href="#all-local-minima-are-global-and-all-saddles-are-strict-saddles" id="toc-all-local-minima-are-global-and-all-saddles-are-strict-saddles" class="nav-link" data-scroll-target="#all-local-minima-are-global-and-all-saddles-are-strict-saddles">All local minima are global and all saddles are strict saddles</a>
  <ul class="collapse">
  <li><a href="#supporting-lemmas" id="toc-supporting-lemmas" class="nav-link" data-scroll-target="#supporting-lemmas">Supporting Lemmas</a></li>
  <li><a href="#proof-of-thm-quad" id="toc-proof-of-thm-quad" class="nav-link" data-scroll-target="#proof-of-thm-quad">Proof of Theorem&nbsp;1</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><em>We look at an example of a simple network whose loss landscape, albeit non-convex, is relatively benign from an optimization perspective. This post is based on Theorem 2.1 of <span class="citation" data-cites="quad-network-landscape">(<a href="#ref-quad-network-landscape" role="doc-biblioref">Soltanolkotabi, Javanmard, and Lee 2019</a>)</span>. </em></p>
<div class="hidden">
%%%%% NEW MATH DEFINITIONS %%%%% $$ %
{lemma}{Lemma}
<p>{}[1]{{#1}} </p>
<p>% Figure reference, lower-case. % Figure reference, capital. For start of sentence % Section reference, lower-case. % Section reference, capital. % Reference to two sections. % Reference to three sections. % Reference to an equation, lower-case. % Reference to an equation, upper case % A raw reference to an equation—avoid using if possible % Reference to a chapter, lower-case. % Reference to an equation, upper case. % Reference to a range of chapters % Reference to an algorithm, lower-case. % Reference to an algorithm, upper case. % Reference to a part, lower case % Reference to a part, upper case </p>
<p>% Random variables % rm is already a command, just don’t name any random variables m </p>
<p>% Random vectors </p>
<p>% Elements of random vectors </p>
<p>% Random matrices </p>
<p>% Elements of random matrices </p>
<p>% Vectors </p>
<p>% Elements of vectors </p>
<p>% Matrix </p>
% Tensor
<p>% </p>
<p>% Graph </p>
<p>% Sets % Don’t use a set called E, because this would be the same as our symbol % for expectation. </p>
<p>% Entries of a matrix </p>
% entries of a tensor % Same font as tensor, without wrapper
<p>% The true underlying data generating distribution % The empirical distribution defined by the training set % The model distribution % Stochastic autoencoder distributions </p>
<p>% Laplace distribution</p>
<p>% Wolfram Mathworld says <span class="math inline">\(L^2\)</span> is for function spaces and <span class="math inline">\(\ell^2\)</span> is for vectors % But then they seem to use <span class="math inline">\(L^2\)</span> for vectors throughout the site, and so does % wikipedia. </p>
<p>% See usage in notation.tex. Chosen to match Daphne’s book.</p>
% MM commands % Defined commands
<p>% </p>
<p>% Caligraphic letters $$</p>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>A surprising aspect when it comes to training neural networks is that first order optimization methods appear to be pretty effective despite the fact that the loss landscape they traverse is typically non-convex. A nice line of work, in particular <span class="citation" data-cites="pmlr-v49-lee16">(<a href="#ref-pmlr-v49-lee16" role="doc-biblioref">Lee et al. 2016</a>)</span>, <span class="citation" data-cites="escape_saddles">(<a href="#ref-escape_saddles" role="doc-biblioref">Du et al. 2017</a>)</span> and related, state results under mild conditions on the initialization and objective function to the effect that if gradient descent converges then it converges to a local minimum almost surely. However, although roughly speaking perturbed versions of gradient descent are able to escape saddles, there is still the question of <strong>why do first order methods not get stuck in bad local minima?</strong> A popular hypothesis for explaining this is that at least for some networks, e.g., those which are sufficiently overparameterized, then local minima are sufficiently `rare’ versus global minima that they are unlikely to be encountered during training.</p>
<p>Here we look at a simple example which illustrates a particularly strong flavor of this idea: in short, for a sufficiently wide, shallow network with quadratic activations then for general data it can be shown that all local minima are global and have zero loss (Theorem 2.1 of <span class="citation" data-cites="quad-network-landscape">(<a href="#ref-quad-network-landscape" role="doc-biblioref">Soltanolkotabi, Javanmard, and Lee 2019</a>)</span>)!</p>
</section>
<section id="preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries">Preliminaries</h2>
<section id="points-on-notation" class="level3">
<h3 class="anchored" data-anchor-id="points-on-notation">Points on notation</h3>
<p>Given a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times d}\)</span> then <span class="math inline">\(a \in \mathbb{R}^{d}\)</span> denotes its <span class="math inline">\(i\)</span>th row. We denote the entry at the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column either as <span class="math inline">\([A]_{ij}\)</span> or <span class="math inline">\(a_{ij}\)</span>. Given a scalar function <span class="math inline">\(f:\mathbb{R}^{m \times d} \rightarrow \mathbb{R}\)</span> we use <span class="math inline">\(\partial_A f(A) \in \mathbb{R}^{m \times d}\)</span> to denote the matrix-scalar derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(A\)</span>, in particular <span class="math inline">\([\partial_A f(A)]_{ij} = \frac{\partial f(A)}{\partial A_ij}\)</span>. Given a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times d}\)</span> we use <span class="math inline">\(vec(A) \in \mathbb{R}^{md}\)</span> to denote the vectorized or flattened version of <span class="math inline">\(A\)</span>. Given a matrix or vector <span class="math inline">\(A \in \mathbb{R}^{m \times d}\)</span> and a scalar function <span class="math inline">\(f(A)\)</span> then we use <span class="math inline">\(\nabla_A f(A)\)</span> to denote the gradient, equivalently <span class="math inline">\(\nabla_A f(A) = \partial_{vec(A)} f(A)\)</span>. We use <span class="math inline">\(A \otimes B\)</span> denote the Kronecker product between matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. We use <span class="math inline">\(A \star B\)</span> denote the Khatri-Rao product between matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. If it is not clear from the context what the dimensions or shape of a tensor are we will use a subscript to clarify, this is most common when working with the 0 tensor.</p>
</section>
<section id="data-network-and-loss" class="level3">
<h3 class="anchored" data-anchor-id="data-network-and-loss">Data, network and loss</h3>
<p>Consider a training sample of <span class="math inline">\(n\)</span> input-label pairs <span class="math inline">\((x_i, y_i)_{i \in [n]}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span> and <span class="math inline">\(y_i \in \mathbb{R}\)</span>, we use <span class="math inline">\(X \in \mathbb{R}^{d \times n}\)</span> to denote the matrix of training inputs stored column-wise. Let <span class="math inline">\(f:\mathbb{R}^d \times \mathbb{R}^{2m \times d} \rightarrow \mathbb{R}\)</span> be a neural network defined as <span class="math display">\[
f(x;W) = \frac{1}{2} \sum_{j=1}^{2m}(-1)^j (w_j^Tx)^2
\]</span> where <span class="math inline">\(w_j\)</span> denotes the <span class="math inline">\(j\)</span>th row of <span class="math inline">\(W\)</span>. We seek to minimize the squared error loss <span class="math inline">\(L:\mathbb{R}^{2m \times d} \rightarrow \mathbb{R}\)</span> of this network over the training sample, <span class="math display">\[
L(W) = \frac{1}{2} \sum_{i =1}^n ( f(x_i, W) - y_i )^2.
\]</span> Observe that <span class="math inline">\(L\)</span> is twice differentiable, we denote the gradient of <span class="math inline">\(L\)</span> as <span class="math inline">\(\nabla L : \mathbb{R}^{2m \times d} \rightarrow \mathbb{R}^{2md}\)</span> and the Hessian as <span class="math inline">\(\nabla^2L: \mathbb{R}^{2m \times d} \rightarrow \mathbb{R}^{2md \times 2md}\)</span>.</p>
</section>
<section id="first-order-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="first-order-derivatives">First order derivatives</h3>
<p>Let <span class="math inline">\(r(W) \in \mathbb{R}^n\)</span> be the residual vector with entries <span class="math inline">\(r_i(W):= f(x_i;W) - y_i\)</span>, then for any <span class="math inline">\(W \in \mathbb{R}^{2m \times d}\)</span> the matrix-scalar derivative <span class="math inline">\(\partial_W L: \mathbb{R}^{2m \times d} \rightarrow \mathbb{R}^{2m \times d}\)</span> is <span class="math display">\[
\partial_W L(W) = \sum_{i=1}^n r_i(W) \partial_W f(x_i; W).
\]</span> As <span class="math inline">\(\frac{\partial f(x_i; W)}{\partial w_r} = (-1)^r (w_r^T x_i) x_i\)</span>, then letting <span class="math inline">\(D \in \mathbb{R}^{2m \times 2m}\)</span> be a diagonal matrix with non-zero entries <span class="math inline">\(D_{jj} = (-1)^j\)</span> we have <span class="math display">\[
\begin{align*}
\partial_W f(x_i; W) = DW x_i x_i^T.
\end{align*}
\]</span> Therefore <span class="math display">\[
\begin{align*}
\partial_W L(W) &amp;= \sum_{i=1}^n r_i(W) \partial_W f(x_i; W)
= DW \sum_{i=1}^n r_i(W)  x_i x_i^T.
\end{align*}
\]</span> Similarly as <span class="math inline">\(\nabla_W f(x; W) = (DW x_i) \otimes x_i\)</span> <span class="math display">\[
\begin{align*}
\nabla L(W) = \sum_{i=1}^n r_i(W) \nabla_W f(x_i; W)
&amp;= \sum_{i=1}^n r_i(W) (DW x_i) \otimes x_i.
\end{align*}
\]</span> We can also write this in terms of the Jacobian <span class="math inline">\(J_F(W)\in \mathbb{R}^{2md \times n}\)</span> of the network on the training sample: if <span class="math inline">\(F(W) := [f(x_1; W), f(x_2;W)... f(x_n; W) ]^T \in \mathbb{R}^{n}\)</span> then<br>
<span class="math display">\[
\begin{align*}
J_F(W) &amp;:= \nabla_W F(W)\\
&amp;= [\nabla_W f(x_1; W),  \nabla_W f(x_2; W)... \nabla_W f(x_n; W)]\\
&amp;= [DWx_1 \otimes x_1, DWx_2 \otimes x_2... DWx_n \otimes x_n] \\
&amp;= DWX \star X.
\end{align*}
\]</span> As a result <span class="math display">\[
\nabla L(W) = J_F(W)r(W).
\]</span></p>
</section>
<section id="second-order-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="second-order-derivatives">Second order derivatives</h3>
<p>Turning our attention to the Hessian, observe as <span class="math inline">\(L(W) = \frac{1}{2} \sum_{i=1}^n r_i(W)^2\)</span> then <span class="math display">\[
\begin{align*}
\frac{\partial^2 L(W)}{\partial w_{rj} w_{lk}} &amp;= \frac{\partial}{\partial w_{rj}} \sum_{i=1}^n r_i(W) \frac{\partial f(x_i; W)}{\partial w_{lk}}\\
&amp;= \sum_{i=1}^n \frac{\partial f(x_i; W)}{\partial w_{rj}} \frac{\partial f(x_i; W)}{\partial w_{lk}} +  \sum_{i=1}^n r_i(W) \frac{\partial^2 f(x_i; W)}{\partial w_{rj}\partial w_{lk}}
\end{align*}
\]</span> Recall <span class="math inline">\(\frac{\partial f(x_i;W)}{\partial w_{rj}} = (-1)^r (w_r^T x_i) x_{ij}\)</span>, as a result for <span class="math inline">\(r \neq l\)</span> then <span class="math inline">\(\frac{\partial^2 f(x_i;W)}{\partial w_{rj} \partial w_{lk}} = 0\)</span> while <span class="math inline">\(\frac{\partial^2 f(x_i;W)}{\partial w_{rj} \partial w_{rk}} = (-1)^r x_{ij} x_{ik}\)</span>. Therefore <span class="math display">\[
\frac{\partial^2 L(W)}{\partial w_{rj} w_{lk}} = \sum_{i=1}^n (-1)^{r + l} (w_r^T x_i) (w_l^T x_i) x_{ij}x_{ik} + \mathbb{1}_{r = l} r_i(W) (-1)^r x_{ij} x_{ik}.
\]</span> As a result <span class="math display">\[
\frac{\partial^2 L(W)}{\partial w_{r} w_{l}} = \sum_{i=1}^n (-1)^{r + l} (w_r^T x_i) (w_l^T x_i) x_{i}x_{i}^T + \mathbb{1}_{r = l} r_i(W) (-1)^r x_{i} x_{i}^T.
\]</span> Based on this observe <span class="math display">\[
\begin{align*}
  &amp;vec(U)^T \nabla_W^2 L(W) vec(U) \\
  &amp;= \sum_{r=1}^{2m}\sum_{l=1}^{2m} u_r^T \frac{\partial^2 L(W)}{\partial w_{r} w_{l}} u_l\\
  &amp;= \sum_{r=1}^{2m}\sum_{l=1}^{2m} \left( \sum_{i=1}^n (-1)^{r + l} (w_r^T x_i) (w_l^T x_i) (u_r^T x_{i})(x_{i}^Tu_l) + \mathbb{1}_{r = l} r_i(W) (-1)^r (u_r^T x_{i})(x_{i}^Tu_l) \right)\\
  &amp;=  \sum_{i=1}^n \sum_{r=1}^{2m}(-1)^r (w_r^T x_i) (u_r^T x_{i}) \sum_{l=1}^{2m}(-1)^l(x_{i}^Tu_l)(w_l^T x_i) + \sum_{i=1}^n r_i(W)  \sum_{r=1}^{2m}(-1)^r (u_r^T x_{i}) \sum_{l=1}^{2m} \mathbb{1}_{r = l} (x_{i}^Tu_l) \\
  &amp;= \sum_{i=1}^n \left(\sum_{r=1}^{2m}(-1)^r (w_r^T x_i) (u_r^T x_{i})\right)^2 + \sum_{i=1}^n r_i(W)  \sum_{r=1}^{2m}(-1)^r (u_r^T x_{i})^2 \\
  &amp;= \sum_{i=1}^n \left( x_i^T W^T D U x_i\right)^2 + \sum_{i=1}^n r_i(W) x_i^T U^T D U x_i.
\end{align*}
\]</span></p>
</section>
<section id="recap-on-stationary-points" class="level3">
<h3 class="anchored" data-anchor-id="recap-on-stationary-points">Recap on stationary points</h3>
<ul>
<li><p>A stationary point <span class="math inline">\(W\)</span> of <span class="math inline">\(L\)</span> satisfies <span class="math inline">\(\nabla L(W) = 0\)</span>. A stationary point is either a local minimum, global minimum or saddle.</p></li>
<li><p>The stationary points of a sufficiently smooth function can be classified based on their local curvature / second order information.</p>
<ul>
<li>If <span class="math inline">\(\nabla^2 L(W) \succ 0\)</span> then is an (isolated) local minimum.</li>
<li>If <span class="math inline">\(\nabla^2 L(W) \prec 0\)</span> then is an (isolated) local maximum.</li>
<li>if <span class="math inline">\(\nabla^2 L(W) \succcurlyeq 0\)</span> then could either be a saddle or a (non-isolated) local minimum,</li>
<li>if <span class="math inline">\(\nabla^2 L(W) \preccurlyeq 0\)</span> then could be either a saddle or a (non-isolated) local maximum.</li>
<li>If <span class="math inline">\(\nabla^2 L(W)\)</span> has both positive and negative eigenvalues then is a saddle point.</li>
</ul></li>
<li><p>In keeping with convention we define a <strong>strict saddle</strong> as any point having at least one direction of negative curvature, i.e., <span class="math inline">\(\lambda_{min}(\nabla^2 L(W)) &lt; 0\)</span>. Roughly speaking strict saddles are nice as GD can escape them almost surely <span class="citation" data-cites="pmlr-v49-lee16">(<a href="#ref-pmlr-v49-lee16" role="doc-biblioref">Lee et al. 2016</a>)</span>, <span class="citation" data-cites="escape_saddles">(<a href="#ref-escape_saddles" role="doc-biblioref">Du et al. 2017</a>)</span> given a random initialization.</p></li>
</ul>
</section>
</section>
<section id="all-local-minima-are-global-and-all-saddles-are-strict-saddles" class="level2">
<h2 class="anchored" data-anchor-id="all-local-minima-are-global-and-all-saddles-are-strict-saddles">All local minima are global and all saddles are strict saddles</h2>
<p>The following is a rephrased version of Theorem 2.1 <span class="citation" data-cites="quad-network-landscape">(<a href="#ref-quad-network-landscape" role="doc-biblioref">Soltanolkotabi, Javanmard, and Lee 2019</a>)</span>.</p>
<div id="thm-quad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>If <span class="math inline">\(m \geq d\)</span> then the following three properties hold for <span class="math inline">\(L(W)\)</span>.</p>
<ol type="1">
<li><p>All local minima are global and all saddle points are strict saddle points.</p></li>
<li><p>In addition if <span class="math inline">\(d \geq 3\)</span> and <span class="math inline">\(n \lesssim d^2\)</span> then for almost all <span class="math inline">\(X\in \mathbb{R}^{d \times n}\)</span> any global optimum <span class="math inline">\(W^*\)</span> of <span class="math inline">\(L\)</span> satisfies <span class="math inline">\(L(W^*)=0\)</span>.</p></li>
</ol>
</div>
<p><strong>The outline of the proof for <a href="#thm-quad">Theorem&nbsp;1</a> is as follows.</strong></p>
<ol type="a">
<li><p>Any stationary point satisfies <span class="math inline">\(DW \sum_{i=1}^n r_i(W) x_i x_i^T = 0\)</span>.</p></li>
<li><p>If in addition <span class="math inline">\(\nabla L(W)=0\)</span> and <span class="math inline">\(\nabla^2L(W) \succcurlyeq 0\)</span> then this implies <span class="math inline">\(\sum_{i=1}^n r_i(W) x_i x_i^T = 0\)</span>.</p></li>
<li><p>By <a href="#lem-proxy">Lemma&nbsp;1</a> if <span class="math inline">\(\sum_{i=1}^n r_i(W) x_i x_i^T = 0\)</span> then <span class="math inline">\(W\)</span> is a global minimizer.</p></li>
<li><p>Furthermore if <span class="math inline">\(W\)</span> is a stationary point and <span class="math inline">\(J_F(W)\)</span> is full rank then as <span class="math inline">\(\nabla L(W) = J_F(W) r(W) = 0\)</span> this implies <span class="math inline">\(r(W) = 0\)</span> and therefore <span class="math inline">\(W\)</span> is also global minimizer with <span class="math inline">\(0\)</span> loss.</p></li>
</ol>
<section id="supporting-lemmas" class="level3">
<h3 class="anchored" data-anchor-id="supporting-lemmas">Supporting Lemmas</h3>
<section id="certificate-for-global-optimality" class="level4">
<h4 class="anchored" data-anchor-id="certificate-for-global-optimality">Certificate for global optimality</h4>
<p>The following lemma gives a sufficient condition for global optimality. Abstracting the argument, this is achieved by decomposing the loss into the composition of two functions, <span class="math inline">\(L(W) = L_G(G(W))\)</span>, we can therefore interpret <span class="math inline">\(L\)</span> as a particular restriction of <span class="math inline">\(L_G\)</span>. Note that any global minimizer of <span class="math inline">\(L\)</span> must also be a global minimizer of <span class="math inline">\(L_G\)</span>. This allows us to substitute the problem of finding a global certificate for <span class="math inline">\(L\)</span> (which might be hard!) with the problem of finding a global certificate for <span class="math inline">\(L_G\)</span> (which hopefully is a lot easier…). In the setting studied here we can relax the problem by using a more arbitrary quadratic form instead of a neural network with quadratic activations. Moreover the proxy loss <span class="math inline">\(L_G\)</span> is convex in the parameters of this quadratic form and hence a global certificate for <span class="math inline">\(L_G\)</span> in this setting is easy to derive!</p>
<div id="lem-proxy" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 </strong></span>If <span class="math inline">\(\tilde{W} \in \mathbb{R}^{2m \times d}\)</span> satisfies <span class="math display">\[
  \sum_{i=1}^n r_i(\tilde{W}) x_i x_i^T = 0
\]</span> then <span class="math inline">\(\tilde{W}\)</span> is a global minimizer of <span class="math inline">\(L(W)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>First of all we note the obvious: a shallow network with quadratic activations is a quadratic form with respect to the input data, indeed <span class="math inline">\(f(x;W) = x^TW^TDWx\)</span>. As a result we can re-write the condition of the lemma as <span class="math display">\[
\sum_{i=1}^n r_i(\tilde{W}) x_i x_i^T = \sum_{i=1}^n (x_i^T \tilde{W}^T D \tilde{W} x_i - y_i)x_i x_i^T =0.
\]</span> We now relax the problem by instead considering a predictor <span class="math inline">\(g(x;M) = x^T M x\)</span> instead of <span class="math inline">\(f(x;W)\)</span> and look at the squared error in this context, <span class="math display">\[
\begin{align*}
L_g(M) &amp;:= \frac{1}{2} \sum_{i=1}^n (g(x_i, M) - y_i)^2.
\end{align*}
\]</span></p>
<p>As <span class="math inline">\(g\)</span> is affine in <span class="math inline">\(M\)</span> then <span class="math inline">\((g(x; M) - y)^2\)</span> is a composition of a convex function with an affine function and therefore is convex. It follows that <span class="math inline">\(L_g\)</span> is differentiable and convex as it is a positively weighted sum of convex, differentiable functions. As a result any global minimizer <span class="math inline">\(M^*\)</span> of <span class="math inline">\(L_g\)</span> satisfies <span class="math inline">\(\nabla L_g(M^*)= 0_{2md}\)</span> or equivalently <span class="math inline">\(\partial_M L_g(M^*)= 0_{2m \times d}\)</span>, which in turn implies <span class="math display">\[
\sum_{i=1}^n(g(x_i, M^*) - y_i) x_i x_i^T = 0.
\]</span> Now consider the square matrix <span class="math inline">\(\tilde{W}^T D \tilde{W}\)</span>: based on the above observation if <span class="math display">\[
  \sum_{i=1}^n(g(x_i, \tilde{W}^T D \tilde{W}) - y_i) x_i x_i^T  = \sum_{i=1}^n(x_i^T \tilde{W}^T D \tilde{W} x_i - y_i) x_i x_i^T= 0
\]</span> then <span class="math inline">\(\tilde{W}^T D \tilde{W}\)</span> is a global minimizer of <span class="math inline">\(L_g\)</span>. By construction <span class="math inline">\(L(W) = L_g(W^T D W)\)</span> for all <span class="math inline">\(W \in \mathbb{R}^{2m \times d}\)</span>, therefore if <span class="math inline">\(\tilde{W}^T D \tilde{W}\)</span> is a global minimizer of <span class="math inline">\(L_g\)</span> then <span class="math inline">\(\tilde{W}\)</span> is a global minimzer of <span class="math inline">\(L\)</span> as <span class="math display">\[
L(\tilde{W}) = L_g(\tilde{W}^T D \tilde{W}) \leq L_g(W^T D W) = L(W)
\]</span> for all <span class="math inline">\(W \in \mathbb{R}^{2m \times d}\)</span>.</p>
</div>
</section>
<section id="proving-x-star-x-is-full-rank-almost-everywhere" class="level4">
<h4 class="anchored" data-anchor-id="proving-x-star-x-is-full-rank-almost-everywhere">Proving <span class="math inline">\(X \star X\)</span> is full rank almost everywhere</h4>
<p>We will show all local minima satisfy the condition <span class="math inline">\((X \star X)r(W) = 0_{d^2}\)</span>. Assuming <span class="math inline">\(d^2 \geq n\)</span> then <span class="math inline">\(X \star X \in \mathbb{R}^{d^2 \times n}\)</span> is a tall matrix. If <span class="math inline">\(rank(X \star X) = n\)</span> then this implies <span class="math inline">\(null(X \star X) = \emptyset\)</span> and therefore <span class="math inline">\((X \star X)r(W) = 0_{d^2}\)</span> implies <span class="math inline">\(r(W) = 0\)</span> which in turn implies <span class="math inline">\(W\)</span> is a global minimum. Let <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> be a rectangular matrix where without loss of generality we assume <span class="math inline">\(m\geq n\)</span> (note the column and row rank are equal and therefore we can always instead study <span class="math inline">\(A^T\)</span> if is this is not true). <span class="math inline">\(A\)</span> is full rank if and only if there exists a square submatrix formed by selecting a subset of size <span class="math inline">\(n\)</span> out of <span class="math inline">\(m\)</span> possible rows which is full rank. Therefore <span class="math inline">\(A\)</span> is not full rank, or singular, if and only if all possible square submatrices formed by selecting <span class="math inline">\(n\)</span> out of <span class="math inline">\(m\)</span> possible rows are not full rank. Recall also a square matrix <span class="math inline">\(B \mathbb{R}^{n \times n}\)</span> is singular if and only its determinant is zero. Recall the determinant of <span class="math inline">\(B\)</span> is a polynomial in the entries of the <span class="math inline">\(B\)</span> and for any non-zero polynomial its zero-set has Lebesgue measure 0. To abstract the argument we are about to present let <span class="math inline">\(G:\mathbb{R}^p \rightarrow \mathbb{R}^{m \times n}\)</span> with <span class="math inline">\(m \geq n\)</span> output matrices whose entries are polynomials of the elements of the input <span class="math inline">\(\theta\)</span>. Consider the Lebesgue product measure on <span class="math inline">\(\mathbb{R}^p\)</span>, which we denote <span class="math inline">\(\mu\)</span>, and for <span class="math inline">\(S \subseteq [m]\)</span> let <span class="math inline">\(G(\theta)_S)\)</span> denote submatrix of <span class="math inline">\(G(\theta)\)</span> formed by taking only those rows with index in <span class="math inline">\(S\)</span>. The kind of result we are after is often derived using the following argument.</p>
<ol type="1">
<li><p>Define <span class="math inline">\(p(\theta) = \sum_{S \subset [n], |S| = n} det(G(\theta)_S)\)</span>.</p></li>
<li><p>As <span class="math inline">\(p\)</span> is a sum of polynomials of the elements of <span class="math inline">\(G(\theta)\)</span> and the elements of <span class="math inline">\(G(\theta)\)</span> are themselves polynomial in <span class="math inline">\(\theta\)</span> then <span class="math inline">\(p\)</span> is polynomial in <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>By construction <span class="math inline">\(rank(G(\theta)) &lt; n\)</span> if and only if <span class="math inline">\(p(\theta) = 0\)</span>.</p></li>
<li><p>Let <span class="math inline">\(Z(p) = \{ \theta \in \mathbb{R}^p: p(\theta) = 0 \}\)</span> be the zero set of <span class="math inline">\(p\)</span>. If there exists a <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(p(\theta) \neq 0\)</span> then <span class="math inline">\(\mu(Z(p)) = 0\)</span>.</p></li>
</ol>
<p>We now apply this argument to our setting. It is worth remarking that in Theorem 2.1 of <span class="citation" data-cites="quad-network-landscape">(<a href="#ref-quad-network-landscape" role="doc-biblioref">Soltanolkotabi, Javanmard, and Lee 2019</a>)</span> it is stated that <span class="math inline">\(n\geq d\)</span> but this is not necessary (which should be intuitive!).</p>
<div id="lem-ae-full-rank" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2 </strong></span>If <span class="math inline">\(d \geq 3\)</span> and <span class="math inline">\(n \lesssim d^2\)</span> then for almost every <span class="math inline">\(X \in \mathbb{R}^{d \times n}\)</span> we have <span class="math inline">\(rank(X \star X) = n\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the argument above it suffices to find just one <span class="math inline">\(X\)</span> such that <span class="math inline">\(rank(X \star X) = n\)</span>. We consider two cases, <span class="math inline">\(n\leq d\)</span> and <span class="math inline">\(d &lt; n \leq cd^2\)</span> for some constant <span class="math inline">\(c \in (0,1]\)</span>. For the first case, consider <span class="math inline">\(X = [e_1, e_2... e_n]\)</span> where <span class="math inline">\(e_i\)</span> is the standard basis with a one at index <span class="math inline">\(i\)</span> and zero everywhere else. Then <span class="math inline">\(X \star X = [e_1, e_2... e_n]\)</span> (here the <span class="math inline">\(e_i\)</span> are being used interchangeably to represent the standard basis vectors in both <span class="math inline">\(\mathbb{R}^d\)</span> and <span class="math inline">\(\mathbb{R}^{d^2})\)</span> which clearly is rank <span class="math inline">\(n\)</span>. For the other case, consider <span class="math inline">\(X\)</span> with unique columns of the form <span class="math inline">\(e_i + e_j\)</span> for <span class="math inline">\(i,j \in [n]\)</span> with <span class="math inline">\(i &lt;j\)</span>. Let <span class="math inline">\(p(i,j) = d(i-1) +j\)</span>. Note <span class="math inline">\([(e_i + e_j) \otimes (e_i + e_j)]_{p(k,l)} = 1\)</span> if and only if <span class="math inline">\(k=l\)</span> and <span class="math inline">\(j = l\)</span>. Denoting <span class="math inline">\(S = \{e_i + e_j: \; 1\leq i &lt; j \leq n \}\)</span> then any <span class="math inline">\(X\)</span> formed from <span class="math inline">\(n\)</span> of the <span class="math inline">\(|S| = (d - 1)(d-2)/2\)</span> elements of <span class="math inline">\(S\)</span> generates an <span class="math inline">\(X \star X\)</span> which is full rank. Note for <span class="math inline">\(|S| &gt;0\)</span> we require <span class="math inline">\(d \geq 3\)</span>.</p>
</div>
</section>
</section>
<section id="proof-of-thm-quad" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-thm-quad">Proof of <a href="#thm-quad">Theorem&nbsp;1</a></h3>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Note as the loss is twice differentiable any local minima or (non-strict) saddle must satisfy the following conditions, <span class="math display">\[
  \begin{align*}
    \nabla L (W) &amp;= 0,\\
     \nabla^2L(W) &amp;\succcurlyeq 0.
  \end{align*}
  \]</span> For Statement 1 it suffices to show any <span class="math inline">\(W\)</span> satisfying these conditions in turn satisfies <span class="math display">\[
\sum_{i=1}^n r_i(W) x_i x_i^T = 0
\]</span> and therefore by <a href="#lem-proxy">Lemma&nbsp;1</a> must be a global minimum. If <span class="math inline">\(W\)</span> is a stationary point then <span class="math display">\[
\begin{align*}
\nabla L(W) = DW \sum_{i=1}^n r_i(W) x_i x_i^T = 0
\end{align*}
\]</span> Recall <span class="math inline">\(DW \in \mathbb{R}^{2m \times d}\)</span> and <span class="math inline">\(m \geq d\)</span>. If <span class="math inline">\(DW\)</span> is full rank then it has a left inverse, i.e., there exists a matrix <span class="math inline">\(H\)</span> such that <span class="math inline">\(HDW = I\)</span>. As a result, if <span class="math inline">\(W\)</span> is stationary and <span class="math inline">\(DW\)</span> is full rank then <span class="math display">\[
HDW\sum_{i=1}^n r_i(W) x_i x_i^T = \sum_{i=1}^n r_i(W) x_i x_i^T = 0.
\]</span> Suppose instead that <span class="math inline">\(DW\)</span> is not full rank. As <span class="math inline">\(W\)</span> is either a local minimum or a non-strict saddle then <span class="math inline">\(\nabla^2L(W) \succcurlyeq 0\)</span> which in turn implies <span class="math display">\[
\sum_{i=1}^n \left( x_i^T D W^T U x_i\right)^2 + \sum_{i=1}^n r_i(W) x_i^T U^T D U x_i \geq 0
\]</span> for any <span class="math inline">\(U \in \mathbb{R}^{2m \times d}\)</span>. Consider <span class="math inline">\(U = ab^T\)</span> for some <span class="math inline">\(a \in R^{2m}\)</span>, <span class="math inline">\(b \in \mathbb{R}^{d}\)</span>. Then <span class="math display">\[
\begin{align*}
\sum_{i=1}^n \left( x_i^T  W^TD U x_i\right)^2 + \sum_{i=1}^n r_i(W) x_i^T U^T D U x_i &amp; = \sum_{i=1}^n \left( x_i^T  W^TD a b^T x_i\right)^2 + \sum_{i=1}^n r_i(W) x_i^T b a^T D a b^T x_i\\
&amp;= \sum_{i=1}^n \left( x_i^T  W^TD a b^T x_i\right)^2 + (a^T D a) \sum_{i=1}^n r_i(W) x_i^T b  b^T x_i\\
&amp;= \sum_{i=1}^n \left( x_i^T  W^T D a b^T x_i\right)^2 + (a^T D a) b^T \left(\sum_{i=1}^n r_i(W) x_i x_i^T\right) b\\
&amp; \geq 0.
\end{align*}
\]</span> Let <span class="math inline">\(E(W^TD), O(W^T D) \in \mathbb{R}^{m \times d}\)</span> denote the submatrices of <span class="math inline">\(W^T d\)</span> formed by taking only the even and odd columns. Again as <span class="math inline">\(DW\)</span> is not full rank it must hold that both <span class="math inline">\(E(DW)\)</span> and <span class="math inline">\(O(DW)\)</span> are not full rank. Therefore let <span class="math inline">\(a_1 \in \mathbb{R}^{2m}\)</span> be a vector whose even indexed elements are all zero and whose odd indexed elements form a subvector which lies in the nullspace of <span class="math inline">\(O(DW)\)</span>. Likewise, let <span class="math inline">\(a_1 \in \mathbb{R}^{2m}\)</span> be a vector whose odd indexed elements are all zero and whose even indexed elements form a subvector which lies in the nullspace of <span class="math inline">\(E(DW)\)</span>. Then <span class="math inline">\(a_1, a_2 \in null(W^T D)\)</span> and as a result the left-hand-side of the above inequality simplifies significantly, <span class="math display">\[
\begin{align*}
\left(a_1^TDa_1 \right) b^T \left(\sum_{i=1}^n r_i(W) x_i x_i^T\right) b &amp;\geq 0\\
\left(a_2^TDa_2 \right) b^T \left(\sum_{i=1}^n r_i(W) x_i x_i^T\right) b &amp;\geq 0
\end{align*}
\]</span> for all <span class="math inline">\(b \in \mathbb{R}^d\)</span>. Furthermore <span class="math display">\[
\begin{align*}
a_1^T D a_1  &amp;= -\sum_{i} a_{1i}^2 &lt; 0\\
a_2^T D a_2  &amp;= \sum_{i} a_{2i}^2 &gt; 0
\end{align*}
\]</span> and therefore <span class="math display">\[
\begin{align*}
\left(a_1^TDa_1 \right) b^T \left(\sum_{i=1}^n r_i(W) x_i x_i^T\right) b &amp; \leq 0,\\
\left(a_2^TDa_2 \right) b^T \left(\sum_{i=1}^n r_i(W) x_i x_i^T\right) b &amp; \geq  0
\end{align*}
\]</span> for all <span class="math inline">\(b \in \mathbb{R}^d\)</span>. This is possible if and only if <span class="math inline">\(\sum_{i=1}^n r_i(W) x_i x_i^T = 0\)</span>. This concludes the proof of Statement 1.</p>
<p>For Statement 2, observe <span class="math display">\[
  \begin{align*}
    \sum_{i=1}^n r_i(W) x_i x_i^T = 0_{d \times d}
  \end{align*}
\]</span> is equivalent to <span class="math display">\[
(X \star X)r(W) = 0_{d^2},
\]</span> recall we use subscripts to clarify the shape of the tensor in question. It suffices to prove that <span class="math inline">\((X \star X) \in \mathbb{R}^{d^2 \times n}\)</span> is full rank as this would then imply <span class="math inline">\(r(W) = 0\)</span>, or equivalently or <span class="math inline">\(f(x_i, W) = y_i\)</span> for all <span class="math inline">\(i \in [n]\)</span>. As long as <span class="math inline">\(d\geq 3\)</span> and <span class="math inline">\(n \lesssim d^2\)</span> then this is true for almost every <span class="math inline">\(X\in \mathbb{R}^{d \times n}\)</span> as per <a href="#lem-ae-full-rank">Lemma&nbsp;2</a>.</p>
</div>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>In this post we have illustrated under relatively mild conditions that a shallow network with quadratic activations has a benign loss landscape in the sense that all global minima are local and all saddles are strict saddles. Interestingly there are analogues with deep linear networks for which likewise all local minima are global, however not all saddles are strict. One of the key tricks we saw as per <a href="#lem-proxy">Lemma&nbsp;1</a> was to find a proxy problem for which global certificates are easy to derive and whose global minimizers are a super set of the global minimizers of the network loss. It would be interesting to see where and how this idea could be extended to other settings.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-escape_saddles" class="csl-entry" role="listitem">
Du, Simon S., Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabás Póczos, and Aarti Singh. 2017. <span>“Gradient Descent Can Take Exponential Time to Escape Saddle Points.”</span> In <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 1067–77. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.
</div>
<div id="ref-pmlr-v49-lee16" class="csl-entry" role="listitem">
Lee, Jason D., Max Simchowitz, Michael I. Jordan, and Benjamin Recht. 2016. <span>“Gradient Descent Only Converges to Minimizers.”</span> In <em>29th Annual Conference on Learning Theory</em>, edited by Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, 49:1246–57. Proceedings of Machine Learning Research. Columbia University, New York, New York, USA: PMLR. <a href="https://proceedings.mlr.press/v49/lee16.html">https://proceedings.mlr.press/v49/lee16.html</a>.
</div>
<div id="ref-quad-network-landscape" class="csl-entry" role="listitem">
Soltanolkotabi, Mahdi, Adel Javanmard, and Jason D. Lee. 2019. <span>“Theoretical Insights into the Optimization Landscape of over-Parameterized Shallow Neural Networks.”</span> <em>IEEE Transactions on Information Theory</em> 65 (2): 742–69. <a href="https://doi.org/10.1109/TIT.2018.2854560">https://doi.org/10.1109/TIT.2018.2854560</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>