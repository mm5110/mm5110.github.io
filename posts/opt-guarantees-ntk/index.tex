% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \ifXeTeX
    \usepackage{mathspec} % this also loads fontspec
  \else
    \usepackage{unicode-math} % this also loads fontspec
  \fi
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Optimization guarantees for neural networks via the NTK},
  pdfauthor={Michael Murray},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Optimization guarantees for neural networks via the NTK}
\author{Michael Murray}
\date{2023-09-27}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[frame hidden, borderline west={3pt}{0pt}{shadecolor}, interior hidden, boxrule=0pt, enhanced, sharp corners, breakable]}{\end{tcolorbox}}\fi

\emph{In this post we will look at how the Neural Tangent Kernel (NTK)
can be used to derive derive training guarantees for sufficiently
overparameterized neural networks.}

\%\%\%\%\% NEW MATH DEFINITIONS \%\%\%\%\% \$\$ \%

\usepackage{amsmath,amsfonts,bm}

\newenvironment{definition}{Definition}
\newenvironment

\{lemma\}\{Lemma\}

\newenvironment{theorem}{Theorem}


% Mark sections of captions for referring to divisions of figures
% \newcommand{\figleft}{{\em (Left)}}
% \newcommand{\figcenter}{{\em (Center)}}
% \newcommand{\figright}{{\em (Right)}}
% \newcommand{\figtop}{{\em (Top)}}
% \newcommand{\figbottom}{{\em (Bottom)}}
% \newcommand{\captiona}{{\em (a)}}
% \newcommand{\captionb}{{\em (b)}}
% \newcommand{\captionc}{{\em (c)}}
% \newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand

\{\newterm\}{[}1{]}\{\{\bf \#1\}\}

\% Figure reference, lower-case. \def\figref#1{figure~\ref{#1}} \%
Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\% Section reference, lower-case. \def\secref#1{section~\ref{#1}} \%
Section reference, capital. \def\Secref#1{Section~\ref{#1}} \% Reference
to two sections. \def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}} \%
Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}} \%
Reference to an equation, lower-case. \def\eqref#1{equation~\ref{#1}} \%
Reference to an equation, upper case \def\Eqref#1{Equation~\ref{#1}} \%
A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}} \% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}} \% Reference to an equation, upper
case. \def\Chapref#1{Chapter~\ref{#1}} \% Reference to a range of
chapters \def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}} \% Reference
to an algorithm, lower-case. \def\algref#1{algorithm~\ref{#1}} \%
Reference to an algorithm, upper case. \def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}} \% Reference to a
part, lower case \def\partref#1{part~\ref{#1}} \% Reference to a part,
upper case \def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}

\% Random variables \def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}} \def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}} \def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}} \def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}} \def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}} \def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}} \def\rl{{\textnormal{l}}} \% rm is already a
command, just don't name any random variables m
\def\rn{{\textnormal{n}}} \def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}} \def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}} \def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}} \def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}} \def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}} \def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\% Random vectors \def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}} \def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}} \def\rvc{{\mathbf{c}}} \def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}} \def\rvf{{\mathbf{f}}} \def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}} \def\rvu{{\mathbf{i}}} \def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}} \def\rvl{{\mathbf{l}}} \def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}} \def\rvo{{\mathbf{o}}} \def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}} \def\rvr{{\mathbf{r}}} \def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}} \def\rvu{{\mathbf{u}}} \def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}} \def\rvx{{\mathbf{x}}} \def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\% Elements of random vectors \def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}} \def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}} \def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}} \def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}} \def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}} \def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}} \def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}} \def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}} \def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}} \def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}} \def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}} \def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}} \def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\% Random matrices \def\rmA{{\mathbf{A}}} \def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}} \def\rmD{{\mathbf{D}}} \def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}} \def\rmG{{\mathbf{G}}} \def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}} \def\rmJ{{\mathbf{J}}} \def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}} \def\rmM{{\mathbf{M}}} \def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}} \def\rmP{{\mathbf{P}}} \def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}} \def\rmS{{\mathbf{S}}} \def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}} \def\rmV{{\mathbf{V}}} \def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}} \def\rmY{{\mathbf{Y}}} \def\rmZ{{\mathbf{Z}}}

\% Elements of random matrices \def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}} \def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}} \def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}} \def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}} \def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}} \def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}} \def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}} \def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}} \def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}} \def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}} \def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}} \def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}} \def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\% Vectors \def\vzero{{\bm{0}}} \def\vone{{\bm{1}}} \def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}} \def\va{{\bm{a}}} \def\vb{{\bm{b}}}
\def\vc{{\bm{c}}} \def\vd{{\bm{d}}} \def\ve{{\bm{e}}} \def\vf{{\bm{f}}}
\def\vg{{\bm{g}}} \def\vh{{\bm{h}}} \def\vi{{\bm{i}}} \def\vj{{\bm{j}}}
\def\vk{{\bm{k}}} \def\vl{{\bm{l}}} \def\vm{{\bm{m}}} \def\vn{{\bm{n}}}
\def\vo{{\bm{o}}} \def\vp{{\bm{p}}} \def\vq{{\bm{q}}} \def\vr{{\bm{r}}}
\def\vs{{\bm{s}}} \def\vt{{\bm{t}}} \def\vu{{\bm{u}}} \def\vv{{\bm{v}}}
\def\vw{{\bm{w}}} \def\vx{{\bm{x}}} \def\vy{{\bm{y}}} \def\vz{{\bm{z}}}

\% Elements of vectors \def\evalpha{{\alpha}} \def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}} \def\evlambda{{\lambda}}
\def\evomega{{\omega}} \def\evmu{{\mu}} \def\evpsi{{\psi}}
\def\evsigma{{\sigma}} \def\evtheta{{\theta}} \def\eva{{a}}
\def\evb{{b}} \def\evc{{c}} \def\evd{{d}} \def\eve{{e}} \def\evf{{f}}
\def\evg{{g}} \def\evh{{h}} \def\evi{{i}} \def\evj{{j}} \def\evk{{k}}
\def\evl{{l}} \def\evm{{m}} \def\evn{{n}} \def\evo{{o}} \def\evp{{p}}
\def\evq{{q}} \def\evr{{r}} \def\evs{{s}} \def\evt{{t}} \def\evu{{u}}
\def\evv{{v}} \def\evw{{w}} \def\evx{{x}} \def\evy{{y}} \def\evz{{z}}

\% Matrix \def\mA{{\bm{A}}} \def\mB{{\bm{B}}} \def\mC{{\bm{C}}}
\def\mD{{\bm{D}}} \def\mE{{\bm{E}}} \def\mF{{\bm{F}}} \def\mG{{\bm{G}}}
\def\mH{{\bm{H}}} \def\mI{{\bm{I}}} \def\mJ{{\bm{J}}} \def\mK{{\bm{K}}}
\def\mL{{\bm{L}}} \def\mM{{\bm{M}}} \def\mN{{\bm{N}}} \def\mO{{\bm{O}}}
\def\mP{{\bm{P}}} \def\mQ{{\bm{Q}}} \def\mR{{\bm{R}}} \def\mS{{\bm{S}}}
\def\mT{{\bm{T}}} \def\mU{{\bm{U}}} \def\mV{{\bm{V}}} \def\mW{{\bm{W}}}
\def\mX{{\bm{X}}} \def\mY{{\bm{Y}}} \def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}} \def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}} \def\mSigma{{\bm{\Sigma}}}

\% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}

\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}

\% \def\tR{{\tens{R}}} \def\tS{{\tens{S}}} \def\tT{{\tens{T}}}
\def\tU{{\tens{U}}} \def\tV{{\tens{V}}} \def\tW{{\tens{W}}}
\def\tX{{\tens{X}}} \def\tY{{\tens{Y}}} \def\tZ{{\tens{Z}}}

\% Graph \def\gA{{\mathcal{A}}} \def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}} \def\gD{{\mathcal{D}}} \def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}} \def\gG{{\mathcal{G}}} \def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}} \def\gJ{{\mathcal{J}}} \def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}} \def\gM{{\mathcal{M}}} \def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}} \def\gP{{\mathcal{P}}} \def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}} \def\gS{{\mathcal{S}}} \def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}} \def\gV{{\mathcal{V}}} \def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}} \def\gY{{\mathcal{Y}}} \def\gZ{{\mathcal{Z}}}

\% Sets \def\sA{{\mathbb{A}}} \def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}} \def\sD{{\mathbb{D}}} \% Don't use a set called E,
because this would be the same as our symbol \% for expectation.
\def\sF{{\mathbb{F}}} \def\sG{{\mathbb{G}}} \def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}} \def\sJ{{\mathbb{J}}} \def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}} \def\sM{{\mathbb{M}}} \def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}} \def\sP{{\mathbb{P}}} \def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}} \def\sS{{\mathbb{S}}} \def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}} \def\sV{{\mathbb{V}}} \def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}} \def\sY{{\mathbb{Y}}} \def\sZ{{\mathbb{Z}}}

\% Entries of a matrix \def\emLambda{{\Lambda}} \def\emA{{A}}
\def\emB{{B}} \def\emC{{C}} \def\emD{{D}} \def\emE{{E}} \def\emF{{F}}
\def\emG{{G}} \def\emH{{H}} \def\emI{{I}} \def\emJ{{J}} \def\emK{{K}}
\def\emL{{L}} \def\emM{{M}} \def\emN{{N}} \def\emO{{O}} \def\emP{{P}}
\def\emQ{{Q}} \def\emR{{R}} \def\emS{{S}} \def\emT{{T}} \def\emU{{U}}
\def\emV{{V}} \def\emW{{W}} \def\emX{{X}} \def\emY{{Y}} \def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\% entries of a tensor \% Same font as tensor, without \bm wrapper

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}} \% The empirical distribution defined
by the training set \newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}} \% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}} \% Stochastic
autoencoder distributions \newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}}

\% Laplace distribution

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}

\% Wolfram Mathworld says \(L^2\) is for function spaces and \(\ell^2\)
is for vectors \% But then they seem to use \(L^2\) for vectors
throughout the site, and so does \% wikipedia.
\newcommand{\normlzero}{L^0} \newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2} \newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa}

\% See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\% MM commands \% Defined commands \newcommand{\reals}{\mathbb{R}}
\newcommand{\sphere}{\mathbb{S}} \newcommand{\comps}{\mathbb{C}}
\newcommand{\naturals}{\mathbb{N}} \newcommand{\borel}{\mathcal{B}}
\newcommand{\defeq}{\vcentcolon=} \newcommand{\eqdef}{=\vcentcolon}
\newcommand{\expec}{\mathbb{E}} \newcommand{\prob}{\mathbb{P}}
\newcommand{\supp}{\text{supp}}

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\% \newcommand{\htanh}{\ensuremath{\text{htanh}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}

\% Caligraphic letters \newcommand{\cA}{\ensuremath{\mathcal{A}}}
\newcommand{\cN}{\ensuremath{\mathcal{N}}}
\newcommand{\cM}{\ensuremath{\mathcal{M}}}
\newcommand{\cB}{\ensuremath{\mathcal{B}}}
\newcommand{\cV}{\ensuremath{\mathcal{V}}}
\newcommand{\cC}{\ensuremath{\mathcal{C}}}
\newcommand{\cF}{\ensuremath{\mathcal{F}}}
\newcommand{\cX}{\ensuremath{\mathcal{X}}}
\newcommand{\cY}{\ensuremath{\mathcal{Y}}}
\newcommand{\cO}{\ensuremath{\mathcal{O}}}
\newcommand{\cH}{\ensuremath{\mathcal{H}}}
\newcommand{\cD}{\ensuremath{\mathcal{D}}}
\newcommand{\cL}{\ensuremath{\mathcal{L}}}
\newcommand{\cI}{\ensuremath{\mathcal{I}}}
\newcommand{\cJ}{\ensuremath{\mathcal{J}}}
\newcommand{\cS}{\ensuremath{\mathcal{S}}}
\newcommand{\cG}{\ensuremath{\mathcal{G}}}
\newcommand{\cP}{\ensuremath{\mathcal{P}}} \$\$

For now let \(f\) be a parameterized function with parameters
\(\theta \in \mathbb{R}^p\), later we will specialize to the setting of
\(f\) being a shallow or deep network. Consider an arbitrary training
sample consisting of \(n\) pairs of points and their corresponding
targets
\(({\bm{x}}_i, y_i)_{i=1}^n \in (\mathbb{R}^d \times \mathbb{R})^n\),
recall the least squares loss defined as \begin{align*}
    L(\theta) &= \frac{1}{2}\sum_{i=1}^n (f(\theta, {\bm{x}}_i) - y_i)^2.
\end{align*} To solve the least squares problem we study the trajectory
through parameter space under gradient flow, a continuous time
simplification of gradient descent (for a review of gradient flow see
Appendix \ref{app-multi-sub:grad-flow}): simplifying our notation by
using \(L(t)\) instead of \(L(\theta(t))\), then for \(t\geq 0\)
\begin{equation} \label{opt2-eq:grad-flow}
    \frac{d \theta(t)}{dt} = - \nabla_{\theta} L(t).
\end{equation} Unless otherwise stated we will typically assume either
that \(f\) is differentiable or at least that \(f\) is differentiable
along the trajectory of gradient flow. With
\({\bm{u}}(t) = [f(\theta(t), {\bm{x}}_1), f(\theta(t), {\bm{x}}_2)... f(\theta(t), {\bm{x}}_n)] \in \mathbb{R}^n\)
denoting the vector of predictions and
\({\bm{r}}(t) = {\bm{u}}(t) - {\bm{y}}\) the vector of residuals at time
\(t \geq 0\), then we denote the Jacobian as \begin{align*}
{\bm{J}}(t) = \nabla_\theta {\bm{r}}(t)= \nabla_\theta {\bm{u}}(t) =
  \begin{bmatrix}
    \frac{\partial f(\theta(t), {\mathbf{x}}_1) }{\partial \theta_1} & \frac{\partial f(\theta(t), {\bm{x}}_2) }{\partial \theta_1} & ... & \frac{\partial f(\theta(t), {\bm{x}}_n) }{\partial \theta_1} \\
    \frac{\partial f(\theta(t), {\bm{x}}_1) }{\partial \theta_2} & \frac{\partial f(\theta(t), {\bm{x}}_2) }{\partial \theta_2} & ... & \frac{\partial f(\theta(t), {\bm{x}}_n) }{\partial \theta_2} \\
    \vdots & \vdots & \ddots & \vdots\\
    \frac{\partial f(\theta(t), {\bm{x}}_1) }{\partial \theta_p} & \frac{\partial f(\theta(t), {\bm{x}}_2) }{\partial \theta_p} & ... & \frac{\partial f(\theta(t), {\bm{x}}_n) }{\partial \theta_p} 
  \end{bmatrix}
  \in \mathbb{R}^{p \times n}.
\end{align*} The Jacobian and its gram,
\({\bm{H}}(t) = {\bm{J}}(t)^T {\bm{J}}(t) \in \mathbb{R}^{n \times n}\),
which is also referred to as the Neural Tangent Kernel (NTK) gram
matrix, will play a critical role in what follows here and also when it
comes to studying linearized neural networks. Note calling
\({\bm{H}}(t)\) the NTK or NTK gram matrix is somewhat misleading as it
can be studied more generally for any sufficiently smooth model not just
neural networks. However, as it is now accepted terminology we will
stick with it. The following proposition illustrates the significance of
this matrix for training.



\end{document}
