@article{lap-revisited,
author = {Chii-Ruey Hwang},
title = {{Laplace's Method Revisited: Weak Convergence of Probability Measures}},
volume = {8},
journal = {The Annals of Probability},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {1177 -- 1182},
keywords = {Laplace's method, smooth manifold, weak convergence},
year = {1980},
doi = {10.1214/aop/1176994579},
URL = {https://doi.org/10.1214/aop/1176994579}
}

@inproceedings{
banerjee2023neural,
title={Neural Tangent Kernel at Initialization: Linear Width Suffices},
author={Arindam Banerjee and Pedro Cisneros-Velarde and Libin Zhu and Misha Belkin},
booktitle={The 39th Conference on Uncertainty in Artificial Intelligence},
year={2023},
url={https://openreview.net/forum?id=VJaoe7Rp9tZ}
}

@misc{clark2012instructors,
      title={The Instructor's Guide to Real Induction}, 
      author={Pete L. Clark},
      year={2012},
      eprint={1208.0973},
      archivePrefix={arXiv},
      primaryClass={math.HO}
}


@book{bhatia97,
  added-at = {2013-06-15T01:58:38.000+0200},
  author = {Bhatia, Rajendra},
  biburl = {https://www.bibsonomy.org/bibtex/269934a372db92a018132c5880987691e/ytyoun},
  interhash = {a52e63731d9a0e304c29b795ed54cf94},
  intrahash = {69934a372db92a018132c5880987691e},
  isbn = {0387948465},
  keywords = {courant-fischer eigenvalues linear.algebra majorization matrix textbook},
  publisher = {Springer},
  timestamp = {2017-02-13T08:18:47.000+0100},
  title = {Matrix Analysis},
  volume = 169,
  year = 1997
}

@inproceedings{
du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@misc{jacot2020neural,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
      author={Arthur Jacot and Franck Gabriel and Clément Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oymak2019moderate,
      title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks}, 
      author={Samet Oymak and Mahdi Soltanolkotabi},
      year={2019},
      eprint={1902.04674},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{
brutzkus2018sgd,
title={{SGD} Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
author={Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJ33wwxRb},
}


@misc{karhadkar2024benign,
      title={Benign overfitting in leaky ReLU networks with moderate input dimension}, 
      author={Kedar Karhadkar and Erin George and Michael Murray and Guido Montúfar and Deanna Needell},
      year={2024},
      eprint={2403.06903},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2023_6e73c39c,
 author = {George, Erin and Murray, Michael and Swartworth, William and Needell, Deanna},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {35139--35189},
 publisher = {Curran Associates, Inc.},
 title = {Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6e73c39cc428c7d264d9820319f31e79-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{escape_saddles, author = {Du, Simon S. and Jin, Chi and Lee, Jason D. and Jordan, Michael I. and P\'{o}czos, Barnab\'{a}s and Singh, Aarti}, title = {Gradient descent can take exponential time to escape saddle points}, year = {2017}, isbn = {9781510860964}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points—it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {1067–1077}, numpages = {11}, location = {Long Beach, California, USA}, series = {NIPS'17} }



@InProceedings{pmlr-v49-lee16,
  title = 	 {Gradient Descent Only Converges to Minimizers},
  author = 	 {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1246--1257},
  year = 	 {2016},
  editor = 	 {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/lee16.pdf},
  url = 	 {https://proceedings.mlr.press/v49/lee16.html},
  abstract = 	 {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.}
}


@ARTICLE{quad-network-landscape,
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D.},
  journal={IEEE Transactions on Information Theory}, 
  title={Theoretical Insights Into the Optimization Landscape of Over-Parameterized Shallow Neural Networks}, 
  year={2019},
  volume={65},
  number={2},
  pages={742-769},
  keywords={Optimization;Training;Biological neural networks;Data models;Numerical models;Convergence;Nonconvex optimization;over-parametrized neural networks;random matrix theory},
  doi={10.1109/TIT.2018.2854560}}
